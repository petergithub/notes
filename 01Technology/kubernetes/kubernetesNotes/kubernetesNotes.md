# kubernetesNotes

Container Network Interface (CNI)

## Recent

batch delete `kdelp $(kgp -l | grep Evicted | awk '{print $1}')`
kubelet summary API `http://localhost:8001/api/v1/nodes/node-name/proxy/stats/summary`

install arthas: `kubectl exec -it podName -- /bin/bash -c "wget https://arthas.aliyun.com/arthas-boot.jar && java -jar arthas-boot.jar"`

GitOps é¡¹ç›®ç»“æ„

1. [a GitOps repository structure standard using ArgoCD. - TheCodingSheikh/kubecodex](https://github.com/TheCodingSheikh/kubecodex)
2. [fluxcd/flux2-kustomize-helm-example: A GitOps workflow example for multi-env deployments with Flux, Kustomize and Helm.](https://github.com/fluxcd/flux2-kustomize-helm-example)

## kubectl

[kubectl Overview](https://jamesdefabia.github.io/docs/user-guide/kubectl-overview/)
[kubectl-cheatsheet An assortment of compact kubectl examples](https://github.com/fabric8io/kansible/blob/master/vendor/k8s.io/kubernetes/docs/user-guide/kubectl-cheatsheet.md)

```sh
#   mkdir -p $HOME/.kube
#   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
#   sudo chown $(id -u):$(id -g) $HOME/.kube/config
# Alternatively, if you are the root user, you can run:
#   export KUBECONFIG=/etc/kubernetes/admin.conf
kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes
kubectl help get
```

`minikube start` Starting a Minikube virtual machine
`kubectl explain pod` kubectl explain to discover possible API object fields
`kubectl explain pod.spec` drill deeper to find out more about each attribute
`kubectl api-resources` Print the supported API resources on the server

`kubectl --kubeconfig=/path/to/cluster-admin.config --namespace monitoring get pod`

`kubectl cluster-info` Displaying cluster information
`kubectl get nodes/pods/secrets/services/deployment`
    `-o wide` request additional columns to display
    `-o yaml` get a YAML descriptor of an existing pod
    `-o json`
    `--show-labels`
    `-L, --label-columns=[]` switch and have each displayed in its own column `-L creation_method,env`
    `-l, --selector=''`
    `-A, --all-namespaces=false` If present, list the requested object(s) across all namespaces
    `-w, --watch` watch for changes
    `--v 6` verbose logging

`kubectl get pods --selector='labelName=labelValue'`
    `'!env'`
    `'env'`
    `env in (prod,devel)`
    `env notin (prod,devel)`

`kubectl create`
  `-f, --filename=[]` Filename, directory, or URL to files to use to create the resource
  `--record` record the command

`kubectl delete pods pod_name`
  `--force` force to delete

`kubectl describe node NODE_NAME`

`kubectl cp /path/to/file /target/path`

Deploying your Node.js app: create ReplicationController `kubectl run kubia --image=luksa/kubia --port=8080 --generator=run/v1`
Accessing your web application:  creating a service object `kubectl expose rc kubia --type=LoadBalancer --name kubia-http`
accessing your service through its external ip `curl 104.155.74.57:8080`

`kubectl scale rc kubia --replicas=3` increasing the desired replica count

```sh
# command is used for creating any resource (not only pods) from a YAML or JSON file.
kubectl create -f FILE_NAME.yaml
# æ›´æ–°
kubectl apply -f FILE_NAME.yaml
kubectl apply -f FOLDER

# åˆ é™¤
kubectl delete -f FILE_NAME.yaml
```

```sh
# cat <<EOF | kubectl apply -f -
kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: testmap
  namespace: monitoring
data:
  webconfig.yml: |
    basic_auth_users:
      prometheus: password
EOF

```

`kubectl edit deploy piggy-mongo` open the YAML definition in your default text editor ä¿®æ”¹
`kubectl patch svc nodeport -p '{"spec":{"externalTrafficPolicy":"Local"}}'` æ·»åŠ 

`kubectl exec -it <pod name> -- /bin/sh`  è¿›å…¥podå†…éƒ¨
`kubectl exec -it [POD_NAME] -c [CONTAINER_NAME] -- /bin/sh -c "kill 1"` restart the specific container

```sh
# forwarding a local network port 8888 to a port 8080 in the pod
kubectl port-forward pod-name 8888:8080
# make this port listen to 0.0.0.0
kubectl port-forward --address 0.0.0.0 svc/[service-name] -n [namespace] [external-port]:[internal-port]
```

COMMUNICATING WITH PODS THROUGH THE API SERVER
`kubectl proxy`
use localhost:8001 rather than the actual API server host and port. Youâ€™ll send a request to the kubia-0 pod like this:
`curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/`

`kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5` creates the HorizontalPodAutoscaler(HPA) object for you and sets the Deployment called kubia as the scaling target
`kubectl get hpa` HorizontalPodAutoscaler
a containerâ€™s CPU utilization is the containerâ€™s actual CPU usage divided by its requested CPU

### Cluster

```sh
# æŸ¥çœ‹å½“å‰çš„é›†ç¾¤ä¸Šä¸‹æ–‡
kubectl config current-context
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„é›†ç¾¤ä¸Šä¸‹æ–‡
kubectl config get-contexts
# åˆ‡æ¢åˆ°æŒ‡å®šçš„é›†ç¾¤ä¸Šä¸‹æ–‡
kubectl config use-context <context-name>
# æŸ¥çœ‹å½“å‰ä¸Šä¸‹æ–‡çš„è¯¦ç»†ä¿¡æ¯
kubectl config view

# ä¿®æ”¹é›†ç¾¤é…ç½®
kubectl config set-context <context-name> --cluster=<cluster-name> --user=<user-name> --namespace=<namespace>
# æ·»åŠ æ–°çš„é›†ç¾¤ä¸Šä¸‹æ–‡
kubectl config set-cluster <cluster-name> --server=<api-server-url> --certificate-authority=<ca-cert-file>
kubectl config set-credentials <user-name> --client-certificate=<client-cert-file> --client-key=<client-key-file>
kubectl config set-context <context-name> --cluster=<cluster-name> --user=<user-name> --namespace=<namespace>

kubectl cluster-info dump
```

### Debug

æ’æŸ¥é—®é¢˜

[Resource types](https://kubernetes.io/docs/reference/kubectl/overview/#resource-types)

[JSONPath Support](https://kubernetes.io/docs/reference/kubectl/jsonpath/)
`kubectl get pods -A -o=jsonpath='{range .items[*]}{.status.hostIP}{"\t"}{.spec.containers[].resources.requests.cpu}{"\t"}{.spec.containers[].resources.requests.memory}{"\t"}{.metadata.name}{"\n"}{end}' --sort-by='.status.hostIP'`

```sh
kubectl cluster-info dump

# retrieving a podâ€™s log with kubectl logs
kubectl logs kubia-manual
kubectl logs -f --tail=10 pod-name
# `-o custom-columns`
# `--sort-by=<jsonpath_exp>` sort the resource list,
# --sort-by=.metadata.name
#  specifying the container name when getting logs of a multi-container pod
# `--previous` figure out why the previous container
kubectl logs kubia-manual -c <container name>

# jsonpath æŸ¥è¯¢ç”³è¯·çš„å†…å­˜
kubectl get pods -A -o=jsonpath='{range .items[*]}{.spec.containers[].resources.requests.memory}{"\t"}{.status.hostIP}{"\t"}{.metadata.name}{"\n"}{end}' | grep 145
# get name cpu
kubectl get pods -o custom-columns=NAME:.metadata.name,CPU:.spec.containers
# get the pod's IP address
kubectl get pod multi-container-pod -o jsonpath={.status.podIP}

# go-template
kubectl get pods -o go-template='{{range .items}}{{.status.podIP}}{{"\n"}}{{end}}'

# è·å– CPU å’Œå†…å­˜ä½¿ç”¨æƒ…å†µ
kubectl top nodes
# pod èµ„æºä½¿ç”¨æƒ…å†µ
kubectl top pods
# container èµ„æºä½¿ç”¨æƒ…å†µ
kubectl top pods --containers
# sort by memory
kubectl top pod --all-namespaces | sort --reverse --key 4 --numeric | grep -v system | less

# åˆ›å»ºä¸´æ—¶è°ƒè¯•å®¹å™¨ï¼ˆKubernetes 1.23+ï¼‰
kubectl debug my-pod -it --image=busybox --target=my-container

# è·å–èµ„æºäº‹ä»¶
kubectl get events
kubectl get events -o custom-columns=Created:.metadata.creationTimestamp,FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message --field-selector involvedObject.kind=Pod,involvedObject.name=mysql-test-78b7567ccc-b96kb
kubectl get events --sort-by=.metadata.creationTimestamp
kubectl get events --sort-by=.metadata.creationTimestamp
kubectl get events --field-selector involvedObject.name=my-pod
kubectl get events -A -o custom-columns=FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message |less
# æŸ¥çœ‹æŒ‡å®š pod çš„ events
kubectl get events --watch -o custom-columns=Created:.metadata.creationTimestamp,FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message --field-selector involvedObject.kind=Pod, involvedObject.name=pod-test-78b7567ccc-b96kb

kubectl get events -o custom-columns=Created:.metadata.creationTimestamp,FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message --field-selector involvedObject.kind=Pod --sort-by=.metadata.creationTimestamp |less
kubectl get events -o custom-columns=Created:.metadata.creationTimestamp,FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message --sort-by=.metadata.creationTimestamp |less

kubectl get pods -A -o=jsonpath='{range .items[*]}{.spec.containers[].resources.requests.memory}{"\t"}{.status.hostIP}{"\t"}{.metadata.name}{"\n"}{end}' | grep 145

# get pod image
kubectl get deployment -o json ems-backend -o=jsonpath='{.spec.template.spec.containers[0].image}' | awk -F : '{print $NF}'
```

### Useful image

```sh
kubectl run -it --rm --restart=Never --image=mysql:8.0.28 mysql-client -- mysql
kubectl run -it --rm --restart=Never --image=redis:6.0.9 redis-client -- bash
kubectl run -it --rm --restart=Never --image=tutum/dnsutils dnsutils
kubectl run -it --rm --restart=Never --image=tutum/dnsutils dnsutils -- dig SRV kubia.default.svc.cluster.local
kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools
kubectl run -it --rm --restart=Never --image=tutum/curl curl
kubectl run -it --rm --image=nicolaka/netshoot netshoot
kubectl run -it --rm --image=nginx nginx
kubectl run -it --rm --image=busybox busybox  # busybox: BusyBox combines tiny versions of many common UNIX utilities
kubectl run -it --rm --image=alpine alpine  # alpine: A minimal Docker image based on Alpine Linux
apk add curl
```

### Setup cli

[Supercharge your Kubernetes setup with OhMyZSH ğŸš€ğŸš€ğŸš€ + awesome command line tools](https://agrimprasad.com/post/supercharge-kubernetes-setup/)

[Kubernetes prompt info for bash and zsh kube-ps1](https://github.com/jonmosco/kube-ps1)

```sh
# Kubectl autocomplete
# BASH
source <(kubectl completion bash) # set up autocomplete in bash into the current shell, bash-completion package should be installed first.
echo "source <(kubectl completion bash)" >> ~/.bashrc # add autocomplete perma

# You can also use a shorthand alias for kubectl that also works with completion:
alias k=kubectl
complete -o default -F __start_kubectl k
# ZSH
source <(kubectl completion zsh)  # set up autocomplete in zsh into the current shell
echo '[[ $commands[kubectl] ]] && source <(kubectl completion zsh)' >> ~/.zshrc # a
```

`brew install kube-ps1 stern`
kube-shell `pip install kube-shell --user -U`

ç»ˆæå·¥å…·k9s

åˆ‡æ¢é›†ç¾¤ç”¨çš„å‘½ä»¤ [kubectx + kubens: Power tools for kubectl](https://github.com/ahmetb/kubectx)
git clone https://github.com/junegunn/fzf.git ~/.fzf
~/.fzf/install

```sh
# [Krew is a tool that makes it easy to use kubectl plugins](https://krew.sigs.k8s.io/docs/user-guide/setup/install/)
# KREW="krew-linux_amd64"
(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)

#  |  | To list krew commands and to get help, run:
#  |  |   $ kubectl krew
#  |  | For a full list of available plugins, run:
#  |  |   $ kubectl krew search
#  |  |
#  |  | You can find documentation at
#  |  |   https://krew.sigs.k8s.io/docs/user-guide/quickstart/.

kubectl krew install ctx
kubectl krew install ns

# the tools will be available as kubectl ctx and kubectl ns
kubectl ctx
kubectl ns

```

bash é…ç½® kube-ps1.sh

```sh
# kubectl {
    source <(kubectl completion bash)
    complete -o default -F __start_kubectl k

    # [kube-ps1: Kubernetes prompt info for bash and zsh](https://github.com/jonmosco/kube-ps1)
    [ -d "/data/software/kube-ps1.sh" ] && source /data/software/kube-ps1.sh
    #PS1='[\u@\h \W $(kube_ps1)]\$ '
    PS1="$(kube_ps1) $PS1"
# } end kubectl
```

## Concept

OCI: Open Container Initiative
CRI: Container Runtime Interface
runc æ˜¯ä¸€ä¸ªå…¼å®¹ociçš„å®¹å™¨è¿è¡Œæ—¶ã€‚å®ƒå®ç°OCIè§„èŒƒå¹¶è¿è¡Œå®¹å™¨è¿›ç¨‹ã€‚

### [Requests and limits](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory)

Meaning of memoryï¼ŒMiè¡¨ç¤ºï¼ˆ1Mi=1024x1024ï¼‰,Mè¡¨ç¤ºï¼ˆ1M=1000x1000ï¼‰ï¼ˆå…¶å®ƒå•ä½ç±»æ¨ï¼Œ å¦‚Ki/K Gi/Gï¼‰
For example, the following represent roughly the same value: `128974848, 129e6, 129M, 123Mi`
1 (byte), 1k (kilobyte) or 1Ki (kibibyte), 1M (megabyte) or 1Mi (mebibyte)

Meaning of CPU:
The expression 0.1 is equivalent to the expression 100m, which can be read as "one hundred millicpu". Some people say "one hundred millicores", and this is understood to mean the same thing. A request with a decimal point, like 0.1, is converted to 100m by the API, and precision finer than 1m is not allowed. For this reason, the form 100m might be preferred.
`0.1 = 100m`

get all pods along with cpu and memory requirements in kubernetes `kubectl get po -o custom-columns="Name:metadata.name,CPU-request:spec.containers[*].resources.requests.cpu,CPU-limit:spec.containers[*].resources.limits.cpu,memory-request:spec.containers[*].resources.requests.memory,memory-limit:spec.containers[*].resources.limits.memory" --sort-by=".spec.containers[*].resources.limits.memory`

### PVC

`kubectl get pvc` volume claim

`kubectl patch pvc test-pvc -p '{"spec":{"resources":{"requests":{"storage":"50Gi"}}}}'` resize pvc

```json
{
    "apiVersion": "v1",
    "kind": "PersistentVolumeClaim",
    "metadata": {
        "annotations": {
            "pv.kubernetes.io/bind-completed": "yes",
            "pv.kubernetes.io/bound-by-controller": "yes",
            "volume.beta.kubernetes.io/storage-provisioner": "cluster.local/nfs-subdir-external-provisioner",
            "volume.kubernetes.io/storage-provisioner": "cluster.local/nfs-subdir-external-provisioner"
        },
        "name": "pvc-test",
        "namespace": "default"
    },
    "spec": {
        "accessModes": [
            "ReadWriteOnce"
        ],
        "resources": {
            "requests": {
                "storage": "1Mi"
            }
        },
        "storageClassName": "nfs-client",
        "volumeMode": "Filesystem",
        "volumeName": "pvc-f10cff78-eac8-4e4a-9de3-1b82ec7446f2"
    }
}
```

### Pod

`k port-forward pod-name 8001:5000` æœ¬åœ°ç«¯å£ 8001 è½¬å‘åˆ° pod çš„ç«¯å£ 5000

`kubectl delete pods $(kubectl get pods | grep Evicted |awk '{print $1}')` delete Evicted pods in current namespace

`kubectl get pods -A | grep Evicted | awk '{print "kubectl delete pods -n ",$1,$2}' | bash -x` delete Evicted pods in all namespaces

#### [Assign Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)

`nodeName: foo-node # schedule pod to specific node`

```yaml
nodeSelector:
  svrtype: web
```

### Service

è®¿é—®URI: `SERVICE_NAME.NAMESPACE.svc.cluster.local`
 `svc.cluster.local` is a configurable cluster domain suffix used in all cluster local service names.

[Debug Services](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/)

### Deployment

```sh
# [kubectl rollout restart | Kubernetes](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_rollout/)

# Restart a deployment
kubectl rollout restart deployment/nginx
# deployment å†å²è®°å½•
kubectl rollout history deployment deployment-name
# Changes the container image defined in a Pod
kubectl set image deployment kubia nodejs=luksa/kubia:v3
# the progress of the rollout
kubectl rollout status deployment kubia
# displaying a deploymentâ€™s rollout history
kubectl rollout history deployment kubia
# undoing a rollout
kubectl rollout undo deployment kubia
kubectl rollout undo deployment kubia --to-revision=1
kubectl rollout history deployment/<Deployment-name>  --revision=<revision-number>  -o yaml
```

deployment strategies:

- RollingUpdate
- Recreate

### Label

`kubectl label pod kubia-manual creation_method=manual` Modifying labels of existing pods
    `--overwrite`
`kubectl get pod -l creation_method=manual` Listing pods using a label selector
`kubectl get pod -l env` To list all pods that include the env label
`kubectl get po -l '!env'` To list all pods that donâ€™t have the env label

### Namespaces

`kubectl create ns name`
`kubectl get ns`
`kubectl get pod --namespace kube-system`

`kubectl config set-context --current --namespace=piggy` change namespace
`alias kcn='kubectl config set-context $(kubectl config current-context) --namespace '`

`kubectl create -f dev.json`

```json
{
  "apiVersion": "v1",
  "kind": "Namespace",
  "metadata": {
    "name": "dev",
    "labels": {
      "name": "dev"
    }
  }
}
```

`kubectl delete ns custom-namespace` delete the whole namespace (the pods will be deleted along with the namespace automatically)
`kubectl delete po --all` Deleting all pods in a namespace, while keeping the namespace
`kubectl delete all --all` Deleting (almost) all resources in a namespace

### Ingress

[Annotations - Ingress-Nginx Controller](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-max-body-size)

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-test
  namespace: default
  annotations:
    # HTTP 413 é”™è¯¯ ï¼ˆ Request entity too large è¯·æ±‚å®ä½“å¤ªå¤§ ï¼‰
    # Custom max body size
    # https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-max-body-size
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    # Client Body Buffer Size
    # https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#client-body-buffer-size
    nginx.ingress.kubernetes.io/client-body-buffer-size: 21m

    # Configuration snippet
    # Using this annotation you can add additional configuration to the NGINX location
    # https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#configuration-snippet
    nginx.ingress.kubernetes.io/configuration-snippet: |
        client_body_buffer_size 21m;

    # ä¸ºNginx Ingressé…ç½®HTTPSåè®®çš„åç«¯æœåŠ¡ï¼Œé»˜è®¤æ˜¯HTTPï¼Œå¦‚æœåç«¯æœåŠ¡æ··åˆä¸¤ç§åè®®ï¼Œå¯ä»¥é…ç½®å¤šä¸ªç›¸åŒåŸŸåçš„ Ingress
    # https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#backend-protocol
    nginx.ingress.kubernetes.io/backend-protocol:  "HTTPS"
    # æ°¸ä¹…é‡å®šå‘
    nginx.ingress.kubernetes.io/permanent-redirect: "https://www.example.com"
    nginx.ingress.kubernetes.io/permanent-redirect-code: "308" # é‡å®šå‘è¿”å›çŠ¶æ€ç 
    # 302 ä¸´æ—¶é‡å®šå‘
    nginx.ingress.kubernetes.io/temporal-redirect: "https://www.volcengine.com" # é‡å®šå‘åˆ°æŒ‡å®šçš„ç›®æ ‡ç½‘ç«™
    # å¼€å¯ HTTP é‡å®šå‘åˆ° HTTPS
    nginx.ingress.kubernetes.io/ssl-redirect: "true" # å¼€å¯ HTTP é‡å®šå‘åˆ° HTTPS

    # enable gzip
    nginx.ingress.kubernetes.io/server-snippet: |
      gzip on;
      gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;

```

#### Ingress è®¾ç½®é»˜è®¤çš„ ingressclass

`kubectl -n ingress-nginx patch ingressclass nginx -p '{"metadata":{"annotations":{"ingressclass.kubernetes.io/is-default-class":"true"}}}'`

```json
{
    "apiVersion": "networking.k8s.io/v1",
    "kind": "IngressClass",
    "metadata": {
        "annotations": {
            "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"networking.k8s.io/v1\",\"kind\":\"IngressClass\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"controller\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\",\"app.kubernetes.io/part-of\":\"ingress-nginx\",\"app.kubernetes.io/version\":\"1.11.2\"},\"name\":\"nginx\"},\"spec\":{\"controller\":\"k8s.io/ingress-nginx\"}}\n"
        },
        "labels": {
            "app.kubernetes.io/component": "controller",
            "app.kubernetes.io/instance": "ingress-nginx",
            "app.kubernetes.io/name": "ingress-nginx",
            "app.kubernetes.io/part-of": "ingress-nginx",
            "app.kubernetes.io/version": "1.11.2"
        },
        "name": "nginx"
    },
    "spec": {
        "controller": "k8s.io/ingress-nginx"
    }
}
```

#### Ingress enable gzip

nginx ingress enable gzip [ConfigMap - Ingress-Nginx Controller](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#use-gzip)

example: [Kubernetes Ingress Compression | James Joy's Blog](https://jamesjoy.site/posts/2023-06-12-kubernetes-ingress-compression)

nginx enable gzip [Compression and Decompression | NGINX Documentation](https://docs.nginx.com/nginx/admin-guide/web-server/compression)

```sh
kecm ingress-nginx-controller -n ingress-nginx -o yaml
```

```yaml
# Edit the file to add use-gzip: "true" and enable-brotli: "true":
apiVersion: v1
data:
  allow-snippet-annotations: "true"
  use-proxy-protocol: "true"
  use-gzip: "true"
  enable-brotli: "true"
kind: ConfigMap
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
```

#### Ingress for service in different namespaces

There is way to achieve ingress in one namespace and service in another namespace via externalName.Checkout [kubernetes-ingress/examples/ingress-resources/externalname-services at main Â· nginx/kubernetes-ingress](https://github.com/nginx/kubernetes-ingress/tree/main/examples/ingress-resources/externalname-services)

```yaml
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  type: ExternalName
  externalName: test-service.namespacename.svc.cluster.local

---

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: my-service
          servicePort: 80
```

#### Ingress è½¬å‘è¯·æ±‚æ—¶å»æ‰ sub path

å½“è¯·æ±‚è·¯å¾„ä¸º `http://yourdomain.com/subpath` æˆ– `http://yourdomain.com/subpath/` æ—¶ï¼Œè¯·æ±‚ä¼šè¢«è½¬å‘åˆ°åç«¯æœåŠ¡ `my-service` çš„æ ¹è·¯å¾„ `/`

ä¾‹å­ `example.com/subpath/query` æ”¹å†™æˆ `localhost/query`

```sh
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  namespace: default
  annotations:
    # è¿™ä¸ªæ³¨è§£ç”¨äºå®šä¹‰è·¯å¾„é‡å†™è§„åˆ™ã€‚/$2 è¡¨ç¤ºå°†åŒ¹é…åˆ°çš„è·¯å¾„éƒ¨åˆ†æ›¿æ¢ä¸ºç›®æ ‡è·¯å¾„ã€‚åœ¨æ­£åˆ™è¡¨è¾¾å¼ä¸­ï¼Œ$2 æ˜¯æ•è·ç»„çš„ç¬¬äºŒä¸ªéƒ¨åˆ†ï¼Œå³ (.*) åŒ¹é…çš„å†…å®¹ã€‚
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  rules:
  - host: example.com
    http:
      paths:
      # path: /subpath(/|$)(.*) ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä»¥ /subpath å¼€å¤´çš„è·¯å¾„
      # (/|$) åŒ¹é… / æˆ–è€…å­—ç¬¦ä¸²çš„ç»“å°¾ã€‚
      # (.*) æ•è· /subpath åé¢çš„ä»»æ„å­—ç¬¦ã€‚
      - path: /subpath(/|$)(.*)
        # ImplementationSpecific å…è®¸è·¯å¾„åŒ¹é…ç±»å‹ç”±å…·ä½“çš„ Ingress æ§åˆ¶å™¨å®ç°å†³å®šã€‚åœ¨ Nginx ä¸­ï¼Œè¿™é€šå¸¸è¡¨ç¤ºä½¿ç”¨æœ€é•¿å‰ç¼€åŒ¹é…ã€‚
        pathType: ImplementationSpecific
        backend:
          service:
            name: my-service
            port:
              number: 80
```

#### Ingress monitor

1. Enabling Metricsï¼š[Prometheus | NGINX Ingress Controller](https://docs.nginx.com/nginx-ingress-controller/logging-and-monitoring/prometheus/)
2. [nginx-prometheus-exporter](https://github.com/nginx/nginx-prometheus-exporter)

### ConfigMap

```sh
kubectl create configmap fortune-config --from-literal=sleep-interval=2 --from-literal=foo=bar --from-literal=bar=baz
kubectl create -f fortune-config.yaml
kubectl create configmap my-config --from-file=config-file.conf
kubectl create configmap my-config --from-file=/path/to/dir

# PATCH
kubectl patch configmap tcp-services --type merge -p '{"data":{"5672": "rabbitmq-system/rabbitmqcluster:5672"}}'

# creating a tls certificate for the ingress
kubectl create secret tls tls-secret-name --cert=tls.cert --key=tls.key
# åŒ…å«è‡ªå®šä¹‰æ–‡ä»¶
kubectl create secret generic generic-name --from-file=https.key --from-file=https.cert --from-file=foo

# Copying Kubernetes Secrets Between Namespaces:
kubectl get secret <secret-name> --namespace=<source-namespace>â€Š -o yaml \
  | sed 's/namespace: <from-namespace>/namespace: <to-namespace>/' \
  | kubectl create -f -

# Update k8s ConfigMap or Secret without deleting the existing one
kubectl create configmap foo --from-file foo.properties -o yaml --dry-run=client | kubectl apply -f -

# åˆ›å»º configMap çš„åŒæ—¶å¢åŠ  label
kubectl label configmap my-config app=grafana env=test
kubectl create configmap --from-file=... --overrides='{"metadata":{"label":"app": "awesomeapp"}}'
kubectl create cm foo -o yaml --dry-run|kubectl label -f- --dry-run -o yaml --local f=b

# ä½¿ç”¨æ–‡ä»¶åˆ›å»ºï¼Œå¹¶å¼•ç”¨å·²æœ‰çš„æ–‡ä»¶ file1.txt
cat <<EOF > configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  file1.txt: |
    $(cat file1.txt)
  file2.txt: |
    $(cat file2.txt)
EOF
```

### cronjob

```sh
# trigger a Kubernetes Scheduled Job manually
kubectl create job --from=cronjob/<cronjob-name> <job-name> -n <namespace-name>

# delete job execution at any time
kubectl delete job <job-name> -n <namespace>
```

### System

`kubectl get events` æŸ¥çœ‹ç›¸å…³äº‹ä»¶

### [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)

- **request**: the scheduler uses this information to decide which node to place the Pod on. The kubelet also reserves at least the request amount of that system resource specifically for that container to use.  It is default to the limits if requests is not set explicitly.
- **limit**: the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set

#### Exceeding the limits

CPU: when a CPU limit is set for a container, the process isnâ€™t given more CPU time than the configured limit.
Memory: When a process tries to allocate memory over its limit, the process is killed (itâ€™s said the container is OOMKilled, where OOM stands for Out Of Memory)

#### pod QoS classes

- BestEffort (the lowest priority)
  1. Itâ€™s assigned to pods that donâ€™t have any requests or limits set at all (in any of their containers)
  2. They will be the first ones killed when memory needs to be freed for other pods.
- Burstable
  In between BestEffort and Guaranteed is the Burstable QoS class. All other pods fall into this class
  When two single-container pods exist, both in the Burstable class, the system will kill the one using more of its requested memory than the other, percentage-wise
- Guaranteed (the highest)
  1. Requests and limits need to be set for both CPU and memory.
  2. They need to be set for each container.
  3. They need to be equal (the limit needs to match the request for each resource in each container).

##### which process gets killed when memory is low

First in line to get killed are pods in the BestEffort class, followed by Burstable pods, and finally Guaranteed pods, which only get killed if system processes need memory.

### Advanced Scheduling

`kubectl taint node node1.k8s node-type=production:NoSchedule` adds a taint with key node-type, value production and the NoSchedule

Three possible effects exist:

- `NoSchedule`, which means pods wonâ€™t be scheduled to the node if they donâ€™t tol- erate the taint.
- `PreferNoSchedule` is a soft version of NoSchedule, meaning the scheduler will try to avoid scheduling the pod to the node, but will schedule it to the node if it canâ€™t schedule it somewhere else.
- `NoExecute`, unlike NoSchedule and PreferNoSchedule that only affect schedul- ing, also affects pods already running on the node. If you add a NoExecute taint to a node, pods that are already running on that node and donâ€™t tolerate the NoExecute taint will be evicted from the node.

```sh
# Remove the taint
# -: The crucial hyphen at the end signals to kubectl that you want to remove this specific taint.
kubectl taint node node1.k8s node-type=production:NoSchedule-

# è®¾ç½® Taint ä¸ºèŠ‚ç‚¹æ·»åŠ ä¸åŒç±»å‹çš„æ±¡ç‚¹ï¼š
# ç¦æ­¢è°ƒåº¦æ–° Pod
kubectl taint nodes node1 key1=value1:NoSchedule
# é©±é€ç°æœ‰ Pod å¹¶ç¦æ­¢è°ƒåº¦æ–° Pod
kubectl taint nodes node1 key1=value1:NoExecute
# å°½é‡é¿å…è°ƒåº¦ï¼ˆè½¯é™åˆ¶ï¼‰
kubectl taint nodes node1 key2=value2:PreferNoSchedule

# åˆ é™¤ Taint é€šè¿‡åœ¨é”®ååæ·»åŠ å‡å·æ¥åˆ é™¤æ±¡ç‚¹ï¼š
kubectl taint nodes node1 key1:NoSchedule-
kubectl taint nodes node1 key1:NoExecute-
kubectl taint nodes node1 key2:PreferNoSchedule-

# èŠ‚ç‚¹ç®¡ç†
kubectl get nodes
kubectl describe node my-node
kubectl cordon my-node                      # æ ‡è®°ä¸å¯è°ƒåº¦
kubectl drain my-node --ignore-daemonsets   # é©±é€Podè¿›è¡Œç»´æŠ¤
kubectl uncordon my-node                    # æ¢å¤è°ƒåº¦
```

### Deploy process

1. build image `docker build -t registry.cn-beijing.aliyuncs.com/namespace/image-name:image-version . -f deploy/Dockerfile`
2. å°†é•œåƒæ¨é€åˆ°Registry `docker push`
3. å¯åŠ¨ pod `kubectl run pod-name -n namespace --image=registry.cn-beijing.aliyuncs.com/namespace/image-name:image-version -it --rm --restart=Never -- bash`

#### Jenkins

[Kubernetes CLI](https://plugins.jenkins.io/kubernetes-cli/) éƒ¨ç½²å¤šä¸ªé›†ç¾¤

```groovy
withKubeConfig([credentialsId: 'k8s_config_prd'
                    ]) {
                  sh "kubectl config view"
                  sh "kubectl get pods"
                }
```

### Volume (Disk)

`Pod The node had condition: [DiskPressure]` è¿™ä¸ªå¼‚å¸¸ä¿¡æ¯ä¼šå‘ç”Ÿåœ¨ Pod åˆ›å»ºçš„æ—¶å€™ï¼Œå¦‚æœ Node æ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´åˆ›å»ºæ–°çš„ Podï¼Œå°±ä¼šæŠ›å‡ºè¿™ä¸ªå¼‚å¸¸

`The node was low on resource: ephemeral-storage` è¿™ä¸ªå¼‚å¸¸ä¿¡æ¯å‘ç”Ÿåœ¨ pod è¿è¡Œè¿‡ç¨‹ä¸­ï¼Œå¦‚æœ pod å†™äº†å¤§é‡çš„æ—¥å¿—ä¿¡æ¯ï¼Œå¯¼è‡´ç£ç›˜è¢«å¤§é‡ä½¿ç”¨ï¼Œå¯¼è‡´æ— æ³•ç»§ç»­è¿è¡Œï¼Œä¼šæŠ›å‡ºè¿™ä¸ªå¼‚å¸¸ã€‚

è®¾ç½® PVC å¤§å° `kubectl patch pvc pvc-name -p '{"spec":{"resources":{"requests":{"storage":"50Gi"}}}}'`

#### Unable to attach or mount volumes - timed out waiting for the condition

[Kubernetes - Kubelet Unable to attach or mount volumes - timed out waiting for the condition - vEducate.co.uk](https://veducate.co.uk/kubelet-unable-attach-volumes/)

The fix is to remove the stale VolumeAttachment. `kubectl delete volumeattachment [volumeattachment_name]`

[å®¹å™¨æœåŠ¡K8Så­˜å‚¨å·æŒ‚è½½å¸¸è§é—®é¢˜-é˜¿é‡Œäº‘å¼€å‘è€…ç¤¾åŒº](https://developer.aliyun.com/article/591884)

ç£ç›˜æŒ‚è½½æ—¥å¿—
tail -f /var/log/alicloud/diskplugin.csi.alibabacloud.com.log | grep d-2zeheqwxc2jrasb0fonj
tail -n 1000 /var/log/messages | grep kubelet |less
kg VolumeAttachment | sort -k 3

### Network

[Mastering Kubernetes Pod-to-Pod Communication: A Comprehensive Guide | by Extio Technology | Medium](https://medium.com/@extio/mastering-kubernetes-pod-to-pod-communication-a-comprehensive-guide-46832b30556b)

Kubernetes å¯¹é›†ç¾¤ç½‘ç»œæœ‰ä»¥ä¸‹è¦æ±‚ï¼š
æ‰€æœ‰çš„ Pod ä¹‹é—´å¯ä»¥åœ¨ä¸ä½¿ç”¨ NAT ç½‘ç»œåœ°å€è½¬æ¢çš„æƒ…å†µä¸‹ç›¸äº’é€šä¿¡ï¼›æ‰€æœ‰çš„ Node ä¹‹é—´å¯ä»¥åœ¨ä¸ä½¿ç”¨ NAT ç½‘ç»œåœ°å€è½¬æ¢çš„æƒ…å†µä¸‹ç›¸äº’é€šä¿¡ï¼›æ¯ä¸ª Pod çœ‹åˆ°çš„è‡ªå·±çš„ IP å’Œå…¶ä»– Pod çœ‹åˆ°çš„ä¸€è‡´ã€‚

#### Kubernetes CNI

[ä»é›¶å¼€å§‹å…¥é—¨ K8s | ç†è§£ CNI å’Œ CNI æ’ä»¶](https://developer.aliyun.com/article/748866)
[kubernetesç½‘ç»œæ¨¡å‹ä¹‹â€œå°è€Œç¾â€flannel](https://zhuanlan.zhihu.com/p/79270447)

CNIï¼Œå®ƒçš„å…¨ç§°æ˜¯ Container Network Interfaceï¼Œå³å®¹å™¨ç½‘ç»œçš„ API æ¥å£ã€‚

å®ƒæ˜¯ K8s ä¸­æ ‡å‡†çš„ä¸€ä¸ªè°ƒç”¨ç½‘ç»œå®ç°çš„æ¥å£ã€‚Kubelet é€šè¿‡è¿™ä¸ªæ ‡å‡†çš„ API æ¥è°ƒç”¨ä¸åŒçš„ç½‘ç»œæ’ä»¶ä»¥å®ç°ä¸åŒçš„ç½‘ç»œé…ç½®æ–¹å¼ï¼Œå®ç°äº†è¿™ä¸ªæ¥å£çš„å°±æ˜¯ CNI æ’ä»¶ï¼Œå®ƒå®ç°äº†ä¸€ç³»åˆ—çš„ CNI API æ¥å£ã€‚å¸¸è§çš„ CNI æ’ä»¶åŒ…æ‹¬ Calicoã€flannelã€Terwayã€Weave Net ä»¥åŠ Contivã€‚

æ’ä»¶è´Ÿè´£ä¸ºæ¥å£é…ç½®å’Œç®¡ç†IPåœ°å€ï¼Œå¹¶ä¸”é€šå¸¸æä¾›ä¸IPç®¡ç†ã€æ¯ä¸ªå®¹å™¨çš„IPåˆ†é…ã€ä»¥åŠå¤šä¸»æœºè¿æ¥ç›¸å…³çš„åŠŸèƒ½ã€‚å®¹å™¨è¿è¡Œæ—¶ä¼šè°ƒç”¨ç½‘ç»œæ’ä»¶ï¼Œä»è€Œåœ¨å®¹å™¨å¯åŠ¨æ—¶åˆ†é…IPåœ°å€å¹¶é…ç½®ç½‘ç»œï¼Œå¹¶åœ¨åˆ é™¤å®¹å™¨æ—¶å†æ¬¡è°ƒç”¨å®ƒä»¥æ¸…ç†è¿™äº›èµ„æºã€‚

K8s é€šè¿‡ CNI é…ç½®æ–‡ä»¶æ¥å†³å®šä½¿ç”¨ä»€ä¹ˆ CNIã€‚åŸºæœ¬çš„ä½¿ç”¨æ–¹æ³•ä¸ºï¼š

1. é¦–å…ˆåœ¨æ¯ä¸ªç»“ç‚¹ä¸Šé…ç½® CNI é…ç½®æ–‡ä»¶(/etc/cni/net.d/xxnet.conf)ï¼Œå…¶ä¸­ xxnet.conf æ˜¯æŸä¸€ä¸ªç½‘ç»œé…ç½®æ–‡ä»¶çš„åç§°ï¼›
2. å®‰è£… CNI é…ç½®æ–‡ä»¶ä¸­æ‰€å¯¹åº”çš„äºŒè¿›åˆ¶æ’ä»¶ï¼›
3. åœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸Šåˆ›å»º Pod ä¹‹åï¼ŒKubelet å°±ä¼šæ ¹æ® CNI é…ç½®æ–‡ä»¶æ‰§è¡Œå‰ä¸¤æ­¥æ‰€å®‰è£…çš„ CNI æ’ä»¶ï¼›
4. ä¸Šæ­¥æ‰§è¡Œå®Œä¹‹åï¼ŒPod çš„ç½‘ç»œå°±é…ç½®å®Œæˆäº†ã€‚

#### Kubernetesé›†ç¾¤é€šä¿¡çš„å®ç°åŸç†

[K8s networkä¹‹å››ï¼šKubernetesé›†ç¾¤é€šä¿¡çš„å®ç°åŸç† | Mr.Muzi](https://marcuseddie.github.io/2021/K8s-Network-Architecture-section-four.html)

#### Kubernetesé›†ç¾¤Podå’ŒServiceä¹‹é—´é€šä¿¡çš„å®ç°åŸç†

[K8s networkä¹‹äº”ï¼šKubernetesé›†ç¾¤Podå’ŒServiceä¹‹é—´é€šä¿¡çš„å®ç°åŸç† | Mr.Muzi](https://marcuseddie.github.io/2021/K8s-Network-Architecture-section-five.html)

iptablesä»£ç†æ¨¡å‹

iptablesä»Kubernetes v1.2ç‰ˆæœ¬å¼€å§‹ç§°ä¸ºkube-proxyçš„é»˜è®¤æ¨¡å¼ï¼Œç›´åˆ°åœ¨v1.12ç‰ˆæœ¬ä¸­è¢«IPVSå–ä»£è€Œæˆä¸ºæ–°çš„kube-proxyé»˜è®¤å·¥ä½œæ¨¡å¼ã€‚åœ¨è¯¥æ¨¡å¼ä¸­ï¼Œkube-proxyä¿®æ”¹äº†iptablesä¸­çš„filterå’Œnatè¡¨ï¼ŒåŒæ—¶åˆå¯¹iptablesçš„é“¾è¿›è¡Œäº†æ‰©å……ï¼Œè‡ªå®šä¹‰äº†`KUBE-SERVICESï¼ŒKUBE-SVC-<HASH>ï¼ŒKUBE-SEP-<HASH>ï¼ŒKUBE-NODEPORTSï¼ŒKUBE-FW-<HASH>ï¼ŒKUBE-XLB-<HASH>ï¼ŒKUBE-POSTROUTINGï¼ŒKUBE-MARK-MASQå’ŒKUBE-MARK-DROP`ä¹ä¸ªé“¾æ¡ï¼Œé€šè¿‡åœ¨`KUBE-SERVICES`é“¾ä¸­åŠ å…¥æ¯ä¸ªServiceçš„ClusterIPå’Œç«¯å£çš„åŒ¹é…è§„åˆ™æ¥å®Œæˆæµé‡çš„åŒ¹é…å’Œé‡å®šå‘å·¥ä½œã€‚Kubernetesè‡ªå®šä¹‰çš„é“¾æ¡ä¸iptablesåŸç”Ÿé“¾æ¡çš„è°ƒç”¨å…³ç³»å¦‚å›¾ 4 æ‰€ç¤º

![å›¾4 Kubernetes è‡ªå®šä¹‰é“¾ä¸ä¸iptablesè°ƒç”¨é“¾çš„å…³ç³»](image/Relationship-between-Kube-customed-chains-and-iptables-demo.png)

Cluster IP
â€ƒâ€ƒå½“é›†ç¾¤å†…Podè®¿é—®Serviceçš„Cluster IPæ—¶ï¼ŒæŠ¥æ–‡ä¼šé€šè¿‡iptablesçš„OUTPUTé“¾è¿›å…¥Kubernetesçš„è‡ªå®šä¹‰é“¾ã€‚å‡è®¾å½“å‰é›†ç¾¤ä¸­æœ‰ä¸€ä¸ªService Aï¼ŒåŒæ—¶æœ‰ä¸‰ä¸ªåç«¯Podç”¨æ¥æä¾›æœåŠ¡Service Aï¼Œkube-proxyé‡‡ç”¨éšæœºè´Ÿè½½å‡è¡¡ç®—æ³•æ¥é€‰æ‹©Podï¼Œé’ˆå¯¹Cluster IPçš„å¤„ç†æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![å›¾-6 Serviceåœ¨Cluster IPæ¨¡å¼ä¸‹æŠ¥æ–‡å¤„ç†è¿‡ç¨‹](image/K8-svc-iptables-clusterIP-mode-flow.png)

Note: æŸäº›é“¾æ¡çš„åå­—ä¸­å«æœ‰ <HASH>å­—æ ·ï¼Œè¿™æ˜¯è¿ç”¨SHA256ç®—æ³•å¯¹â€œnamespace + name + portname+åè®®åâ€ç”Ÿæˆå“ˆå¸Œå€¼ï¼Œç„¶åé€šè¿‡base32å¯¹è¯¥å“ˆå¸Œå€¼ç¼–ç ï¼Œæœ€åå–ç¼–ç å€¼çš„å‰16ä½çš„å€¼ã€‚

1. æŠ¥æ–‡ä¼šé¦–å…ˆè¿›å…¥KUBE-SERVICESé“¾æ¡ã€‚KUBE-SERVICEé’ˆå¯¹æ¯ä¸ªServiceä¼šäº§ç”Ÿä¸¤æ¡åŒ¹é…è§„åˆ™ï¼Œè§„åˆ™ï¼ˆ1ï¼‰è¡¨ç¤ºå¦‚æœæŠ¥æ–‡çš„æºåœ°å€ä¸æ˜¯é›†ç¾¤å†…IPåœ°å€ï¼ŒåŒæ—¶ï¼ŒæŠ¥æ–‡åŒ¹é…äº†è¯·æ±‚Serviceçš„åè®®å’Œç«¯å£ï¼Œé‚£ä¹ˆå°±è·³è½¬åˆ°ï¼ˆ-jï¼‰KUBE-MARK-MASQé“¾æ¡ï¼Œåœ¨æŠ¥æ–‡ä¸­åŠ å…¥ä¸€ä¸ªç‰¹æ®Šçš„é˜²ç«å¢™æ ‡è¯†ï¼Œæ‰“ä¸Šè¿™ä¸ªæ ‡è¯†çš„æŠ¥æ–‡ä¼šåœ¨POSTROUTINGé˜¶æ®µæ‰§è¡ŒSNAT(Source Network Address Translation)ã€‚å¦‚æœç¡®å®å‘½ä¸­äº†è§„åˆ™ï¼ˆ1ï¼‰ï¼Œé‚£ä¹ˆåœ¨æ‰“å®Œæ ‡è®°åä¼šç»§ç»­æ£€æŸ¥è§„åˆ™ï¼ˆ2ï¼‰ï¼Œè§„åˆ™ï¼ˆ2ï¼‰ä¼šå°†æŠ¥æ–‡å¸¦å…¥ä¸‹ä¸€ä¸ªé“¾æ¡ KUBE-SVC-<HASH>ã€‚
2. KUBE-SVC-<HASH>åŒ…å«äº†å½“å‰æä¾›Serviceçš„åç«¯Podã€è´Ÿè½½å‡è¡¡æ¨¡å¼ç­‰æ¶ˆæ¯ã€‚kube-proxyé»˜è®¤é‡‡ç”¨çš„éšæœºè´Ÿè½½ç®—æ³•ï¼Œå› æ­¤åœ¨è¿™ç§ç®—æ³•ä¸‹ä¼šä¸ºæ¯ä¸ªPodåˆ†é…ä¸€ä¸ªå‘½ä¸­æ¦‚ç‡ã€‚åœ¨å›¾-6ä¸­ï¼Œä¸‰ä¸ªPodè¢«å‘½ä¸­çš„æ¦‚ç‡éƒ½æ˜¯ä¸‰åˆ†ä¹‹ä¸€ã€‚å½“é€‰ä¸­ä¸€ä¸ªPodåï¼Œå°±ä¼šè·³è½¬åˆ°å’ŒPodç›¸å¯¹åº”çš„KUBE-SEP-<HASH>ä¸Šã€‚
3. æ¯ä¸ªKUBE-SEP-<HASH>å’Œä¸€ä¸ªPodç›¸å¯¹åº”ï¼Œä¸”æ¯ä¸ªKUBE-SEP-<HASH>å‡æœ‰ä¸¤æ¡è§„åˆ™ã€‚è§„åˆ™ï¼ˆ2ï¼‰è¡¨ç¤ºå¯¹è¯·æ±‚åšDNATï¼Œå°†è¯·æ±‚çš„ç›®çš„åœ°å€ç”±åŸæ¥çš„ClusterIPï¼šPortè½¬æ¢æˆPod_IPï¼šPortã€‚è¿™æ ·å°±å°†Podè®¿é—®Serviceå˜æˆäº†Podå’ŒPodä¹‹é—´çš„è®¿é—®ã€‚è§„åˆ™ï¼ˆ1ï¼‰çš„ç›®çš„æ˜¯ä¸ºäº†åº”å¯¹Hairpinning å‘å¤¹é—®é¢˜è€Œè®¾è®¡çš„ï¼šService Açš„åç«¯Podä¸­æœ‰å¯èƒ½ä¼šæœ‰æŸä¸ªPodè®¿é—®Service Aï¼Œç„¶åç»è¿‡iptablesæ—¶åˆæ°å¥½é€‰ä¸­äº†è‡ªå·±ä½œä¸ºæœåŠ¡çš„æä¾›æ–¹ã€‚æ¢å¥è¯è¯´ï¼ŒPodè¦ä¸ºè‡ªå·±å‘å‡ºå»çš„æœåŠ¡è¯·æ±‚åšå‡ºå“åº”ã€‚åœ¨Kubernetesä¸­è¿™æ ·ä¼šé€ æˆè®¿é—®å¤±è´¥ï¼Œå¦‚æœå½“å‡ºç°è¿™ç§åœºæ™¯æ—¶å°±è·³è½¬åˆ°KUBE-MARK-MASQé“¾æ¡æ‰§è¡ŒSNATï¼Œå°†è¯·æ±‚çš„æºåœ°å€ç”±Podè‡ªèº«å˜æˆèŠ‚ç‚¹çš„node IPï¼Œè¿™æ ·å°±åˆå˜æˆäº†æ­£å¸¸çš„æœåŠ¡è¯·æ±‚å’Œå“åº”æ¨¡å¼ã€‚å¦‚å›¾7æ‰€ç¤ºï¼Œå·¦è¾¹æ˜¯æ²¡æœ‰åšSNATçš„åœºæ™¯ï¼ŒPod Aæ”¶åˆ°äº†ä¸€ä¸ªè‡ªå·±å‘å‡ºçš„æœåŠ¡è¯·æ±‚ï¼Œè¯·æ±‚çš„æºå’Œç›®çš„åœ°å€éƒ½æ˜¯è‡ªå·±ï¼Œå½“å‘é€å“åº”ç»™è‡ªå·±æ—¶ä¼šå¯¼è‡´å¤±è´¥ã€‚å³è¾¹æ˜¯å€ŸåŠ©SNATè§£å†³Hairpinningé—®é¢˜çš„åœºæ™¯ï¼ŒPod Aè®¿é—®è‡ªå·±æ‰€å±æœåŠ¡çš„è¯·æ±‚åˆ°è¾¾Linuxå†…æ ¸æ—¶ä¼šé€šè¿‡SNATå°†æºåœ°å€ç”±Pod Açš„IPå˜æˆèŠ‚ç‚¹çš„Node IPã€‚å½“Pod Aå‘é€å“åº”æŠ¥æ–‡æ—¶ï¼ŒæŠ¥æ–‡å…ˆå‘é€ç»™Node IPï¼Œç„¶ååœ¨Linuxå†…æ ¸ä¸­å†æ¬¡è¿›è¡ŒNATï¼Œå°†æºIPç”±Pod Açš„IPæ”¹æˆServiceçš„IPï¼Œç›®çš„IPç”±NodeèŠ‚ç‚¹çš„IPæ”¹ä¸ºPod Açš„IPï¼Œè¿™æ ·å°±å¯ä»¥æ­£å¸¸å·¥ä½œäº†ã€‚
  ![Hairpinningé—®é¢˜åŠå…¶è§£å†³æ–¹æ³•](image/K8s-iptables-Hairpinning-demo.png)
  å›¾ - 7 Hairpinningé—®é¢˜åŠå…¶è§£å†³æ–¹æ³•

4. æ‰§è¡Œå®ŒDNATåï¼Œä¼šè·³è½¬åˆ°POSTROUTINGé“¾æ¡ã€‚POSTROUTINGä¼šæ— æ¡ä»¶è·³è½¬åˆ°KUBE-POSTROUTINGé“¾æ¡ï¼Œè¿™ä¸ªé“¾æ¡ä¼šæ£€æŸ¥æŠ¥æ–‡æ˜¯å¦æœ‰è·³è½¬åˆ°KUBE-MARK-MASQé“¾æ¡è¢«æ‰“ä¸Šé˜²ç«å¢™æ ‡è¯†ï¼Œå¦‚æœæœ‰çš„è¯å°±ä¼šæ‰§è¡ŒSNATï¼Œå°†æŠ¥æ–‡çš„æºåœ°å€å˜ä¸ºèŠ‚ç‚¹çš„node IPã€‚
5. æœ€åç”±POSTROUTINGå°†æŠ¥æ–‡å‘å‡ºåè®®æ ˆã€‚

NOTE: SNATå’ŒMASQUERADçš„åŒºåˆ«

- SNATæ˜¯æŒ‡åœ¨æ•°æ®åŒ…ä»ç½‘å¡å‘é€å‡ºå»çš„æ—¶å€™ï¼ŒæŠŠæ•°æ®åŒ…ä¸­çš„æºåœ°å€éƒ¨åˆ†æ›¿æ¢ä¸ºæŒ‡å®šçš„IPï¼Œè¿™æ ·ï¼Œæ¥æ”¶æ–¹å°±è®¤ä¸ºæ•°æ®åŒ…çš„æ¥æºæ˜¯è¢«æ›¿æ¢çš„é‚£ä¸ªæŒ‡å®šIPçš„ä¸»æœºã€‚
- MASQUERADEæ˜¯SNATçš„ä¸€ä¸ªç‰¹ä¾‹ã€‚MASQUERADEæ˜¯ç”¨å‘é€æ•°æ®çš„ç½‘å¡ä¸Šçš„IPæ¥æ›¿æ¢æºIPï¼Œå› æ­¤ï¼Œå¯¹äºé‚£äº›IPä¸å›ºå®šçš„åœºåˆï¼Œæ¯”å¦‚é€šè¿‡DHCPåˆ†é…IPçš„æƒ…å†µä¸‹ï¼Œå°±å¾—ç”¨MASQUERADEã€‚

##### åŒä¸€ä¸ª Node èŠ‚ç‚¹å†…çš„ Pod ä¸èƒ½é€šè¿‡ Service äº’è®¿

ä¿®æ”¹é›†ç¾¤kube proxy é…ç½® masqueradeAll true
[Kubernetes åŒä¸€ä¸ª Node èŠ‚ç‚¹å†…çš„ Pod ä¸èƒ½é€šè¿‡ Service äº’è®¿ - å“ˆå¸Œ](https://www.haxi.cc/archives/Kubernetes-åŒä¸€ä¸ª-Node-èŠ‚ç‚¹å†…çš„-Pod-ä¸èƒ½é€šè¿‡-Service-äº’è®¿.html)
[Kubernetes åŒä¸€ä¸ª Node èŠ‚ç‚¹å†…çš„ Pod ä¸èƒ½é€šè¿‡ Service äº’è®¿ - é¢å£è€…çš„é€»è¾‘ - åšå®¢å›­](https://www.cnblogs.com/longgor/p/13588191.html)

[Debug Services | Kubernetes](https://kubernetes.io/docs/tasks/debug/debug-application/debug-service)

[Virtual IPs and Service Proxies | Kubernetes](https://kubernetes.io/docs/reference/networking/virtual-ips/)
[IP Masquerade Agent User Guide | Kubernetes](https://kubernetes.io/docs/tasks/administer-cluster/ip-masq-agent/)

[kubernetes - Pods running on the same node can't access to each other through service - Stack Overflow](https://stackoverflow.com/questions/64073696/pods-running-on-the-same-node-cant-access-to-each-other-through-service/78910247#78910247)

åŸå› ï¼škube-proxyé€šè¿‡åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šåˆ›å»ºç›¸åŒçš„ipvsè§„åˆ™ï¼ˆå…³é—­rapï¼‰ï¼Œå½“podè®¿é—®é›†ç¾¤å†…svcï¼ˆvipï¼‰æ—¶ï¼Œè¯·æ±‚ä¼šè¢«å½“å‰èŠ‚ç‚¹vipæ¥å—ï¼Œæ­¤æ—¶ï¼Œipvsä¼šè¿›è¡ŒDNATæ“ä½œï¼Œè€Œåœ¨å›æŠ¥æ—¶ï¼Œä¸¤ä¸ªpodå¤„äºåŒä¸€ä¸ªveth-partçš„ä¸€é¢ï¼Œæ­¤æ—¶æµé‡å¹¶ä¸ä¼šèµ°ç½‘å…³ï¼Œæ‰€ä»¥å›æŠ¥çš„æ—¶å€™æºipå’Œç›®çš„ipéƒ½æ˜¯ä¸¤ä¸ªpodçš„ipï¼Œä½†æ˜¯åœ¨è¯·æ±‚å‘é€æ—¶ï¼Œç›®çš„ipä¸ºvipï¼Œæ­¤æ—¶ä¼šä¸¢å¼ƒæ‰è¯·æ±‚ã€‚

using flannel as CNI on Kubernetes v1.30.1 and it turned out that flannel needs masquerade to be set true while kube-proxy default value has masqueradeAll: false. Changing it to true and restarting kube-proxy pods solved the problem

the steps:

```sh
# set masqueradeAll: true
kubectl -n kube-system edit cm kube-proxy
# to restart all kube proxy pods
kubectl -n kube-system delete pod -l k8s-app=kube-proxy
```

#### Flannel

[ä» Flannel å­¦ä¹  Kubernetes overlay ç½‘ç»œ](https://atbug.com/cross-node-traffic-on-flannel-vxlan-network/)

```sh
# IP å‘½ä»¤

# ip neighbour show | awk '$3=="flannel.1"{print $0}'
10.244.2.0 dev flannel.1 lladdr 5e:5f:7e:bd:65:ea PERMANENT
10.244.0.0 dev flannel.1 lladdr 76:7d:9c:b5:29:9d PERMANENT

# bridge fdb show flannel.1 |awk '$3=="flannel.1"{print $0}'
26:22:89:97:04:e8 dev flannel.1 dst 192.168.102.103 self permanent    # è½¬å‘è‡³k8s-node2èŠ‚ç‚¹
76:7d:9c:b5:29:9d dev flannel.1 dst 192.168.102.101 self permanent    # è½¬å‘è‡³k8s-masterèŠ‚ç‚¹
```

#### kubectl netshoot

[nicolaka/netshoot: a Docker + Kubernetes network trouble-shooting swiss-army container](https://github.com/nicolaka/netshoot)

```sh
kubectl netshoot debug podName --image-name nicolaka/netshoot --image-tag v0.13
# spin up a throwaway pod for troubleshooting
kubectl netshoot run tmp-shell --image-name nicolaka/netshoot --image-tag v0.13

# debug using an ephemeral container in an existing pod
kubectl netshoot debug my-existing-pod

# create a debug session on a node
kubectl netshoot debug node/my-node
```

#### é˜¿é‡Œäº‘ACKé›†ç¾¤ä¸­çš„ç½‘ç»œé—®é¢˜

[é˜¿é‡Œäº‘ACKé›†ç¾¤ä¸­çš„ç½‘ç»œé—®é¢˜](https://itopic.org/ack-network-issue.html)
[pod æ— æ³•è®¿é—®é›†ç¾¤ ingress slb ç»‘å®šçš„å…¬ç½‘åŸŸå | Blog](https://zijin-m.github.io/Blog/problems/k8s/externalTrafficPolicy-local.html)

#### é˜¿é‡Œäº‘Kubernetesæ‰˜ç®¡ç‰ˆå¼€å¯ hairpin æ¨¡å¼

æ®å®¢æœå›å¤ç›®å‰(2020-11-20)æ²¡æœ‰ç®€å•çš„é…ç½®æ–¹å¼, ä½†å‘ç°ä¸‹é¢çš„æ–¹å¼æš‚æ—¶æœ‰æ•ˆæœ:

1. ç¡®è®¤ç½‘ç»œæ’ä»¶ç±»å‹æ˜¯ csi
`ps aux | grep kubelet | grep /usr/bin/kubelet | grep network-plugin`

2. æ‰¾åˆ°é…ç½®æ–‡ä»¶ä½ç½® `ls /etc/cni/net.d/*flannel.conf`
åœ¨æ¯å° node æœºå™¨ä¸Šçš„é…ç½®æ–‡ä»¶ /etc/cni/net.d/10-flannel.conf å¢åŠ   "hairpinMode": true

```yaml
{
  "name": "cb0",
  "cniVersion":"0.3.0",
  "type": "flannel",
  "delegate": {
    # "hairpinMode": true,
    "isDefaultGateway": true
  }
}
```

```sh
# scp 10-flannel.conf host1:/etc/cni/net.d/10-flannel.conf
for host in host1 host2; do echo $host; scp 10-flannel.conf $host:/etc/cni/net.d/10-flannel.conf; done

ç”Ÿæˆ pod åä¿®æ”¹æ–¹å¼
for intf in /sys/devices/virtual/net/cni0/brif/*; do echo 1 > $intf/hairpin_mode; done

# éªŒè¯
for intf in /sys/devices/virtual/net/cni0/brif/*; do echo "$intf"; cat $intf/hairpin_mode; done
```

#### ç½‘ç»œé—®é¢˜æ’æŸ¥

1. æŸ¥çœ‹ route -n è·¯ç”±è¡¨
2. æŸ¥çœ‹ arp è¡¨ arp -a
3. æ£€æŸ¥éš§é“çŠ¶æ€

## Kubernetes internal

### Components of the Control Plane

- The etcd distributed persistent storage
- The API server
- The Scheduler
- The Controller Manager

These components store and manage the state of the cluster, but they arenâ€™t what runs the application containers.

### Components running on the worker nodes

The task of running your containers is up to the components running on each worker node:

- The Kubelet
- The Kubernetes Service Proxy (kube-proxy)
- The Container Runtime (Docker, rkt, or others)

## helm

[Helm | Installing Helm](https://helm.sh/docs/intro/install/)

[Harbor docs | Working with OCI Helm Charts](https://goharbor.io/docs/2.12.0/working-with-projects/working-with-oci/working-with-helm-oci-charts/)

[What are your best practices deploying helm charts? : r/kubernetes](https://www.reddit.com/r/kubernetes/comments/1jn7lwo/what_are_your_best_practices_deploying_helm_charts)

### helm Chart Management

```sh
helm create <name>         # Creates a chart directory along with the common files and directories used in a chart.
helm package <chart-path>               # Packages a chart into a versioned chart archive file.
helm lint <chart>                       # Run tests to examine a chart and identify possible issues:
helm show all <chart>                   # Inspect a chart and list its contents:
helm show values <chart>                # Displays the contents of the values.yaml file
helm template <chart>           # å¾—åˆ°ç­‰æ•ˆçš„ k8s manifests
helm pull <chart>                       # Download/pull chart
helm pull <chart> --untar=true          # If set to true, will untar the chart after downloading it
helm pull <chart> --verify              # Verify the package before using it
helm pull <chart> --version <number>    # Default-latest is used, specify a version constraint for the chart version to use
helm dependency list <chart>            # Display a list of a chartâ€™s dependencies:
```

### helm push chart

[Harbor docs | Managing Helm Charts](https://goharbor.io/docs/2.7.0/working-with-projects/working-with-images/managing-helm-charts/)

Push Charts to the Repository Server with the CLI

```sh
# As an alternative, you can also upload charts via the CLI. It is not supported by the native helm CLI. A plugin from the community should be installed before pushing. Run helm plugin install to install the push plugin first.
helm plugin install https://github.com/chartmuseum/helm-push
helm push --ca-file=ca.crt --username=admin --password=passw0rd chart_repo/hello-helm-0.1.0.tgz myrepo

# if your helm version is >= v3.7.0, please use the following command
helm cm-push --ca-file=ca.crt --username=admin --password=passw0rd chart_repo/hello-helm-0.1.0.tgz myrepo

helm registry login https://harbor.com/harbor -u 'robot' -p '3wfBwWv'

helm push nfs-subdir-external-provisioner-4.0.18.tgz oci://harbor.com/k8s
```

### Install and Uninstall Apps

```sh
helm install <name> <chart>                           # Install the chart with a name
helm install <name> <chart> --namespace <namespace>   # Install the chart in a specific namespace
helm install <name> <chart> --set key1=val1,key2=val2 # Set values on the command line (can specify multiple or separate values with commas)
helm install <name> <chart> --values <yaml-file/url>  # Install the chart with your specified values
helm install <name> <chart> --dry-run --debug         # Run a test installation to validate chart (p)
helm install <name> <chart> --verify                  # Verify the package before using it
helm install <name> <chart> --dependency-update       # update dependencies if they are missing before installing the chart
helm uninstall <name>                                 # Uninstall a release
```

There are two ways to pass configuration data during install

- `--values` (or `-f`): Specify a YAML file with overrides. This can be specified multiple times and the rightmost file will take precedence
- `--set`: Specify overrides on the command line.

### Perform App Upgrade and Rollback

```sh
helm upgrade <release> <chart>                            # Upgrade a release
helm upgrade <release> <chart> --atomic                   # If set, upgrade process rolls back changes made in case of failed upgrade.
helm upgrade <release> <chart> --dependency-update        # update dependencies if they are missing before installing the chart
helm upgrade <release> <chart> --version <version_number> # specify a version constraint for the chart version to use
helm upgrade <release> <chart> --values                   # specify values in a YAML file or a URL (can specify multiple)
helm upgrade <release> <chart> --set key1=val1,key2=val2  # Set values on the command line (can specify multiple or separate valuese)
helm upgrade <release> <chart> --force                    # Force resource updates through a replacement strategy
helm rollback <release> <revision>                        # Roll back a release to a specific revision
helm rollback <release> <revision>  --cleanup-on-fail     # Allow deletion of new resources created in this rollback when rollback fails
```

### List, Add, Remove, and Update Repositories

```sh
helm repo add <repo-name> <url>   # Add a repository from the internet:
helm repo list                    # List added chart repositories
helm repo update                  # Update information of available charts locally from chart repositories
helm repo remove <repo_name>      # Remove one or more chart repositories
helm repo index <DIR>             # Read the current directory and generate an index file based on the charts found.
helm repo index <DIR> --merge     # Merge the generated index with an existing index file
helm search repo <keyword>        # Search repositories for a keyword in charts
helm search hub <keyword>         # Search for charts in the Artifact Hub or your own hub instance
```

### ä¸‹è½½ dependency ç¦»çº¿å®‰è£…

[Helm | Helm Dependency](https://helm.sh/docs/helm/helm_dependency/)

Starting from 2.2.0, repository can be defined as the path to the directory of the dependency charts stored locally. The path should start with a prefix of "file://". For example,

```yaml
# Chart.yaml
dependencies:
- name: nginx
  version: "1.2.3"
  repository: "file://../dependency_chart/nginx"
```

### Helm Release monitoring

```sh
helm list                       # Lists all of the releases for a specified namespace, uses current namespace context if namespace not specified
helm list --all                 # Show all releases without any filter applied, can use -a
helm list --all-namespaces      # List releases across all namespaces, we can use -A
helm list -l key1=value1,key2=value2 # Selector (label query) to filter on, supports '=', '==', and '!='
helm list --date                # Sort by release date
helm list --deployed            # Show deployed releases. If no other is specified, this will be automatically enabled
helm list --pending             # Show pending releases
helm list --failed              # Show failed releases
helm list --uninstalled         # Show uninstalled releases (if 'helm uninstall --keep-history' was used)
helm list --superseded          # Show superseded releases
helm list -o yaml               # Prints the output in the specified format. Allowed values: table, json, yaml (default table)
helm status <release>           # This command shows the status of a named release.
helm status <release> --revision <number>   # if set, display the status of the named release with revision
helm history <release>          # Historical revisions for a given release.
helm env                        # Env prints out all the environment information in use by Helm.
```

### Download Release Information

```sh
helm get all <release>      # A human readable collection of information about the notes, hooks, supplied values, and generated manifest file of the given release.
helm get hooks <release>    # This command downloads hooks for a given release. Hooks are formatted in YAML and separated by the YAML '---\n' separator.
helm get manifest <release> # A manifest is a YAML-encoded representation of the Kubernetes resources that were generated from this release's chart(s). If a chart is dependent on other charts, those resources will also be included in the manifest.
helm get notes <release>    # Shows notes provided by the chart of a named release.
helm get values <release>   # Downloads a values file for a given release. use -o to format output
```

### Plugin Management

```sh
helm plugin install <path/url1>     # Install plugins
helm plugin list                    # View a list of all installed plugins
helm plugin update <plugin>         # Update plugins
helm plugin uninstall <plugin>      # Uninstall a plugin


# é€šè¿‡æ’ä»¶è·å– image list
helm plugin install https://github.com/nikhilsbhat/helm-images
helm images get <release>  # get image
helm images get /path/to/your/helm-chart -f /path/to/your/helm-chart/values.yaml
```

### helm ä½¿ç”¨ä¾‹å­

```sh
/usr/local/bin/helm

helm repo add ali-incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator
helm repo add ali-stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
#æ·»åŠ é˜¿é‡Œäº‘çš„ chart ä»“åº“
helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
# æ·»åŠ ç§æœ‰habor ä»“åº“
helm repo add --username=admin --password=xxxxxxxx my_harbor https://xxxxx:8443/chartrepo/library

#æŸ¥çœ‹é…ç½®çš„ chart ä»“åº“æœ‰å“ªäº›
helm repo list
#åˆ é™¤ chart ä»“åº“åœ°å€
helm repo remove aliyun
#ä»æŒ‡å®š chart ä»“åº“åœ°å€æœç´¢ chart
helm search repo aliyun
helm search repo gitlab-ce
# æŸ¥çœ‹ chart ä¿¡æ¯
helm show chart aliyun/memcached
# To see all configurable options with detailed comments
helm show values prometheus-community/kube-prometheus-stack
# Displays the contents of the values.yaml file. get a simple idea of the features of this chart
helm show chart prometheus-community/kube-prometheus-stack
# Inspect a chart and list its contents
helm show all prometheus-community/kube-prometheus-stack

helm fetch ali-stable/gitlab-ce

# Install charts
# helm install command can install from several sources
# A chart repository
helm install myrelease oci://<registry url>/<project>/<chart name> --version <version>
# A local chart archive
helm install foo foo-0.1.1.tgz
# An unpacked chart directory
helm install foo path/to/foo
# A full URL
helm install foo https://example.com/charts/foo-1.2.3.tgz

# å¯¼å‡ºé»˜è®¤å€¼æ–‡ä»¶ values.yaml
helm inspect values prometheus-community/kube-prometheus-stack > values.yaml
# ä½¿ç”¨é…ç½®æ–‡ä»¶ä¿®æ”¹é»˜è®¤å€¼
helm install kubernetes-dashboard /data/kubernetes-dashboard-7.5.tgz -f /data/value.yaml --create-namespace --namespace kubernetes-dashboard

helm uninstall gitlab

# To keep track of a release's state, or to re-read configuration information
helm status happy-panda

helm registry login <registry url>
```

ä¸‹è½½ chart åŒ…åˆ°æœ¬åœ°

```sh
# ä¸‹è½½ chart åŒ…åˆ°æœ¬åœ°
[root@master1 ~]# helm pull aliyun/memcached
[root@master1 ~]# tar zxvf memcached-2.0.1.tgz
[root@master1 ~]# cd memcached
[root@master1 memcached]# ls
Chart.yaml README.md templates values.yaml
# Chart.yamlï¼š chart çš„åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç‰ˆæœ¬åå­—ä¹‹ç±»
# templatesï¼šå­˜æ”¾ k8s çš„éƒ¨ç½²èµ„æºæ¨¡æ¿ï¼Œé€šè¿‡æ¸²æŸ“å˜é‡å¾—åˆ°éƒ¨ç½²æ–‡ä»¶
# values.yamlï¼šå­˜æ”¾å…¨å±€å˜é‡ï¼Œtemplates ä¸‹çš„æ–‡ä»¶å¯ä»¥è°ƒç”¨

[root@xianchaomaster1 memcached]# cd templates/
[root@xianchaomaster1 templates]# ls
_helpers.tpl NOTES.txt pdb.yaml statefulset.yaml svc.yaml
# _helpers.tpl å­˜æ”¾èƒ½å¤Ÿå¤ç”¨çš„æ¨¡æ¿
# NOTES.txt ä¸ºç”¨æˆ·æä¾›ä¸€ä¸ªå…³äº chart éƒ¨ç½²åä½¿ç”¨è¯´æ˜çš„æ–‡ä»¶
```

helm package

```sh
# æ‰§è¡Œhelm package -d <chartæ‰€åœ¨ç›®å½•> <chartåç§°>,ä¼šç”Ÿæˆä¸€ä¸ªå‹ç¼©åŒ…ï¼Œåå­—ä¸º<chartåç§°>-<chartæ–‡ä»¶å†…çš„version>.tgz
# ä¸‹é¢çš„å‘½ä»¤åœ¨ç›®å½•appchart-uat-cdä¼šç”Ÿæˆappchart-uat-cd-1.0.0.tgz
helm package -d appchart-uat-cd appchart-uat-cd
```

ä¸Šä¼  chart åŒ…åˆ°ä»“åº“

```sh
helm push mychart-1.0.0.tgz my-repo

# æ›´æ–°å­˜å‚¨åº“: ä¸Šä¼ æ–°çš„ Helm Chart åï¼Œä½ éœ€è¦æ›´æ–° Helm å­˜å‚¨åº“çš„ç´¢å¼•æ–‡ä»¶ã€‚
# ç”±äº Harbor ä¸»è¦ç”¨äºå®¹å™¨é•œåƒç®¡ç†ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ Helm Chart ä»“åº“ï¼Œå› æ­¤åœ¨æ›´æ–° Helm Chart æ—¶éœ€è¦æ‰‹åŠ¨é‡æ–°ä¸Šä¼ å¹¶æ›´æ–°ç´¢å¼•æ–‡ä»¶ã€‚
helm repo update

```

### helm chart æ¬è¿

[containers/skopeo: Work with remote images registries - retrieving information, images, signing content](https://github.com/containers/skopeo)

[ã€äº‘åŸç”Ÿå®ç”¨æŠ€å·§ã€‘ä½¿ç”¨ skopeo æ‰¹é‡åŒæ­¥ helm chart ä¾èµ–é•œåƒ-è…¾è®¯äº‘å¼€å‘è€…ç¤¾åŒº-è…¾è®¯äº‘](https://cloud.tencent.com/developer/article/2065531)
[sir5kong/helm-charts-hub: Kubernetes Helm Charts é•œåƒç«™ï¼Œä¸­å›½åŒºç½‘ç»œåŠ é€Ÿ](https://github.com/sir5kong/helm-charts-hub)

```sh
# [ç¦»çº¿é›†ç¾¤ | JuiceFS Document Center](https://juicefs.com/docs/zh/csi/administration/offline/)
# è·å–éœ€è¦æ¬è¿çš„é•œåƒåˆ—è¡¨
helm template kube-prometheus-stack-62.7.0.tgz | grep -E ' *image:' | sed 's/ *image: //' | sort | uniq > images.txt
```

### Migration helm chart

```sh
#!/bin/bash

# è„šæœ¬éœ€è¦ä¿®æ”¹ï¼Œæ‹‰å–çš„é•œåƒä¸æ­£ç¡®
# Set the private registry URL
private_registry_url="your_private_registry_url"

# Set the chart directory
chart_dir="charts/my-chart"

# Function to modify Chart.yaml and values.yaml
modify_chart() {
  chart_name=$(grep -m 1 "name:" $chart_dir/Chart.yaml | cut -d ":" -f 2 | sed 's/ //g')
  sed -i "s/repository:.*/repository: $private_registry_url\/$chart_name/" $chart_dir/values.yaml
  sed -i "s/registry:.*/registry: $private_registry_url/" $chart_dir/Chart.yaml
}

# Function to pull and push Docker images
pull_and_push_images() {
  chart_name=$(grep -m 1 "name:" $chart_dir/Chart.yaml | cut -d ":" -f 2 | sed 's/ //g')
  helm dependency update $chart_dir
  helm dep build $chart_dir
  for image in $(helm dep list $chart_dir | awk '{print $2}'); do
    docker pull $image
    docker tag $image $private_registry_url/$chart_name/$image
    docker push $private_registry_url/$chart_name/$image
  done
}

# Loop through multiple charts (optional)
for chart in charts/*; do
  cd $chart
  modify_chart
  pull_and_push_images
  cd ..
done

# Modify a single chart (optional)
# modify_chart
# pull_and_push_images
```

### é•œåƒåŠ é€Ÿè„šæœ¬ ä¿®æ”¹é•œåƒä»“åº“åœ°å€

æ ¸å¿ƒä»£ç 

```sh
# è‡ªåŠ¨åŒ–é•œåƒåœ°å€æ›¿æ¢è„šæœ¬ demo
find ./ -typeÂ f -nameÂ "*.yaml"Â -execÂ sed -i \
Â  Â  -eÂ 's|registry.k8s.io|m.daocloud.io/registry.k8s.io|g'Â \
Â  Â  -eÂ 's|quay.io|m.daocloud.io/quay.io|g'Â \
Â  Â  -eÂ 's|docker.io|m.daocloud.io/docker.io|g'Â {} \;
```

é•œåƒåŠ é€Ÿè„šæœ¬ å®Œæ•´ä»£ç 

```sh
#!/bin/bash

# æ£€æµ‹æ“ä½œç³»ç»Ÿç±»å‹
ifÂ [[Â "$(uname)"Â ==Â "Darwin"Â ]];Â then
# macOS
Â  SED_CMD="sed -i ''"
else
# Linux å’Œå…¶ä»–
Â  SED_CMD="sed -i"
fi

# æŸ¥æ‰¾å½“å‰ç›®å½•åŠå­ç›®å½•ä¸‹çš„æ‰€æœ‰ YAML æ–‡ä»¶
find . -typeÂ f -nameÂ "values.yaml"Â -o -nameÂ "values.yml"Â |Â whilereadÂ yaml_file;Â do
echo"å¤„ç†æ–‡ä»¶:Â $yaml_file"

# ä½¿ç”¨ awk å¤„ç†æ•´ä¸ªæ–‡ä»¶ï¼Œä»¥å¤„ç†éš”è¡Œçš„ registry å’Œ repository
Â  awk -v file="$yaml_file"Â -v sed_cmd="$SED_CMD"'
Â  BEGIN { registry = ""; in_block = 0; }

Â  /registry:/ {
Â  Â  # æå– registry å€¼
Â  Â  for (i=1; i<=NF; i++) {
Â  Â  Â  if ($i == "registry:") {
Â  Â  Â  Â  registry = $(i+1);
Â  Â  Â  Â  gsub(/[",]/, "", registry); Â # ç§»é™¤å¯èƒ½çš„å¼•å·å’Œé€—å·
Â  Â  Â  Â  in_block = 1;
Â  Â  Â  Â  print "æ‰¾åˆ° registry:", registry, "åœ¨æ–‡ä»¶", file;
Â  Â  Â  }
Â  Â  }
Â  }

Â  /repository:/ {
Â  Â  if (in_block && registry != "") {
Â  Â  Â  # æå– repository å€¼
Â  Â  Â  for (i=1; i<=NF; i++) {
Â  Â  Â  Â  if ($i == "repository:") {
Â  Â  Â  Â  Â  repo = $(i+1);
Â  Â  Â  Â  Â  gsub(/[",]/, "", repo); Â # ç§»é™¤å¯èƒ½çš„å¼•å·å’Œé€—å·
Â  Â  Â  Â  Â  print "æ‰¾åˆ°åŒ¹é…çš„ repository:", repo, "åœ¨æ–‡ä»¶", file;

Â  Â  Â  Â  Â  # æ„å»ºå¹¶æ‰§è¡Œ sed å‘½ä»¤
Â  Â  Â  Â  Â  cmd = sed_cmd " '\''s|repository: " repo "|repository: " registry "/" repo "|g'\'' " file;
Â  Â  Â  Â  Â  system(cmd);

Â  Â  Â  Â  Â  # é‡ç½®çŠ¶æ€
Â  Â  Â  Â  Â  in_block = 0;
Â  Â  Â  Â  Â  registry = "";
Â  Â  Â  Â  }
Â  Â  Â  }
Â  Â  }
Â  }

Â  # å¦‚æœé‡åˆ°æ–°çš„å—å¼€å§‹ï¼Œé‡ç½®çŠ¶æ€
Â  /^[^ ]/ {
Â  Â  if ($1 != "registry:" && $1 != "repository:") {
Â  Â  Â  in_block = 0;
Â  Â  Â  registry = "";
Â  Â  }
Â  }
Â  '"$yaml_file"

# ç„¶åæ›¿æ¢æ‰€æœ‰ registry åœ°å€
$SED_CMD's|registry: docker.io|registry: m.daocloud.io|g'"$yaml_file"
$SED_CMD's|registry: registry.k8s.io|registry: m.daocloud.io|g'"$yaml_file"
$SED_CMD's|registry: quay.io|registry: m.daocloud.io|g'"$yaml_file"
$SED_CMD's|registry: ghcr.io|registry: m.daocloud.io|g'"$yaml_file"

echo"å®Œæˆå¤„ç†:Â $yaml_file"
done

echoÂ "æ‰€æœ‰ YAML æ–‡ä»¶å¤„ç†å®Œæˆï¼"
```

## é›†ç¾¤é…ç½®

### kubelet é…ç½®

[Reconfiguring a kubeadm cluster | Kubernetes](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure/)

[é€šè¿‡é…ç½®æ–‡ä»¶è®¾ç½® kubelet å‚æ•° | Kubernetes](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/)

æŸ¥çœ‹æœ€ç»ˆç”Ÿæ•ˆçš„é…ç½®ï¼Œå‚è€ƒ [Set Kubelet Parameters Via A Configuration File | Kubernetes](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/#viewing-the-kubelet-configuration)

```sh
# åœ¨ç»ˆç«¯ä¸­ä½¿ç”¨ kubectl proxy å¯åŠ¨ä»£ç†æœåŠ¡å™¨
kubectl proxy
# å…¶è¾“å‡ºå¦‚ä¸‹ï¼š
# Starting to serve on 127.0.0.1:8001

# ä½¿ç”¨ curl æ¥è·å– kubelet é…ç½®ã€‚ å°† <node-name> æ›¿æ¢ä¸ºèŠ‚ç‚¹çš„å®é™…åç§°ï¼š
curl -X GET http://127.0.0.1:8001/api/v1/nodes/<node-name>/proxy/configz | jq .
```

æŸ¥çœ‹ä½¿ç”¨çš„é…ç½®æ–‡ä»¶

```sh
# kubelet status
systemctl status kubelet.service

# kubectl å¯åŠ¨çš„é…ç½®æ–‡ä»¶
less /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

# ç¡®è®¤ä¸Šé¢çš„å†…å®¹ï¼Œä½¿ç”¨çš„ --config=/var/lib/kubelet/config.yaml
ps aux | grep kubectl
# /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9

# æŸ¥çœ‹é…ç½®æ–‡ä»¶å†…å®¹
less /var/lib/kubelet/config.yaml
```

é€šè¿‡ kubeadm ä¿®æ”¹é…ç½®ï¼Œå‚è€ƒ[Reconfiguring a kubeadm cluster | Kubernetes](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure/#applying-kubelet-configuration-changes)

```sh
# Updating the KubeletConfiguration
# document [Kubelet Configuration (v1beta1) | Kubernetes](https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration)
kubectl edit cm -n kube-system kubelet-config

# Reflecting the kubelet changes
kubeadm upgrade node phase kubelet-config
# [upgrade] Reading configuration from the cluster...
# [upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
# [upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config3426306141/config.yaml
# [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
# [upgrade] The configuration for this node was successfully updated!
# [upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.

systemctl restart kubelet
```

### æ¸…ç†ä¸å†ç”¨çš„é•œåƒ

Kubernetes ä¼šè‡ªåŠ¨æ¸…ç†æ— ç”¨çš„é•œåƒï¼Œå‚è€ƒ[åƒåœ¾æ”¶é›† | Kubernetes](https://kubernetes.io/zh-cn/docs/concepts/architecture/garbage-collection/#containers-images)

é€šè¿‡ kubeadm ä¿®æ”¹é…ç½®ï¼Œå‚è€ƒ[Reconfiguring a kubeadm cluster | Kubernetes](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure/#applying-kubelet-configuration-changes)

```sh
# Updating the KubeletConfiguration
# document [Kubelet Configuration (v1beta1) | Kubernetes](https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration)
kubectl edit cm -n kube-system kubelet-config

# ä¿®æ”¹å†…å®¹ æœåŠ¡å™¨ç£ç›˜å‘Šè­¦é˜ˆå€¼æ˜¯ 80%ï¼Œè¿™é‡Œè®¾ç½®æˆ 79% å¼€å§‹æ¸…ç†ï¼Œé¿å…å‘Šè­¦
    imageGCHighThresholdPercent: 79
    imageGCLowThresholdPercent: 74

# ç™»å½•æ¯ä¸€å° nodeï¼ŒReflecting the kubelet changes
kubeadm upgrade node phase kubelet-config
# [upgrade] Reading configuration from the cluster...
# [upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
# [upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config3426306141/config.yaml
# [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
# [upgrade] The configuration for this node was successfully updated!
# [upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.

systemctl restart kubelet
```

æ‰‹åŠ¨ä½¿ç”¨è„šæœ¬æ¸…ç†é•œåƒ

```sh
# æŸ¥è¯¢æ‰€æœ‰åœ¨ç”¨çš„é•œåƒ
kubectl get pods --all-namespaces --output=jsonpath='{..image}' |less
```

ctr æ¸…ç†æ— ç”¨é•œåƒ

```sh
#!/bin/bash
# è·å–æ‰€æœ‰é•œåƒçš„åˆ—è¡¨
images=$(ctr -n=k8s.io images ls -q)

# éå†é•œåƒåˆ—è¡¨
for image in $images; do
  # æ£€æŸ¥æ˜¯å¦æœ‰å®¹å™¨æ­£åœ¨ä½¿ç”¨è¯¥é•œåƒ
  if ! ctr -n=k8s.io containers ls | grep -q "$image"; then
    echo "Deleting unused image: $image"
    ctr -n=k8s.io images rm "$image"
  fi
done
```

### Kubernetes è¿ç§»èŠ‚ç‚¹ Kubelet æ•°æ®å­˜å‚¨ç›®å½•

é»˜è®¤ä½ç½®æ˜¯ /var/lib/kubelet å»ºè®®ä½¿ç”¨ symlink è½¯è¿æ¥æˆ– mountï¼Œä¸å»ºè®®è¿ç§»åˆ°å…¶ä»–ä½ç½®

```sh
# ç¡®è®¤å½“å‰ kubelet æ•°æ®ç›®å½•
df -h | grep kubelet
[root@qj-master01 user]# df -h | grep /var/lib/kubelet
tmpfs  7.6G   12K  7.6G   1% /var/lib/kubelet/pods/0f8fef47-20da-49b7-baa3-39e817bab9af/volumes/kubernetes.io~projected/kube-api-access-4lwqv
tmpfs  170M   12K  170M   1% /var/lib/kubelet/pods/0e3316be-0886-41fe-ade5-2480e48228a8/volumes/kubernetes.io~projected/kube-api-access-bf2zg

kubectl cordon node_name
# 1 é©±é€åœ¨è¯¥èŠ‚ç‚¹ä¸Šè¿è¡Œçš„Pod
kubectl drain node_name --ignore-daemonsets

# 2 åœæ­¢èŠ‚ç‚¹Kubeletå’ŒDockeræœåŠ¡
systemctl stop kubelet
service stop containerd
# systemctl stop docker

# 3 ç›®å½•å¸è½½
umount /var/lib/kubelet
# umount /var/lib/kubelet/pods/*/volumes/kubernetes.io~*/*

# 4 æ•°æ®å¤‡ä»½
mkdir -p /data/lib/
sudo rsync -Pavz /var/lib/containerd /data/lib/containerd
# sudo rsync -Pavz /var/lib/kubelet /data/lib/kubelet
sudo mv /data/lib/kubelet /data/lib/kubelet.bak
sudo ln -s /data/lib/kubelet /var/lib/kubelet

# 5 ä¿®æ”¹èŠ‚ç‚¹Kubeletæ•°æ®ç›˜ç›®å½•
# kubelet é…ç½®æ–‡ä»¶è·¯å¾„é€šè¿‡ systemctl status kubelet.service æŸ¥çœ‹
# åœ¨Kubeletçš„é…ç½®æ–‡ä»¶ä¸­è®¾ç½® --data-dir å‚æ•°æ¥æŒ‡å®šæ‰€éœ€çš„ç›®å½•è·¯å¾„ï¼Œä¿®æ”¹é…ç½®æ–‡ä»¶ /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf ã€‚
Environment="KUBELET_EXTRA_ARGS=--root-dir=/data/lib/kubelet --node-ip=xxx --hostname-override=master3 "

# 6 åˆ›å»ºkubeletæ•°æ®ç›®å½•å¹¶é‡å¯èŠ‚ç‚¹Kubeletå’ŒDockeræœåŠ¡
systemctl daemon-reload
systemctl restart containerd
# systemctl restart docker
systemctl restart kubelet

# 7 æ£€æŸ¥kubeletæ•°æ®ç›®å½•æ˜¯å¦ä¿®æ”¹æˆåŠŸ
df -h | grep kubelet

# 8 å–æ¶ˆèŠ‚ç‚¹æ±¡ç‚¹
kubectl uncordon node_name
```

### æ‰‹åŠ¨æ›´æ–°è¯ä¹¦

[æ‰‹åŠ¨æ›´æ–°è¯ä¹¦ ä½¿ç”¨ kubeadm è¿›è¡Œè¯ä¹¦ç®¡ç† | Kubernetes](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#manual-certificate-renewal)

å‡å¦‚ä½ é€šè¿‡ kubeadm å®‰è£… Kubernetesï¼Œå¤§å¤šæ•°è¯ä¹¦ä¼šè¢«å­˜å‚¨åœ¨ /etc/kubernetes/pki ä¸­ã€‚ æœ¬æ–‡æ¡£ä¸­çš„æ‰€æœ‰è·¯å¾„éƒ½æ˜¯ç›¸å¯¹äºè¯¥ç›®å½•çš„ï¼Œä½†ç”¨æˆ·è´¦å·è¯ä¹¦é™¤å¤–ï¼Œkubeadm å°†å…¶æ”¾åœ¨ /etc/kubernetes ä¸­ã€‚

```sh
# æ£€æŸ¥è¯ä¹¦ä½•æ—¶è¿‡æœŸ
kubeadm certs check-expiration
# Reading configuration from the cluster, check config file
kubectl -n kube-system get cm kubeadm-config -o yaml

# kubeadm renews all the certificates during control plane upgrade.
# æ‰‹åŠ¨æ›´æ–°è¯ä¹¦ è¿™ä¸ªå‘½ä»¤éœ€è¦åœ¨æ‰€æœ‰æ§åˆ¶é¢æ¿èŠ‚ç‚¹ä¸Šæ‰§è¡Œã€‚
kubeadm certs renew all

# certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed
# certificate for serving the Kubernetes API renewed
# certificate the apiserver uses to access etcd renewed
# certificate for the API server to connect to kubelet renewed
# certificate embedded in the kubeconfig file for the controller manager to use renewed
# certificate for liveness probes to healthcheck etcd renewed
# certificate for etcd nodes to communicate with each other renewed
# certificate for serving etcd renewed
# certificate for the front proxy client renewed
# certificate embedded in the kubeconfig file for the scheduler manager to use renewed
# certificate embedded in the kubeconfig file for the super-admin renewed

# Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates.

mv /etc/kubernetes/manifests /etc/kubernetes/manifests.bak
# ç­‰å¾…20ç§’é’Ÿï¼Œpod (kube-apiserver, kube-controller-manager, kube-scheduler and etcd) å…³é—­å ç§»å›æ–‡ä»¶
mv /etc/kubernetes/manifests.bak /etc/kubernetes/manifests
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### Kubernetes Gateway API

Gateway API v1.0: GA Release October 31, 2023

[Kubernetes Gateway API æ­£å¼å‘å¸ƒå¹¶å¼•å…¥ ingress2gateway é¡¹ç›®ç”¨äºç®€åŒ– Gateway API å‡çº§ | äº‘åŸç”Ÿç¤¾åŒºï¼ˆä¸­å›½ï¼‰](https://cloudnative.to/blog/gateway-api-ingress2gateway/)

![Gateway API](image/Gateway-API.png)

[Gateway API | Kubernetes](https://kubernetes.io/docs/concepts/services-networking/gateway/)
[Getting started - Kubernetes Gateway API](https://gateway-api.sigs.k8s.io/guides/)

### Setup prometheus + grafana

[Alerts firing right after setting up kube-prometheus-stack](https://groups.google.com/g/prometheus-users/c/_aI-HySJ-xM/m/kqrL1FYVCQAJ?pli=1)
[KubeControllerManagerDown & kubeSchedulerDown firing on kubeadm 1.18 cluster Â· Issue #718 Â· prometheus-operator/kube-prometheus](https://github.com/prometheus-operator/kube-prometheus/issues/718)

kube-scheduler -> bind-address=127.0.0.1
kube-controller-manager -> bind-address=127.0.0.1
etcd -> listen-metrics-urls=http://127.0.0.1:<port>

```sh
# ä¿®æ”¹ç›‘å¬çš„ip
kubectl -n kube-system edit configmap kube-proxy
# metricsBindAddress: "0.0.0.0:10249"

# åœ¨ä¸‰å° master èŠ‚ç‚¹ä¿®æ”¹æ–‡ä»¶ï¼Œåé‡å¯Pod
sed -e "s/- --bind-address=127.0.0.1/- --bind-address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-controller-manager.yaml
sed -e "s/- --bind-address=127.0.0.1/- --bind-address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-scheduler.yaml
sed -e "s#- --listen-metrics-urls=http://127.0.0.1:2381#- --listen-metrics-urls=http://0.0.0.0:2381#" -i /etc/kubernetes/manifests/etcd.yaml

```

### Setup PostgreSQL

[Can I host Postgres on k8s myself? : r/kubernetes](https://www.reddit.com/r/kubernetes/comments/1j854ze/can_i_host_postgres_on_k8s_myself/?share_id=3JOfzJdVHRh_sLHXd4w9Y&utm_content=1&utm_medium=ios_app&utm_name=iossmf&utm_source=share&utm_term=22)

[Postgres databases in Kubernetes Â· Stonegarden](https://blog.stonegarden.dev/articles/2024/10/k8s-postgres/)

[Why would you run PostgreSQL in Kubernetes, and how?](https://www.cloudraft.io/blog/why-would-you-run-postgresql-on-kubernetes)

Can I host Postgres on k8s myself?

1. Zalando Postgres Operator
2. CloudNativePG
3. [stackgres](https://stackgres.io/)ï¼šfor sharding
4. [bitnami postgresql helm chart](https://hub.docker.com/r/bitnami/postgresql)
5. KubeDB

## Best Practice

### å£°æ˜æ¯ä¸ªPodçš„resource

[é«˜å¯é æ¨èé…ç½®](https://help.aliyun.com/document_detail/128157.html)
Kubernetesé‡‡ç”¨é™æ€èµ„æºè°ƒåº¦æ–¹å¼ï¼Œå¯¹äºæ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„å‰©ä½™èµ„æºï¼Œå®ƒæ˜¯è¿™æ ·è®¡ç®—çš„ï¼šèŠ‚ç‚¹å‰©ä½™èµ„æº=èŠ‚ç‚¹æ€»èµ„æº-å·²ç»åˆ†é…å‡ºå»çš„èµ„æºï¼Œå¹¶ä¸æ˜¯å®é™…ä½¿ç”¨çš„èµ„æºã€‚å¦‚æœæ‚¨è‡ªå·±æ‰‹åŠ¨è¿è¡Œä¸€ä¸ªå¾ˆè€—èµ„æºçš„ç¨‹åºï¼ŒKuberneteså¹¶ä¸èƒ½æ„ŸçŸ¥åˆ°ã€‚

å¦å¤–æ‰€æœ‰Podä¸Šéƒ½è¦å£°æ˜resourcesã€‚å¯¹äºæ²¡æœ‰å£°æ˜resourcesçš„Podï¼Œå®ƒè¢«è°ƒåº¦åˆ°æŸä¸ªèŠ‚ç‚¹åï¼ŒKubernetesä¹Ÿä¸ä¼šåœ¨å¯¹åº”èŠ‚ç‚¹ä¸Šæ‰£æ‰è¿™ä¸ªPodä½¿ç”¨çš„èµ„æºã€‚å¯èƒ½ä¼šå¯¼è‡´èŠ‚ç‚¹ä¸Šè°ƒåº¦è¿‡å»å¤ªå¤šçš„Podã€‚

how to find the requests and limits?

[Does anyone actually have a good way to deal with OOMKilled pods in Kubernetes? : r/kubernetes](https://www.reddit.com/r/kubernetes/comments/1mphmvl/does_anyone_actually_have_a_good_way_to_deal_with/)

Use Goldilocks or VPA in recommendation mode and let it run for a month and take the suggested requests and limits. Stress test and performance test your applications and isolate whether you have issues like memory leaks, or at the very least understand the failure modes of your system.

[FairwindsOps/goldilocks: Get your resource requests "Just Right"](https://github.com/FairwindsOps/goldilocks)

## Troubleshooting

### Pod exit code

[Docker Container Exit Codes Explained | hoangtrinhj.com](https://hoangtrinhj.com/docker-container-exit-codes)

[é€šè¿‡ Exit Code å®šä½ Pod å¼‚å¸¸é€€å‡ºåŸå›  | è…¾è®¯äº‘](https://intl.cloud.tencent.com/zh/document/product/457/35758)

[SIGTERM : Linux Graceful Termination | Exit code 143, Signal 15](https://komodor.com/learn/sigterm-signal-15-exit-code-143-linux-graceful-termination/)

Common exit codes (`128+x`) associated with docker containers are:

- Exit Code 0: Absence of an attached foreground process
- Exit Code 1: Indicates failure due to application error
- Exit Code 137: `128+9` Indicates failure as container received SIGKILL (Manual intervention or â€˜oom-killerâ€™ [OUT-OF-MEMORY])
- Exit Code 139: `128+11` Indicates failure as container received SIGSEGV
- Exit Code 143: `128+15` Indicates failure as container received SIGTERM

x: see `man signal`

### [Troubleshooting a failed certificate request | cert-manager](https://cert-manager.io/docs/faq/troubleshooting/)

There are several resources that are involved in requesting a certificate.
Automated Certificate Management Environment (ACME).

```text
  (  +---------+  )
  (  | Ingress |  ) Optional                                              ACME Only!
  (  +---------+  )
         |                                                     |
         |   +-------------+      +--------------------+       |  +-------+       +-----------+
         |-> | Certificate |----> | CertificateRequest | ----> |  | Order | ----> | Challenge |
             +-------------+      +--------------------+       |  +-------+       +-----------+
                                                               |
```

1. Checking the Certificate resource `kubectl get certificate` or `kubectl describe certificate <certificate-name>`
2. Checking the CertificateRequest `kubectl describe certificaterequest <CertificateRequest name>`
3. Check the issuer state
   1. `kubectl describe issuer <Issuer name>`
   2. `kubectl describe clusterissuer <ClusterIssuer name>`
4. [Troubleshooting Issuing ACME Certificates | cert-manager](https://cert-manager.io/docs/faq/acme/): ACME(e.g. Letâ€™s Encrypt)
   1. Check Orders `kubectl describe order example-com-2745722290-439160286`. If the Order is not completing successfully, you can debug the challenges for the Order
   2. Check Challenges `kubectl describe challenge example-com-2745722290-4391602865-0`
      1. [HTTP01 troubleshooting](https://cert-manager.io/docs/faq/acme/#http01-troubleshooting)
      2. [DNS01 troubleshooting](https://cert-manager.io/docs/faq/acme/#dns01-troubleshooting)
         1. [alidns-webhook/bundle.yaml](https://github.com/pragkent/alidns-webhook/blob/master/deploy/bundle.yaml)
      3. Check the pod status and log `kubectl log alidns-webhook-78df4cfddd-cnjsd`

## Example

### æš´éœ² TCP æœåŠ¡

#### service type = CluserIP é€šè¿‡Ingress

é˜¿é‡Œäº‘ kubernetes é…ç½®

1. é…ç½® configmap: å®¹å™¨æœåŠ¡ -> åº”ç”¨é…ç½®-> é…ç½®é¡¹ -> tcp-services -> æ·»åŠ  åç§°: config-name, å€¼: namespace/serviceName:service port, æ³¨æ„åç§°config-name éœ€è¦é…ç½®åˆ° ingress å®¹å™¨ç«¯å£, ä¾‹å¦‚ `5672:rabbitmq-system/rabbitmqcluster:5672` æˆ–è€…ç”¨å‘½ä»¤ `kubectl -n kube-system patch configmap tcp-services --type merge -p '{"data":{"5672": "rabbitmq-system/rabbitmqcluster:5672"}}'`
2. é…ç½® service: å®¹å™¨æœåŠ¡ -> ç½‘ç»œ -> æœåŠ¡ -> nginx-ingress-lb -> æ›´æ–° å¢åŠ ç«¯å£
   port æ˜¯æš´éœ²çš„å…¬ç½‘ç«¯å£(æ§åˆ¶å°å«åš æœåŠ¡ç«¯å£), targetPort æ˜¯ configmap åç§° (æ§åˆ¶å°å«åš å®¹å™¨ç«¯å£), è¿™é‡Œå®¹å™¨ç«¯å£åªèƒ½æ˜¯æ•°å­—, æ‰€ä»¥åè¿‡æ¥é™åˆ¶ç¬¬ä¸€æ­¥çš„ config-name åªèƒ½ç”¨æ•°å­—

ç„¶å `ingress` ä¼šåŠ¨æ€è¯»å–`tcp-services` æš´éœ²çš„ç«¯å£ `tcp-services-configmap=$(POD_NAMESPACE)/tcp-services`

reference:
[ç©è½¬Kubernetes TCP Ingress](https://developer.aliyun.com/article/603225)
[Exposing TCP and UDP services](https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services)

#### service type = NodePort

1. åœ¨è´Ÿè½½å‡è¡¡è®¾ç½®ç«¯å£æ˜ å°„, ç›‘å¬è™šæ‹ŸæœåŠ¡å™¨ç»„çš„ç«¯å£è®¾ç½®ä¸º NodePort 30022
2. å®‰å…¨ç»„æ”¾å¼€ nodePort 30022

## ä½¿ç”¨ kubecost åˆ†æ Kubernetes æˆæœ¬

kubecost æ˜¯ç›®å‰è¾ƒä¼˜ç§€çš„å¼€æº Kubernetes æˆæœ¬åˆ†æå·¥å…·ã€‚kubecost ç›®å‰æ”¯æŒ é˜¿é‡Œäº‘ã€AWS ç­‰äº‘å‚å•†å¯¹æ¥ï¼Œå®ƒèƒ½å¤Ÿæä¾›é›†ç¾¤ä¸­å‘½åç©ºé—´ã€åº”ç”¨ç­‰å„ç±»èµ„æºæˆæœ¬åˆ†é…ï¼Œç”¨æˆ·è¿˜å¯ä»¥åŸºäºè¿™äº›ä¿¡æ¯åœ¨ Kubecost ä¸­è®¾ç½®é¢„ç®—å’Œè­¦æŠ¥ï¼Œå¸®åŠ©è¿ç»´å’Œè´¢åŠ¡ç®¡ç†äººå‘˜è¿›ä¸€æ­¥å®ç°æˆæœ¬ç®¡ç†ã€‚

## containerd

### Docker vs. Containerd

[ä¸€æ–‡å¸¦ä½ äº†è§£Dockerä¸Containerdçš„åŒºåˆ«-è…¾è®¯äº‘å¼€å‘è€…ç¤¾åŒº-è…¾è®¯äº‘](https://cloud.tencent.com/developer/article/2327654)

[Docker, containerd, CRI-O and runcä¹‹é—´çš„åŒºåˆ«ï¼Ÿ - Zhai_David - åšå®¢å›­](https://www.cnblogs.com/chuanzhang053/p/16784668.html)

[å®¹å™¨æœåŠ¡ å¦‚ä½•é€‰æ‹© Containerd å’Œ Docker-å¸¸è§é—®é¢˜-æ–‡æ¡£ä¸­å¿ƒ-è…¾è®¯äº‘](https://cloud.tencent.com/document/product/457/35747)
[å¦‚ä½•é€‰æ‹©Dockerã€ContainerdåŠå®‰å…¨æ²™ç®±è¿è¡Œæ—¶_å®¹å™¨æœåŠ¡ Kubernetes ç‰ˆ ACK(ACK)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/comparison-of-docker-containerd-and-sandboxed-container)

Containerdï¼šè°ƒç”¨é“¾æ›´çŸ­ï¼Œç»„ä»¶æ›´å°‘ï¼Œæ›´ç¨³å®šï¼Œå ç”¨èŠ‚ç‚¹èµ„æºæ›´å°‘ã€‚å»ºè®®é€‰æ‹© Containerdã€‚

ä½œä¸º K8S å®¹å™¨è¿è¡Œæ—¶ï¼Œéƒ¨ç½²ç»“æ„å¯¹æ¯”

Docker: kubelet --> docker shim ï¼ˆåœ¨ kubelet è¿›ç¨‹ä¸­ï¼‰ --> dockerd --> containerd
Containerd: kubelet --> cri pluginï¼ˆåœ¨ containerd è¿›ç¨‹ä¸­ï¼‰ --> containerd

Containerd å’Œ Docker ç»„ä»¶å¸¸ç”¨å‘½ä»¤æ˜¯ä»€ä¹ˆï¼Ÿ

Containerd ä¸æ”¯æŒ docker API å’Œ docker CLIï¼Œä½†æ˜¯å¯ä»¥é€šè¿‡ cri-tool å‘½ä»¤å®ç°ç±»ä¼¼çš„åŠŸèƒ½ã€‚

### containerd å‘½ä»¤

ctr æ˜¯ containerd çš„ä¸€ä¸ªå®¢æˆ·ç«¯å·¥å…·ã€‚
crictl æ˜¯ CRI å…¼å®¹çš„å®¹å™¨è¿è¡Œæ—¶å‘½ä»¤è¡Œæ¥å£ï¼Œå¯ä»¥ä½¿ç”¨å®ƒæ¥æ£€æŸ¥å’Œè°ƒè¯• k8s èŠ‚ç‚¹ä¸Šçš„å®¹å™¨è¿è¡Œæ—¶å’Œåº”ç”¨ç¨‹åºã€‚

the debug tool "ctr" does not parse or use the cri config (/etc/containerd/config.toml) in any way.. the cri config is for crictl, rancher, kubernernetes, ... You can pass your username and password at the ctr command line.

```sh
# è¾“å‡ºå½“å‰ k8s çš„ç‰ˆæœ¬ï¼Œä»ç»“æœå¯ä»¥è®¤ä¸º crictl æ˜¯ç”¨äº k8s çš„ã€‚
crictl -v

# è¾“å‡º containerd çš„ç‰ˆæœ¬
ctr -v


# Save the Image as a Tar File
ctr image export dashboard.tar dashboard
ctr -n=k8s.io image export dashboard.tar dashboard

# ä½¿ç”¨ctrå¯¼å…¥é•œåƒ
ctr image import dashboard.tar

# ctræ˜¯containerdè‡ªå¸¦çš„å·¥å…·ï¼Œæœ‰å‘½åç©ºé—´çš„æ¦‚å¿µï¼Œè‹¥æ˜¯k8sç›¸å…³çš„é•œåƒï¼Œéƒ½é»˜è®¤åœ¨k8s.ioè¿™ä¸ªå‘½åç©ºé—´ï¼Œæ‰€ä»¥å¯¼å…¥é•œåƒæ—¶éœ€è¦æŒ‡å®šå‘½ä»¤ç©ºé—´ä¸º k8s.io
# ä½¿ç”¨ctrå‘½ä»¤æŒ‡å®šå‘½åç©ºé—´å¯¼å…¥é•œåƒ
ctr -n=k8s.io image import dashboard.tar

#æŸ¥è¯¢é•œåƒ
ctr -n=k8s.io images ls
crictl images

# pull image
# kubeadm config images list --kubernetes-version=v1.15.2
ctr image pull k8s.gcr.io/prometheus-adapter/prometheus-adapter:v0.9.1
crictl pull k8s.gcr.io/prometheus-adapter/prometheus-adapter:v0.9.1

# åˆ›å»º k8s.io å‘½åç©ºé—´
ctr ns create k8s.io

# æŸ¥çœ‹å‘½åç©ºé—´
ctr ns ls
```

|           | docker                  | ctrï¼ˆcontainerdï¼‰              | crictlï¼ˆkubernetesï¼‰       |
|-----------|-------------------------|------------------------------|--------------------------|
| åˆ›å»ºæ–°å®¹å™¨     | docker create           | ctr container create         | crictl create            |
| å¯åŠ¨/å…³é—­å®¹å™¨   | docker start/stop       | ctr task start/kill          | crictl start/stop        |
| è¿è¡Œæ–°å®¹å™¨     | docker run              | ctr run                      | æ— ï¼ˆæœ€å°å•å…ƒä¸º podï¼‰             |
| æŸ¥çœ‹è¿è¡Œçš„å®¹å™¨   | docker ps               | ctr task ls/ctr container ls | crictl ps                |
| åˆ é™¤å®¹å™¨      | docker rm               | ctr container rm             | crictl rm                |
| æŸ¥çœ‹å®¹å™¨æ—¥å¿—    | docker logs             | æ—                             | crictl logs              |
| æŸ¥çœ‹å®¹å™¨æ•°æ®    | docker inspect          | ctr container info           | crictl inspect           |
| æŸ¥çœ‹å®¹å™¨èµ„æº    | docker stats            | æ—                             | crictl stats             |
| å®¹å™¨å†…éƒ¨æ‰§è¡Œå‘½ä»¤  | docker exec             | æ—                             | crictl exec              |
| attach    | docker attach           | æ—                             | crictl attach            |
| ä¿®æ”¹é•œåƒæ ‡ç­¾    | docker tag              | ctr image tag                | æ—                         |
| å¯¼å…¥é•œåƒ      | docker load             | ctr image import             | æ—                         |
| å¯¼å‡ºé•œåƒ      | docker save             | ctr image export             | æ—                         |
| æŸ¥çœ‹é•œåƒ      | docker images           | ctr image ls                 | crictl images            |
| åˆ é™¤é•œåƒ      | docker rmi              | ctr image rm                 | crictl rmi               |
| æ‹‰å–é•œåƒ      | docker pull             | ctr image pull               | crictl pull              |
| æ¨é€é•œåƒ      | docker push             | ctr image push               | æ—                         |
| æŸ¥çœ‹é•œåƒè¯¦æƒ…    | docker inspect IMAGE-ID | ?                            | crictl inspect IMAGE-ID  |
| æ˜¾ç¤º POD åˆ—è¡¨ | æ—                        | æ—                             | crictl pods              |

### è®¾ç½® containerd æ‹‰å– http ç§æœ‰ä»“åº“

[How to pull docker image from a insecure private registry with latest Kubernetes - Stack Overflow](https://stackoverflow.com/questions/72419513/how-to-pull-docker-image-from-a-insecure-private-registry-with-latest-kubernetes)

```sh
# [containerd/docs/cri/registry.md at main Â· containerd/containerd](https://github.com/containerd/containerd/blob/main/docs/cri/registry.md)
# vi /etc/containerd/config.toml

# data
# root = "/var/lib/containerd"
root = "/data/lib/containerd"

      [plugins."io.containerd.grpc.v1.cri".registry.configs]
        [plugins."io.containerd.grpc.v1.cri".registry.configs."docker.io"] # edited line
          [plugins."io.containerd.grpc.v1.cri".registry.configs."docker.io".auth] # edited line
            username = "USERNAME"
            password = "PASSWORD"
          [plugins."io.containerd.grpc.v1.cri".registry.configs."docker.io".tls] # edited line
            ca_file = "" # edited line
            cert_file = "" # edited line
            insecure_skip_verify = true # edited line
            key_file = "" # edited line
        [plugins."io.containerd.grpc.v1.cri".registry.configs."gcr.io"] # edited line
          [plugins."io.containerd.grpc.v1.cri".registry.configs."gcr.io".auth] # edited line
            username = "USERNAME"
            password = "PASSWORD"
      [plugins."io.containerd.grpc.v1.cri".registry.headers]
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"] # edited line
          endpoint = ["http://registry-1.docker.io"] # edited line
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."gcr.io"] # edited line
          endpoint = ["https://gcr.io"] # edited line


# Restart containerd:
service containerd restart
sudo crictl pull gcr.io/your-gcp-project-id/busybox
```

### å®¢æˆ·ç«¯å·¥å…· nerdctl

https://github.com/containerd/nerdctl/releases

ç²¾ç®€ (nerdctl--linux-amd64.tar.gz): åªåŒ…å« nerdctl
å®Œæ•´ (nerdctl-full--linux-amd64.tar.gz): åŒ…å« containerd, runc, and CNI ç­‰ä¾èµ–

nerdctl çš„ç›®æ ‡å¹¶ä¸æ˜¯å•çº¯åœ°å¤åˆ¶ docker çš„åŠŸèƒ½ï¼Œå®ƒè¿˜å®ç°äº†å¾ˆå¤š docker ä¸å…·å¤‡çš„åŠŸèƒ½ï¼Œä¾‹å¦‚å»¶è¿Ÿæ‹‰å–é•œåƒï¼ˆlazy-pullingï¼‰ã€é•œåƒåŠ å¯†ï¼ˆimgcryptï¼‰ç­‰ã€‚å…·ä½“çœ‹ nerdctlã€‚

é€šè¿‡ nerdctl ç™»å½• harbor

```sh
echo Harbor12345 | nerdctl login --username "admin" --password-stdin  myharbor-minio.com:443
nerdctl login --username "admin" --password Harbor12345 myharbor-minio.com:443
# ç™»å‡º
nerdctl logout
```

## ip-netns management tool

[ip-netns(8) - Linux manual page](https://man7.org/linux/man-pages/man8/ip-netns.8.html)
[Tracing the path of network traffic in Kubernetes](https://learnk8s.io/kubernetes-network-packets)

The network namespaces can be managed by the ip-netns management tool,

```sh
# to list the namespaces on a host.
ip netns list

# run the exec command inside the namespace cni-0f226515
ip netns exec cni-ebbbed0d-7b1c-36fb-b412-ce337ff74778 ip a

# run the netstat command inside that namespace
# verify that the container listens for HTTP traffic from within the namespace
ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac netstat -lnp

# find the latest named network namespace
ls -lt /var/run/netns
```

`lsns` is a command for listing all available namespaces on a host.
  `-t, --type type` The supported types are mnt, net, ipc, user, pid and uts.
  `-p, --task pid` Display only the namespaces held by the process with this pid. `lsns -p 5777`

exampleï¼š

```sh
lsns -t net -t mnt

```
