# kubernetesNotes

Container Network Interface (CNI)

## Recent

batch delete `kdelp $(kgp -l | grep Evicted | awk '{print $1}')`
kubelet summary API `http://localhost:8001/api/v1/nodes/node-name/proxy/stats/summary`

install arthas: `kubectl exec -it podName -- /bin/bash -c "wget https://arthas.aliyun.com/arthas-boot.jar && java -jar arthas-boot.jar"`

### è·å–ä¿¡æ¯ æ’æŸ¥é—®é¢˜

kubectl get events -A -o custom-columns=FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message |less

æŸ¥çœ‹æŒ‡å®š pod çš„ events
kubectl get events --watch -o custom-columns=Created:.metadata.creationTimestamp,FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message --field-selector involvedObject.kind=Pod, involvedObject.name=pod-test-78b7567ccc-b96kb

kubectl get events -o custom-columns=Created:.metadata.creationTimestamp,FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message --field-selector involvedObject.kind=Pod --sort-by=.metadata.creationTimestamp |less
kubectl get events -o custom-columns=Created:.metadata.creationTimestamp,FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message --sort-by=.metadata.creationTimestamp |less

kubectl get pods -A -o=jsonpath='{range .items[*]}{.spec.containers[].resources.requests.memory}{"\t"}{.status.hostIP}{"\t"}{.metadata.name}{"\n"}{end}' | grep 145

```sh
# get pod image
kubectl get deployment -o json ems-backend -o=jsonpath='{.spec.template.spec.containers[0].image}' | awk -F : '{print $NF}'
```

## Concept

OCI: Open Container Initiative
CRI: Container Runtime Interface
runc æ˜¯ä¸€ä¸ªå…¼å®¹ociçš„å®¹å™¨è¿è¡Œæ—¶ã€‚å®ƒå®ç°OCIè§„èŒƒå¹¶è¿è¡Œå®¹å™¨è¿›ç¨‹ã€‚

### [Requests and limits](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory)

Meaning of memoryï¼ŒMiè¡¨ç¤ºï¼ˆ1Mi=1024x1024ï¼‰,Mè¡¨ç¤ºï¼ˆ1M=1000x1000ï¼‰ï¼ˆå…¶å®ƒå•ä½ç±»æ¨ï¼Œ å¦‚Ki/K Gi/Gï¼‰
For example, the following represent roughly the same value: `128974848, 129e6, 129M, 123Mi`
1 (byte), 1k (kilobyte) or 1Ki (kibibyte), 1M (megabyte) or 1Mi (mebibyte)

Meaning of CPU:
The expression 0.1 is equivalent to the expression 100m, which can be read as "one hundred millicpu". Some people say "one hundred millicores", and this is understood to mean the same thing. A request with a decimal point, like 0.1, is converted to 100m by the API, and precision finer than 1m is not allowed. For this reason, the form 100m might be preferred.
`0.1 = 100m`

get all pods along with cpu and memory requirements in kubernetes `kubectl get po -o custom-columns="Name:metadata.name,CPU-request:spec.containers[*].resources.requests.cpu,CPU-limit:spec.containers[*].resources.limits.cpu,memory-request:spec.containers[*].resources.requests.memory,memory-limit:spec.containers[*].resources.limits.memory" --sort-by=".spec.containers[*].resources.limits.memory`

### Common

`kubectl logs -f --tail=10 pod-name`
  `-o custom-columns` option an
  `--sort-by=<jsonpath_exp>` sort the resource list, `--sort-by=.metadata.name`
[Resource types](https://kubernetes.io/docs/reference/kubectl/overview/#resource-types)

[JSONPath Support](https://kubernetes.io/docs/reference/kubectl/jsonpath/)
`kubectl get pods -A -o=jsonpath='{range .items[*]}{.status.hostIP}{"\t"}{.spec.containers[].resources.requests.cpu}{"\t"}{.spec.containers[].resources.requests.memory}{"\t"}{.metadata.name}{"\n"}{end}' --sort-by='.status.hostIP'`

```sh
# get name cpu
kubectl get pods -o custom-columns=NAME:.metadata.name,CPU:.spec.containers
# get the pod's IP address
kubectl get pod multi-container-pod -o jsonpath={.status.podIP}
```

### Debug

æ’æŸ¥é—®é¢˜
`kubectl get events`
`kubectl -n namespace top pods --containers`
`kubectl top pod --all-namespaces | sort --reverse --key 4 --numeric | grep -v system | less` sort by memory
`kubectl get events -o custom-columns=Created:.metadata.creationTimestamp,FirstSeen:.firstTimestamp,LastSeen:.lastTimestamp,Count:.count,From:.source.component,Type:.type,Reason:.reason,Message:.message --field-selector involvedObject.kind=Pod,involvedObject.name=mysql-test-78b7567ccc-b96kb`
`kubectl get events --sort-by=.metadata.creationTimestamp`
`kubectl get events -o yaml|less`

jsonpath æŸ¥è¯¢ç”³è¯·çš„å†…å­˜
`kubectl get pods -A -o=jsonpath='{range .items[*]}{.spec.containers[].resources.requests.memory}{"\t"}{.status.hostIP}{"\t"}{.metadata.name}{"\n"}{end}' | grep 145`

go-template `kubectl get pods -o go-template='{{range .items}}{{.status.podIP}}{{"\n"}}{{end}}'`

`kubectl cluster-info dump`

### Command

[kubectl Overview](https://jamesdefabia.github.io/docs/user-guide/kubectl-overview/)

`kubectl help get`

`minikube start` Starting a Minikube virtual machine
`kubectl explain pod` kubectl explain to discover possible API object fields
`kubectl explain pod.spec` drill deeper to find out more about each attribute
`kubectl api-resources` Print the supported API resources on the server

`kubectl --kubeconfig=/path/to/cluster-admin.config --namespace monitoring get pod`

`kubectl cluster-info` Displaying cluster information
`kubectl get nodes/pods/secrets/services/deployment`
    `-o wide` request additional columns to display
    `-o yaml` get a YAML descriptor of an existing pod
    `-o json`
    `--show-labels`
    `-L, --label-columns=[]` switch and have each displayed in its own column `-L creation_method,env`
    `-l, --selector=''`
    `-A, --all-namespaces=false` If present, list the requested object(s) across all namespaces
    `-w, --watch` watch for changes
    `--v 6` verbose logging

`kubectl get pods --selector='labelName=labelValue'`
    `'!env'`
    `'env'`
    `env in (prod,devel)`
    `env notin (prod,devel)`

`kubectl create`
  `-f, --filename=[]` Filename, directory, or URL to files to use to create the resource
  `--record` record the command

`kubectl delete pods pod_name`
  `--force` force to delete

`kubectl describe node NODE_NAME`

`kubectl cp /path/to/file /target/path`

Deploying your Node.js app: create ReplicationController `kubectl run kubia --image=luksa/kubia --port=8080 --generator=run/v1`
Accessing your web application:  creating a service object `kubectl expose rc kubia --type=LoadBalancer --name kubia-http`
accessing your service through its external ip `curl 104.155.74.57:8080`

`kubectl scale rc kubia --replicas=3` increasing the desired replica count

`kubectl create -f FILE_NAME.yaml` command is used for creating any resource (not only pods) from a YAML or JSON file.
`kubectl apply -f FILE_NAME.yaml` æ›´æ–°
`kubectl apply -f FOLDER` æ›´æ–°
`kubectl edit deploy piggy-mongo` open the YAML definition in your default text editor ä¿®æ”¹
`kubectl patch svc nodeport -p '{"spec":{"externalTrafficPolicy":"Local"}}'` æ·»åŠ 

`docker logs <container id>`
`kubectl logs kubia-manual` retrieving a podâ€™s log with kubectl logs
`kubectl logs kubia-manual -c <container name>` specifying the container name when getting logs of a multi-container pod
    `--previous` figure out why the previous container

`kubectl exec -it <pod name> -- /bin/sh`  è¿›å…¥podå†…éƒ¨
`kubectl exec -it [POD_NAME] -c [CONTAINER_NAME] -- /bin/sh -c "kill 1"` restart the specific container

```sh
# forwarding a local network port 8888 to a port 8080 in the pod
kubectl port-forward pod-name 8888:8080
# make this port listen to 0.0.0.0
kubectl port-forward --address 0.0.0.0 svc/[service-name] -n [namespace] [external-port]:[internal-port]
```

`k top node` node èµ„æºä½¿ç”¨æƒ…å†µ
`k top pod` pod èµ„æºä½¿ç”¨æƒ…å†µ
`k top pod --containers` container èµ„æºä½¿ç”¨æƒ…å†µ

COMMUNICATING WITH PODS THROUGH THE API SERVER
`kubectl proxy`
use localhost:8001 rather than the actual API server host and port. Youâ€™ll send a request to the kubia-0 pod like this:
`curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/`

`kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5` creates the HorizontalPodAutoscaler(HPA) object for you and sets the Deployment called kubia as the scaling target
`kubectl get hpa` HorizontalPodAutoscaler
a containerâ€™s CPU utilization is the containerâ€™s actual CPU usage divided by its requested CPU
`kubectl cordon <node>` marks the node as unschedulable (but doesnâ€™t do anything with pods running on that node).
`kubectl drain <node>` marks the node as unschedulable and then evicts all the pods from the node.

### PVC

`kubectl get pvc` volume claim

`kubectl patch pvc test-pvc -p '{"spec":{"resources":{"requests":{"storage":"50Gi"}}}}'` resize pvc

```json
{
    "apiVersion": "v1",
    "kind": "PersistentVolumeClaim",
    "metadata": {
        "annotations": {
            "pv.kubernetes.io/bind-completed": "yes",
            "pv.kubernetes.io/bound-by-controller": "yes",
            "volume.beta.kubernetes.io/storage-provisioner": "cluster.local/nfs-subdir-external-provisioner",
            "volume.kubernetes.io/storage-provisioner": "cluster.local/nfs-subdir-external-provisioner"
        },
        "name": "pvc-test",
        "namespace": "default"
    },
    "spec": {
        "accessModes": [
            "ReadWriteOnce"
        ],
        "resources": {
            "requests": {
                "storage": "1Mi"
            }
        },
        "storageClassName": "nfs-client",
        "volumeMode": "Filesystem",
        "volumeName": "pvc-f10cff78-eac8-4e4a-9de3-1b82ec7446f2"
    }
}
```

### Pod

`k port-forward pod-name 8001:5000` æœ¬åœ°ç«¯å£ 8001 è½¬å‘åˆ° pod çš„ç«¯å£ 5000

`kubectl delete pods $(kubectl get pods | grep Evicted |awk '{print $1}')` delete Evicted pods in current namespace

`kubectl get pods -A | grep Evicted | awk '{print "kubectl delete pods -n ",$1,$2}' | bash -x` delete Evicted pods in all namespaces

#### [Assign Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)

`nodeName: foo-node # schedule pod to specific node`

```yaml
nodeSelector:
  svrtype: web

```

### Service

è®¿é—®URI: `SERVICE_NAME.NAMESPACE.svc.cluster.local`
 `svc.cluster.local` is a configurable cluster domain suffix used in all cluster local service names.

[Debug Services](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/)

### Deployment

```sh
# [kubectl rollout restart | Kubernetes](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_rollout/)

# Restart a deployment
kubectl rollout restart deployment/nginx
# deployment å†å²è®°å½•
kubectl rollout history deployment deployment-name
# Changes the container image defined in a Pod
kubectl set image deployment kubia nodejs=luksa/kubia:v3
# the progress of the rollout
kubectl rollout status deployment kubia
# displaying a deploymentâ€™s rollout history
kubectl rollout history deployment kubia
# undoing a rollout
kubectl rollout undo deployment kubia
kubectl rollout undo deployment kubia --to-revision=1
kubectl rollout history deployment/<Deployment-name>  --revision=<revision-number>  -o yaml
```

deployment strategies:

* RollingUpdate
* Recreate

### Label

`kubectl label pod kubia-manual creation_method=manual` Modifying labels of existing pods
    `--overwrite`
`kubectl get pod -l creation_method=manual` Listing pods using a label selector
`kubectl get pod -l env` To list all pods that include the env label
`kubectl get po -l '!env'` To list all pods that donâ€™t have the env label

### Namespaces

`kubectl create ns name`
`kubectl get ns`
`kubectl get pod --namespace kube-system`

`kubectl config set-context --current --namespace=piggy` change namespace
`alias kcn='kubectl config set-context $(kubectl config current-context) --namespace '`

`kubectl create -f dev.json`

```json
{
  "apiVersion": "v1",
  "kind": "Namespace",
  "metadata": {
    "name": "dev",
    "labels": {
      "name": "dev"
    }
  }
}
```

`kubectl delete ns custom-namespace` delete the whole namespace (the pods will be deleted along with the namespace automatically)
`kubectl delete po --all` Deleting all pods in a namespace, while keeping the namespace
`kubectl delete all --all` Deleting (almost) all resources in a namespace

### Ingress

```yaml
    # HTTP 413 é”™è¯¯ ï¼ˆ Request entity too large è¯·æ±‚å®ä½“å¤ªå¤§ ï¼‰
    # Custom max body size
    # https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-max-body-size
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    # Client Body Buffer Size
    # https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#client-body-buffer-size
    nginx.ingress.kubernetes.io/client-body-buffer-size: 21m

    # Configuration snippet
    # Using this annotation you can add additional configuration to the NGINX location
    # https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#configuration-snippet
    nginx.ingress.kubernetes.io/configuration-snippet: |
        client_body_buffer_size 21m;

    # ä¸ºNginx Ingressé…ç½®HTTPSåè®®çš„åç«¯æœåŠ¡ï¼Œé»˜è®¤æ˜¯HTTPï¼Œå¦‚æœåç«¯æœåŠ¡æ··åˆä¸¤ç§åè®®ï¼Œå¯ä»¥é…ç½®å¤šä¸ªç›¸åŒåŸŸåçš„ Ingress
    # https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#backend-protocol
    nginx.ingress.kubernetes.io/backend-protocol:  "HTTPS"

```

è®¾ç½®é»˜è®¤çš„ ingressclass
`kubectl -n ingress-nginx patch ingressclass nginx -p '{"metadata":{"annotations":{"ingressclass.kubernetes.io/is-default-class":"true"}}}'`

```json
{
    "apiVersion": "networking.k8s.io/v1",
    "kind": "IngressClass",
    "metadata": {
        "annotations": {
            "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"networking.k8s.io/v1\",\"kind\":\"IngressClass\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"controller\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\",\"app.kubernetes.io/part-of\":\"ingress-nginx\",\"app.kubernetes.io/version\":\"1.11.2\"},\"name\":\"nginx\"},\"spec\":{\"controller\":\"k8s.io/ingress-nginx\"}}\n"
        },
        "labels": {
            "app.kubernetes.io/component": "controller",
            "app.kubernetes.io/instance": "ingress-nginx",
            "app.kubernetes.io/name": "ingress-nginx",
            "app.kubernetes.io/part-of": "ingress-nginx",
            "app.kubernetes.io/version": "1.11.2"
        },
        "name": "nginx"
    },
    "spec": {
        "controller": "k8s.io/ingress-nginx"
    }
}
```

### ConfigMap

`kubectl create configmap fortune-config --from-literal=sleep-interval=2 --from-literal=foo=bar --from-literal=bar=baz`
`kubectl create -f fortune-config.yaml`
`kubectl create configmap my-config --from-file=config-file.conf`
`kubectl create configmap my-config --from-file=/path/to/dir`

PATCH `kubectl patch configmap tcp-services --type merge -p '{"data":{"5672": "rabbitmq-system/rabbitmqcluster:5672"}}'`

CREATING A TLS CERTIFICATE FOR THE INGRESS
`kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key`
`kubectl create secret generic fortune-https --from-file=https.key --from-file=https.cert --from-file=foo`

```sh
# Copying Kubernetes Secrets Between Namespaces:
kubectl get secret <secret-name> --namespace=<source-namespace>â€Š -o yaml \
  | sed 's/namespace: <from-namespace>/namespace: <to-namespace>/' \
  | kubectl create -f -

# Update k8s ConfigMap or Secret without deleting the existing one
kubectl create configmap foo --from-file foo.properties -o yaml --dry-run=client | kubectl apply -f -

# åˆ›å»º configMap çš„åŒæ—¶å¢åŠ  label
kubectl label configmap my-config app=grafana env=test
kubectl create configmap --from-file=... --overrides='{"metadata":{"label":"app": "awesomeapp"}}'
kubectl create cm foo -o yaml --dry-run|kubectl label -f- --dry-run -o yaml --local f=b

# ä½¿ç”¨æ–‡ä»¶åˆ›å»ºï¼Œå¹¶å¼•ç”¨å·²æœ‰çš„æ–‡ä»¶ file1.txt
cat <<EOF > configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  file1.txt: |
    $(cat file1.txt)
  file2.txt: |
    $(cat file2.txt)
EOF
```

### System

`kubectl get events` æŸ¥çœ‹ç›¸å…³äº‹ä»¶

### [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)

* **request**: the scheduler uses this information to decide which node to place the Pod on. The kubelet also reserves at least the request amount of that system resource specifically for that container to use.  It is default to the limits if requests is not set explicitly.
* **limit**: the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set

#### Exceeding the limits

CPU: when a CPU limit is set for a container, the process isnâ€™t given more CPU time than the configured limit.
Memory: When a process tries to allocate memory over its limit, the process is killed (itâ€™s said the container is OOMKilled, where OOM stands for Out Of Memory)

#### pod QoS classes

* BestEffort (the lowest priority)
  1. Itâ€™s assigned to pods that donâ€™t have any requests or limits set at all (in any of their containers)
  2. They will be the first ones killed when memory needs to be freed for other pods.
* Burstable
  In between BestEffort and Guaranteed is the Burstable QoS class. All other pods fall into this class
  When two single-container pods exist, both in the Burstable class, the system will kill the one using more of its requested memory than the other, percentage-wise
* Guaranteed (the highest)
  1. Requests and limits need to be set for both CPU and memory.
  2. They need to be set for each container.
  3. They need to be equal (the limit needs to match the request for each resource in each container).

##### which process gets killed when memory is low

First in line to get killed are pods in the BestEffort class, followed by Burstable pods, and finally Guaranteed pods, which only get killed if system processes need memory.

### Advanced Scheduling

`kubectl taint node node1.k8s node-type=production:NoSchedule` adds a taint with key node-type, value production and the NoSchedule

Three possible effects exist:

* `NoSchedule`, which means pods wonâ€™t be scheduled to the node if they donâ€™t tol- erate the taint.
* `PreferNoSchedule` is a soft version of NoSchedule, meaning the scheduler will try to avoid scheduling the pod to the node, but will schedule it to the node if it canâ€™t schedule it somewhere else.
* `NoExecute`, unlike NoSchedule and PreferNoSchedule that only affect schedul- ing, also affects pods already running on the node. If you add a NoExecute taint to a node, pods that are already running on that node and donâ€™t tolerate the NoExecute taint will be evicted from the node.

### Deploy process

1. build image `docker build -t registry.cn-beijing.aliyuncs.com/namespace/image-name:image-version . -f deploy/Dockerfile`
2. å°†é•œåƒæ¨é€åˆ°Registry `docker push`
3. å¯åŠ¨ pod `kubectl run pod-name -n namespace --image=registry.cn-beijing.aliyuncs.com/namespace/image-name:image-version -it --rm --restart=Never -- bash`

#### Jenkins

[Kubernetes CLI](https://plugins.jenkins.io/kubernetes-cli/) éƒ¨ç½²å¤šä¸ªé›†ç¾¤

```groovy
withKubeConfig([credentialsId: 'k8s_config_prd'
                    ]) {
                  sh "kubectl config view"
                  sh "kubectl get pods"
                }
```

### Volume (Disk)

`Pod The node had condition: [DiskPressure]` è¿™ä¸ªå¼‚å¸¸ä¿¡æ¯ä¼šå‘ç”Ÿåœ¨ Pod åˆ›å»ºçš„æ—¶å€™ï¼Œå¦‚æœ Node æ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´åˆ›å»ºæ–°çš„ Podï¼Œå°±ä¼šæŠ›å‡ºè¿™ä¸ªå¼‚å¸¸

`The node was low on resource: ephemeral-storage` è¿™ä¸ªå¼‚å¸¸ä¿¡æ¯å‘ç”Ÿåœ¨ pod è¿è¡Œè¿‡ç¨‹ä¸­ï¼Œå¦‚æœ pod å†™äº†å¤§é‡çš„æ—¥å¿—ä¿¡æ¯ï¼Œå¯¼è‡´ç£ç›˜è¢«å¤§é‡ä½¿ç”¨ï¼Œå¯¼è‡´æ— æ³•ç»§ç»­è¿è¡Œï¼Œä¼šæŠ›å‡ºè¿™ä¸ªå¼‚å¸¸ã€‚

è®¾ç½® PVC å¤§å° `kubectl patch pvc pvc-name -p '{"spec":{"resources":{"requests":{"storage":"50Gi"}}}}'`

#### Unable to attach or mount volumes - timed out waiting for the condition

[Kubernetes - Kubelet Unable to attach or mount volumes - timed out waiting for the condition - vEducate.co.uk](https://veducate.co.uk/kubelet-unable-attach-volumes/)

The fix is to remove the stale VolumeAttachment. `kubectl delete volumeattachment [volumeattachment_name]`

[å®¹å™¨æœåŠ¡K8Så­˜å‚¨å·æŒ‚è½½å¸¸è§é—®é¢˜-é˜¿é‡Œäº‘å¼€å‘è€…ç¤¾åŒº](https://developer.aliyun.com/article/591884)

ç£ç›˜æŒ‚è½½æ—¥å¿—
tail -f /var/log/alicloud/diskplugin.csi.alibabacloud.com.log | grep d-2zeheqwxc2jrasb0fonj
tail -n 1000 /var/log/messages | grep kubelet |less
kg VolumeAttachment | sort -k 3

### Network

[Mastering Kubernetes Pod-to-Pod Communication: A Comprehensive Guide | by Extio Technology | Medium](https://medium.com/@extio/mastering-kubernetes-pod-to-pod-communication-a-comprehensive-guide-46832b30556b)

Kubernetes å¯¹é›†ç¾¤ç½‘ç»œæœ‰ä»¥ä¸‹è¦æ±‚ï¼š
æ‰€æœ‰çš„ Pod ä¹‹é—´å¯ä»¥åœ¨ä¸ä½¿ç”¨ NAT ç½‘ç»œåœ°å€è½¬æ¢çš„æƒ…å†µä¸‹ç›¸äº’é€šä¿¡ï¼›æ‰€æœ‰çš„ Node ä¹‹é—´å¯ä»¥åœ¨ä¸ä½¿ç”¨ NAT ç½‘ç»œåœ°å€è½¬æ¢çš„æƒ…å†µä¸‹ç›¸äº’é€šä¿¡ï¼›æ¯ä¸ª Pod çœ‹åˆ°çš„è‡ªå·±çš„ IP å’Œå…¶ä»– Pod çœ‹åˆ°çš„ä¸€è‡´ã€‚

#### Kubernetes CNI

[ä»é›¶å¼€å§‹å…¥é—¨ K8s | ç†è§£ CNI å’Œ CNI æ’ä»¶](https://developer.aliyun.com/article/748866)
[kubernetesç½‘ç»œæ¨¡å‹ä¹‹â€œå°è€Œç¾â€flannel](https://zhuanlan.zhihu.com/p/79270447)

CNIï¼Œå®ƒçš„å…¨ç§°æ˜¯ Container Network Interfaceï¼Œå³å®¹å™¨ç½‘ç»œçš„ API æ¥å£ã€‚

å®ƒæ˜¯ K8s ä¸­æ ‡å‡†çš„ä¸€ä¸ªè°ƒç”¨ç½‘ç»œå®ç°çš„æ¥å£ã€‚Kubelet é€šè¿‡è¿™ä¸ªæ ‡å‡†çš„ API æ¥è°ƒç”¨ä¸åŒçš„ç½‘ç»œæ’ä»¶ä»¥å®ç°ä¸åŒçš„ç½‘ç»œé…ç½®æ–¹å¼ï¼Œå®ç°äº†è¿™ä¸ªæ¥å£çš„å°±æ˜¯ CNI æ’ä»¶ï¼Œå®ƒå®ç°äº†ä¸€ç³»åˆ—çš„ CNI API æ¥å£ã€‚å¸¸è§çš„ CNI æ’ä»¶åŒ…æ‹¬ Calicoã€flannelã€Terwayã€Weave Net ä»¥åŠ Contivã€‚

æ’ä»¶è´Ÿè´£ä¸ºæ¥å£é…ç½®å’Œç®¡ç†IPåœ°å€ï¼Œå¹¶ä¸”é€šå¸¸æä¾›ä¸IPç®¡ç†ã€æ¯ä¸ªå®¹å™¨çš„IPåˆ†é…ã€ä»¥åŠå¤šä¸»æœºè¿æ¥ç›¸å…³çš„åŠŸèƒ½ã€‚å®¹å™¨è¿è¡Œæ—¶ä¼šè°ƒç”¨ç½‘ç»œæ’ä»¶ï¼Œä»è€Œåœ¨å®¹å™¨å¯åŠ¨æ—¶åˆ†é…IPåœ°å€å¹¶é…ç½®ç½‘ç»œï¼Œå¹¶åœ¨åˆ é™¤å®¹å™¨æ—¶å†æ¬¡è°ƒç”¨å®ƒä»¥æ¸…ç†è¿™äº›èµ„æºã€‚

K8s é€šè¿‡ CNI é…ç½®æ–‡ä»¶æ¥å†³å®šä½¿ç”¨ä»€ä¹ˆ CNIã€‚åŸºæœ¬çš„ä½¿ç”¨æ–¹æ³•ä¸ºï¼š

1. é¦–å…ˆåœ¨æ¯ä¸ªç»“ç‚¹ä¸Šé…ç½® CNI é…ç½®æ–‡ä»¶(/etc/cni/net.d/xxnet.conf)ï¼Œå…¶ä¸­ xxnet.conf æ˜¯æŸä¸€ä¸ªç½‘ç»œé…ç½®æ–‡ä»¶çš„åç§°ï¼›
2. å®‰è£… CNI é…ç½®æ–‡ä»¶ä¸­æ‰€å¯¹åº”çš„äºŒè¿›åˆ¶æ’ä»¶ï¼›
3. åœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸Šåˆ›å»º Pod ä¹‹åï¼ŒKubelet å°±ä¼šæ ¹æ® CNI é…ç½®æ–‡ä»¶æ‰§è¡Œå‰ä¸¤æ­¥æ‰€å®‰è£…çš„ CNI æ’ä»¶ï¼›
4. ä¸Šæ­¥æ‰§è¡Œå®Œä¹‹åï¼ŒPod çš„ç½‘ç»œå°±é…ç½®å®Œæˆäº†ã€‚

#### Kubernetesé›†ç¾¤é€šä¿¡çš„å®ç°åŸç†

[K8s networkä¹‹å››ï¼šKubernetesé›†ç¾¤é€šä¿¡çš„å®ç°åŸç† | Mr.Muzi](https://marcuseddie.github.io/2021/K8s-Network-Architecture-section-four.html)

#### Kubernetesé›†ç¾¤Podå’ŒServiceä¹‹é—´é€šä¿¡çš„å®ç°åŸç†

[K8s networkä¹‹äº”ï¼šKubernetesé›†ç¾¤Podå’ŒServiceä¹‹é—´é€šä¿¡çš„å®ç°åŸç† | Mr.Muzi](https://marcuseddie.github.io/2021/K8s-Network-Architecture-section-five.html)

iptablesä»£ç†æ¨¡å‹

iptablesä»Kubernetes v1.2ç‰ˆæœ¬å¼€å§‹ç§°ä¸ºkube-proxyçš„é»˜è®¤æ¨¡å¼ï¼Œç›´åˆ°åœ¨v1.12ç‰ˆæœ¬ä¸­è¢«IPVSå–ä»£è€Œæˆä¸ºæ–°çš„kube-proxyé»˜è®¤å·¥ä½œæ¨¡å¼ã€‚åœ¨è¯¥æ¨¡å¼ä¸­ï¼Œkube-proxyä¿®æ”¹äº†iptablesä¸­çš„filterå’Œnatè¡¨ï¼ŒåŒæ—¶åˆå¯¹iptablesçš„é“¾è¿›è¡Œäº†æ‰©å……ï¼Œè‡ªå®šä¹‰äº†`KUBE-SERVICESï¼ŒKUBE-SVC-<HASH>ï¼ŒKUBE-SEP-<HASH>ï¼ŒKUBE-NODEPORTSï¼ŒKUBE-FW-<HASH>ï¼ŒKUBE-XLB-<HASH>ï¼ŒKUBE-POSTROUTINGï¼ŒKUBE-MARK-MASQå’ŒKUBE-MARK-DROP`ä¹ä¸ªé“¾æ¡ï¼Œé€šè¿‡åœ¨`KUBE-SERVICES`é“¾ä¸­åŠ å…¥æ¯ä¸ªServiceçš„ClusterIPå’Œç«¯å£çš„åŒ¹é…è§„åˆ™æ¥å®Œæˆæµé‡çš„åŒ¹é…å’Œé‡å®šå‘å·¥ä½œã€‚Kubernetesè‡ªå®šä¹‰çš„é“¾æ¡ä¸iptablesåŸç”Ÿé“¾æ¡çš„è°ƒç”¨å…³ç³»å¦‚å›¾ 4 æ‰€ç¤º

![å›¾4 Kubernetes è‡ªå®šä¹‰é“¾ä¸ä¸iptablesè°ƒç”¨é“¾çš„å…³ç³»](image/Relationship-between-Kube-customed-chains-and-iptables-demo.png)

Cluster IP
â€ƒâ€ƒå½“é›†ç¾¤å†…Podè®¿é—®Serviceçš„Cluster IPæ—¶ï¼ŒæŠ¥æ–‡ä¼šé€šè¿‡iptablesçš„OUTPUTé“¾è¿›å…¥Kubernetesçš„è‡ªå®šä¹‰é“¾ã€‚å‡è®¾å½“å‰é›†ç¾¤ä¸­æœ‰ä¸€ä¸ªService Aï¼ŒåŒæ—¶æœ‰ä¸‰ä¸ªåç«¯Podç”¨æ¥æä¾›æœåŠ¡Service Aï¼Œkube-proxyé‡‡ç”¨éšæœºè´Ÿè½½å‡è¡¡ç®—æ³•æ¥é€‰æ‹©Podï¼Œé’ˆå¯¹Cluster IPçš„å¤„ç†æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![å›¾-6 Serviceåœ¨Cluster IPæ¨¡å¼ä¸‹æŠ¥æ–‡å¤„ç†è¿‡ç¨‹](image/K8-svc-iptables-clusterIP-mode-flow.png)

Note: æŸäº›é“¾æ¡çš„åå­—ä¸­å«æœ‰ <HASH>å­—æ ·ï¼Œè¿™æ˜¯è¿ç”¨SHA256ç®—æ³•å¯¹â€œnamespace + name + portname+åè®®åâ€ç”Ÿæˆå“ˆå¸Œå€¼ï¼Œç„¶åé€šè¿‡base32å¯¹è¯¥å“ˆå¸Œå€¼ç¼–ç ï¼Œæœ€åå–ç¼–ç å€¼çš„å‰16ä½çš„å€¼ã€‚

1. æŠ¥æ–‡ä¼šé¦–å…ˆè¿›å…¥KUBE-SERVICESé“¾æ¡ã€‚KUBE-SERVICEé’ˆå¯¹æ¯ä¸ªServiceä¼šäº§ç”Ÿä¸¤æ¡åŒ¹é…è§„åˆ™ï¼Œè§„åˆ™ï¼ˆ1ï¼‰è¡¨ç¤ºå¦‚æœæŠ¥æ–‡çš„æºåœ°å€ä¸æ˜¯é›†ç¾¤å†…IPåœ°å€ï¼ŒåŒæ—¶ï¼ŒæŠ¥æ–‡åŒ¹é…äº†è¯·æ±‚Serviceçš„åè®®å’Œç«¯å£ï¼Œé‚£ä¹ˆå°±è·³è½¬åˆ°ï¼ˆ-jï¼‰KUBE-MARK-MASQé“¾æ¡ï¼Œåœ¨æŠ¥æ–‡ä¸­åŠ å…¥ä¸€ä¸ªç‰¹æ®Šçš„é˜²ç«å¢™æ ‡è¯†ï¼Œæ‰“ä¸Šè¿™ä¸ªæ ‡è¯†çš„æŠ¥æ–‡ä¼šåœ¨POSTROUTINGé˜¶æ®µæ‰§è¡ŒSNAT(Source Network Address Translation)ã€‚å¦‚æœç¡®å®å‘½ä¸­äº†è§„åˆ™ï¼ˆ1ï¼‰ï¼Œé‚£ä¹ˆåœ¨æ‰“å®Œæ ‡è®°åä¼šç»§ç»­æ£€æŸ¥è§„åˆ™ï¼ˆ2ï¼‰ï¼Œè§„åˆ™ï¼ˆ2ï¼‰ä¼šå°†æŠ¥æ–‡å¸¦å…¥ä¸‹ä¸€ä¸ªé“¾æ¡ KUBE-SVC-<HASH>ã€‚
2. KUBE-SVC-<HASH>åŒ…å«äº†å½“å‰æä¾›Serviceçš„åç«¯Podã€è´Ÿè½½å‡è¡¡æ¨¡å¼ç­‰æ¶ˆæ¯ã€‚kube-proxyé»˜è®¤é‡‡ç”¨çš„éšæœºè´Ÿè½½ç®—æ³•ï¼Œå› æ­¤åœ¨è¿™ç§ç®—æ³•ä¸‹ä¼šä¸ºæ¯ä¸ªPodåˆ†é…ä¸€ä¸ªå‘½ä¸­æ¦‚ç‡ã€‚åœ¨å›¾-6ä¸­ï¼Œä¸‰ä¸ªPodè¢«å‘½ä¸­çš„æ¦‚ç‡éƒ½æ˜¯ä¸‰åˆ†ä¹‹ä¸€ã€‚å½“é€‰ä¸­ä¸€ä¸ªPodåï¼Œå°±ä¼šè·³è½¬åˆ°å’ŒPodç›¸å¯¹åº”çš„KUBE-SEP-<HASH>ä¸Šã€‚
3. æ¯ä¸ªKUBE-SEP-<HASH>å’Œä¸€ä¸ªPodç›¸å¯¹åº”ï¼Œä¸”æ¯ä¸ªKUBE-SEP-<HASH>å‡æœ‰ä¸¤æ¡è§„åˆ™ã€‚è§„åˆ™ï¼ˆ2ï¼‰è¡¨ç¤ºå¯¹è¯·æ±‚åšDNATï¼Œå°†è¯·æ±‚çš„ç›®çš„åœ°å€ç”±åŸæ¥çš„ClusterIPï¼šPortè½¬æ¢æˆPod_IPï¼šPortã€‚è¿™æ ·å°±å°†Podè®¿é—®Serviceå˜æˆäº†Podå’ŒPodä¹‹é—´çš„è®¿é—®ã€‚è§„åˆ™ï¼ˆ1ï¼‰çš„ç›®çš„æ˜¯ä¸ºäº†åº”å¯¹Hairpinning å‘å¤¹é—®é¢˜è€Œè®¾è®¡çš„ï¼šService Açš„åç«¯Podä¸­æœ‰å¯èƒ½ä¼šæœ‰æŸä¸ªPodè®¿é—®Service Aï¼Œç„¶åç»è¿‡iptablesæ—¶åˆæ°å¥½é€‰ä¸­äº†è‡ªå·±ä½œä¸ºæœåŠ¡çš„æä¾›æ–¹ã€‚æ¢å¥è¯è¯´ï¼ŒPodè¦ä¸ºè‡ªå·±å‘å‡ºå»çš„æœåŠ¡è¯·æ±‚åšå‡ºå“åº”ã€‚åœ¨Kubernetesä¸­è¿™æ ·ä¼šé€ æˆè®¿é—®å¤±è´¥ï¼Œå¦‚æœå½“å‡ºç°è¿™ç§åœºæ™¯æ—¶å°±è·³è½¬åˆ°KUBE-MARK-MASQé“¾æ¡æ‰§è¡ŒSNATï¼Œå°†è¯·æ±‚çš„æºåœ°å€ç”±Podè‡ªèº«å˜æˆèŠ‚ç‚¹çš„node IPï¼Œè¿™æ ·å°±åˆå˜æˆäº†æ­£å¸¸çš„æœåŠ¡è¯·æ±‚å’Œå“åº”æ¨¡å¼ã€‚å¦‚å›¾7æ‰€ç¤ºï¼Œå·¦è¾¹æ˜¯æ²¡æœ‰åšSNATçš„åœºæ™¯ï¼ŒPod Aæ”¶åˆ°äº†ä¸€ä¸ªè‡ªå·±å‘å‡ºçš„æœåŠ¡è¯·æ±‚ï¼Œè¯·æ±‚çš„æºå’Œç›®çš„åœ°å€éƒ½æ˜¯è‡ªå·±ï¼Œå½“å‘é€å“åº”ç»™è‡ªå·±æ—¶ä¼šå¯¼è‡´å¤±è´¥ã€‚å³è¾¹æ˜¯å€ŸåŠ©SNATè§£å†³Hairpinningé—®é¢˜çš„åœºæ™¯ï¼ŒPod Aè®¿é—®è‡ªå·±æ‰€å±æœåŠ¡çš„è¯·æ±‚åˆ°è¾¾Linuxå†…æ ¸æ—¶ä¼šé€šè¿‡SNATå°†æºåœ°å€ç”±Pod Açš„IPå˜æˆèŠ‚ç‚¹çš„Node IPã€‚å½“Pod Aå‘é€å“åº”æŠ¥æ–‡æ—¶ï¼ŒæŠ¥æ–‡å…ˆå‘é€ç»™Node IPï¼Œç„¶ååœ¨Linuxå†…æ ¸ä¸­å†æ¬¡è¿›è¡ŒNATï¼Œå°†æºIPç”±Pod Açš„IPæ”¹æˆServiceçš„IPï¼Œç›®çš„IPç”±NodeèŠ‚ç‚¹çš„IPæ”¹ä¸ºPod Açš„IPï¼Œè¿™æ ·å°±å¯ä»¥æ­£å¸¸å·¥ä½œäº†ã€‚
  ![Hairpinningé—®é¢˜åŠå…¶è§£å†³æ–¹æ³•](image/K8s-iptables-Hairpinning-demo.png)
  å›¾ - 7 Hairpinningé—®é¢˜åŠå…¶è§£å†³æ–¹æ³•

4. æ‰§è¡Œå®ŒDNATåï¼Œä¼šè·³è½¬åˆ°POSTROUTINGé“¾æ¡ã€‚POSTROUTINGä¼šæ— æ¡ä»¶è·³è½¬åˆ°KUBE-POSTROUTINGé“¾æ¡ï¼Œè¿™ä¸ªé“¾æ¡ä¼šæ£€æŸ¥æŠ¥æ–‡æ˜¯å¦æœ‰è·³è½¬åˆ°KUBE-MARK-MASQé“¾æ¡è¢«æ‰“ä¸Šé˜²ç«å¢™æ ‡è¯†ï¼Œå¦‚æœæœ‰çš„è¯å°±ä¼šæ‰§è¡ŒSNATï¼Œå°†æŠ¥æ–‡çš„æºåœ°å€å˜ä¸ºèŠ‚ç‚¹çš„node IPã€‚
5. æœ€åç”±POSTROUTINGå°†æŠ¥æ–‡å‘å‡ºåè®®æ ˆã€‚

NOTE: SNATå’ŒMASQUERADçš„åŒºåˆ«

* SNATæ˜¯æŒ‡åœ¨æ•°æ®åŒ…ä»ç½‘å¡å‘é€å‡ºå»çš„æ—¶å€™ï¼ŒæŠŠæ•°æ®åŒ…ä¸­çš„æºåœ°å€éƒ¨åˆ†æ›¿æ¢ä¸ºæŒ‡å®šçš„IPï¼Œè¿™æ ·ï¼Œæ¥æ”¶æ–¹å°±è®¤ä¸ºæ•°æ®åŒ…çš„æ¥æºæ˜¯è¢«æ›¿æ¢çš„é‚£ä¸ªæŒ‡å®šIPçš„ä¸»æœºã€‚
* MASQUERADEæ˜¯SNATçš„ä¸€ä¸ªç‰¹ä¾‹ã€‚MASQUERADEæ˜¯ç”¨å‘é€æ•°æ®çš„ç½‘å¡ä¸Šçš„IPæ¥æ›¿æ¢æºIPï¼Œå› æ­¤ï¼Œå¯¹äºé‚£äº›IPä¸å›ºå®šçš„åœºåˆï¼Œæ¯”å¦‚é€šè¿‡DHCPåˆ†é…IPçš„æƒ…å†µä¸‹ï¼Œå°±å¾—ç”¨MASQUERADEã€‚

##### åŒä¸€ä¸ª Node èŠ‚ç‚¹å†…çš„ Pod ä¸èƒ½é€šè¿‡ Service äº’è®¿

ä¿®æ”¹é›†ç¾¤kube proxy é…ç½® masqueradeAll true
[Kubernetes åŒä¸€ä¸ª Node èŠ‚ç‚¹å†…çš„ Pod ä¸èƒ½é€šè¿‡ Service äº’è®¿ - å“ˆå¸Œ](https://www.haxi.cc/archives/Kubernetes-åŒä¸€ä¸ª-Node-èŠ‚ç‚¹å†…çš„-Pod-ä¸èƒ½é€šè¿‡-Service-äº’è®¿.html)
[Kubernetes åŒä¸€ä¸ª Node èŠ‚ç‚¹å†…çš„ Pod ä¸èƒ½é€šè¿‡ Service äº’è®¿ - é¢å£è€…çš„é€»è¾‘ - åšå®¢å›­](https://www.cnblogs.com/longgor/p/13588191.html)

[Debug Services | Kubernetes](https://kubernetes.io/docs/tasks/debug/debug-application/debug-service)

[Virtual IPs and Service Proxies | Kubernetes](https://kubernetes.io/docs/reference/networking/virtual-ips/)
[IP Masquerade Agent User Guide | Kubernetes](https://kubernetes.io/docs/tasks/administer-cluster/ip-masq-agent/)

[kubernetes - Pods running on the same node can't access to each other through service - Stack Overflow](https://stackoverflow.com/questions/64073696/pods-running-on-the-same-node-cant-access-to-each-other-through-service/78910247#78910247)

åŸå› ï¼škube-proxyé€šè¿‡åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šåˆ›å»ºç›¸åŒçš„ipvsè§„åˆ™ï¼ˆå…³é—­rapï¼‰ï¼Œå½“podè®¿é—®é›†ç¾¤å†…svcï¼ˆvipï¼‰æ—¶ï¼Œè¯·æ±‚ä¼šè¢«å½“å‰èŠ‚ç‚¹vipæ¥å—ï¼Œæ­¤æ—¶ï¼Œipvsä¼šè¿›è¡ŒDNATæ“ä½œï¼Œè€Œåœ¨å›æŠ¥æ—¶ï¼Œä¸¤ä¸ªpodå¤„äºåŒä¸€ä¸ªveth-partçš„ä¸€é¢ï¼Œæ­¤æ—¶æµé‡å¹¶ä¸ä¼šèµ°ç½‘å…³ï¼Œæ‰€ä»¥å›æŠ¥çš„æ—¶å€™æºipå’Œç›®çš„ipéƒ½æ˜¯ä¸¤ä¸ªpodçš„ipï¼Œä½†æ˜¯åœ¨è¯·æ±‚å‘é€æ—¶ï¼Œç›®çš„ipä¸ºvipï¼Œæ­¤æ—¶ä¼šä¸¢å¼ƒæ‰è¯·æ±‚ã€‚

using flannel as CNI on Kubernetes v1.30.1 and it turned out that flannel needs masquerade to be set true while kube-proxy default value has masqueradeAll: false. Changing it to true and restarting kube-proxy pods solved the problem

the steps:

```sh
# set masqueradeAll: true
kubectl -n kube-system edit cm kube-proxy
# to restart all kube proxy pods
kubectl -n kube-system delete pod -l k8s-app=kube-proxy
```

#### Flannel

[ä» Flannel å­¦ä¹  Kubernetes overlay ç½‘ç»œ](https://atbug.com/cross-node-traffic-on-flannel-vxlan-network/)

```sh
# IP å‘½ä»¤

# ip neighbour show | awk '$3=="flannel.1"{print $0}'
10.244.2.0 dev flannel.1 lladdr 5e:5f:7e:bd:65:ea PERMANENT
10.244.0.0 dev flannel.1 lladdr 76:7d:9c:b5:29:9d PERMANENT

# bridge fdb show flannel.1 |awk '$3=="flannel.1"{print $0}'
26:22:89:97:04:e8 dev flannel.1 dst 192.168.102.103 self permanent    # è½¬å‘è‡³k8s-node2èŠ‚ç‚¹
76:7d:9c:b5:29:9d dev flannel.1 dst 192.168.102.101 self permanent    # è½¬å‘è‡³k8s-masterèŠ‚ç‚¹
```

#### kubectl netshoot

[nicolaka/netshoot: a Docker + Kubernetes network trouble-shooting swiss-army container](https://github.com/nicolaka/netshoot)

```sh
kubectl netshoot debug podName --image-name nicolaka/netshoot --image-tag v0.13
# spin up a throwaway pod for troubleshooting
kubectl netshoot run tmp-shell --image-name nicolaka/netshoot --image-tag v0.13

# debug using an ephemeral container in an existing pod
kubectl netshoot debug my-existing-pod

# create a debug session on a node
kubectl netshoot debug node/my-node
```

#### é˜¿é‡Œäº‘ACKé›†ç¾¤ä¸­çš„ç½‘ç»œé—®é¢˜

[é˜¿é‡Œäº‘ACKé›†ç¾¤ä¸­çš„ç½‘ç»œé—®é¢˜](https://itopic.org/ack-network-issue.html)
[pod æ— æ³•è®¿é—®é›†ç¾¤ ingress slb ç»‘å®šçš„å…¬ç½‘åŸŸå | Blog](https://zijin-m.github.io/Blog/problems/k8s/externalTrafficPolicy-local.html)

#### é˜¿é‡Œäº‘Kubernetesæ‰˜ç®¡ç‰ˆå¼€å¯ hairpin æ¨¡å¼

æ®å®¢æœå›å¤ç›®å‰(2020-11-20)æ²¡æœ‰ç®€å•çš„é…ç½®æ–¹å¼, ä½†å‘ç°ä¸‹é¢çš„æ–¹å¼æš‚æ—¶æœ‰æ•ˆæœ:

1. ç¡®è®¤ç½‘ç»œæ’ä»¶ç±»å‹æ˜¯ csi
`ps aux | grep kubelet | grep /usr/bin/kubelet | grep network-plugin`

2. æ‰¾åˆ°é…ç½®æ–‡ä»¶ä½ç½® `ls /etc/cni/net.d/*flannel.conf`
åœ¨æ¯å° node æœºå™¨ä¸Šçš„é…ç½®æ–‡ä»¶ /etc/cni/net.d/10-flannel.conf å¢åŠ   "hairpinMode": true

```yaml
{
  "name": "cb0",
  "cniVersion":"0.3.0",
  "type": "flannel",
  "delegate": {
    # "hairpinMode": true,
    "isDefaultGateway": true
  }
}
```

```sh
# scp 10-flannel.conf host1:/etc/cni/net.d/10-flannel.conf
for host in host1 host2; do echo $host; scp 10-flannel.conf $host:/etc/cni/net.d/10-flannel.conf; done

ç”Ÿæˆ pod åä¿®æ”¹æ–¹å¼
for intf in /sys/devices/virtual/net/cni0/brif/*; do echo 1 > $intf/hairpin_mode; done

# éªŒè¯
for intf in /sys/devices/virtual/net/cni0/brif/*; do echo "$intf"; cat $intf/hairpin_mode; done
```

## Kubernetes internal

### Components of the Control Plane

* The etcd distributed persistent storage
* The API server
* The Scheduler
* The Controller Manager

These components store and manage the state of the cluster, but they arenâ€™t what runs the application containers.

### Components running on the worker nodes

The task of running your containers is up to the components running on each worker node:

* The Kubelet
* The Kubernetes Service Proxy (kube-proxy)
* The Container Runtime (Docker, rkt, or others)

## Useful image

`kubectl run -it --rm --restart=Never --image=mysql:8.0.28 mysql-client -- mysql`
`kubectl run -it --rm --restart=Never --image=redis:6.0.9 redis-client -- bash`
`kubectl run -it --rm --restart=Never --image=tutum/dnsutils dnsutils`
`kubectl run -it --rm --restart=Never --image=tutum/dnsutils dnsutils -- dig SRV kubia.default.svc.cluster.local`
`kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools`
`kubectl run -it --rm --restart=Never --image=tutum/curl curl`
`kubectl run -it --rm --image=nicolaka/netshoot netshoot`
`kubectl run -it --rm --image=nginx nginx`
`kubectl run -it --rm --image=busybox busybox`  busybox: BusyBox combines tiny versions of many common UNIX utilities
`kubectl run -it --rm --image=alpine alpine`  alpine: A minimal Docker image based on Alpine Linux
`apk add curl` install curl

## helm

[Helm | Installing Helm](https://helm.sh/docs/intro/install/)

[Harbor docs | Managing Helm Charts](https://goharbor.io/docs/2.7.0/working-with-projects/working-with-images/managing-helm-charts/)

### helm Chart Management

```sh
helm create <name>         # Creates a chart directory along with the common files and directories used in a chart.
helm package <chart-path>               # Packages a chart into a versioned chart archive file.
helm lint <chart>                       # Run tests to examine a chart and identify possible issues:
helm show all <chart>                   # Inspect a chart and list its contents:
helm show values <chart>                # Displays the contents of the values.yaml file
helm pull <chart>                       # Download/pull chart
helm pull <chart> --untar=true          # If set to true, will untar the chart after downloading it
helm pull <chart> --verify              # Verify the package before using it
helm pull <chart> --version <number>    # Default-latest is used, specify a version constraint for the chart version to use
helm dependency list <chart>            # Display a list of a chartâ€™s dependencies:
```

### helm push chart

[Harbor docs | Managing Helm Charts](https://goharbor.io/docs/2.7.0/working-with-projects/working-with-images/managing-helm-charts/)

Push Charts to the Repository Server with the CLI

```sh
# As an alternative, you can also upload charts via the CLI. It is not supported by the native helm CLI. A plugin from the community should be installed before pushing. Run helm plugin install to install the push plugin first.
helm plugin install https://github.com/chartmuseum/helm-push
helm push --ca-file=ca.crt --username=admin --password=passw0rd chart_repo/hello-helm-0.1.0.tgz myrepo

# if your helm version is >= v3.7.0, please use the following command
helm cm-push --ca-file=ca.crt --username=admin --password=passw0rd chart_repo/hello-helm-0.1.0.tgz myrepo
```

### Install and Uninstall Apps

```sh
helm install <name> <chart>                           # Install the chart with a name
helm install <name> <chart> --namespace <namespace>   # Install the chart in a specific namespace
helm install <name> <chart> --set key1=val1,key2=val2 # Set values on the command line (can specify multiple or separate values with commas)
helm install <name> <chart> --values <yaml-file/url>  # Install the chart with your specified values
helm install <name> <chart> --dry-run --debug         # Run a test installation to validate chart (p)
helm install <name> <chart> --verify                  # Verify the package before using it
helm install <name> <chart> --dependency-update       # update dependencies if they are missing before installing the chart
helm uninstall <name>                                 # Uninstall a release
```

There are two ways to pass configuration data during install

* `--values` (or `-f`): Specify a YAML file with overrides. This can be specified multiple times and the rightmost file will take precedence
* `--set`: Specify overrides on the command line.

### Perform App Upgrade and Rollback

```sh
helm upgrade <release> <chart>                            # Upgrade a release
helm upgrade <release> <chart> --atomic                   # If set, upgrade process rolls back changes made in case of failed upgrade.
helm upgrade <release> <chart> --dependency-update        # update dependencies if they are missing before installing the chart
helm upgrade <release> <chart> --version <version_number> # specify a version constraint for the chart version to use
helm upgrade <release> <chart> --values                   # specify values in a YAML file or a URL (can specify multiple)
helm upgrade <release> <chart> --set key1=val1,key2=val2  # Set values on the command line (can specify multiple or separate valuese)
helm upgrade <release> <chart> --force                    # Force resource updates through a replacement strategy
helm rollback <release> <revision>                        # Roll back a release to a specific revision
helm rollback <release> <revision>  --cleanup-on-fail     # Allow deletion of new resources created in this rollback when rollback fails
```

### List, Add, Remove, and Update Repositories

```sh
helm repo add <repo-name> <url>   # Add a repository from the internet:
helm repo list                    # List added chart repositories
helm repo update                  # Update information of available charts locally from chart repositories
helm repo remove <repo_name>      # Remove one or more chart repositories
helm repo index <DIR>             # Read the current directory and generate an index file based on the charts found.
helm repo index <DIR> --merge     # Merge the generated index with an existing index file
helm search repo <keyword>        # Search repositories for a keyword in charts
helm search hub <keyword>         # Search for charts in the Artifact Hub or your own hub instance
```

### ä¸‹è½½ dependency ç¦»çº¿å®‰è£…

[Helm | Helm Dependency](https://helm.sh/docs/helm/helm_dependency/)

Starting from 2.2.0, repository can be defined as the path to the directory of the dependency charts stored locally. The path should start with a prefix of "file://". For example,

```yaml
# Chart.yaml
dependencies:
- name: nginx
  version: "1.2.3"
  repository: "file://../dependency_chart/nginx"
```

### Helm Release monitoring

```sh
helm list                       # Lists all of the releases for a specified namespace, uses current namespace context if namespace not specified
helm list --all                 # Show all releases without any filter applied, can use -a
helm list --all-namespaces      # List releases across all namespaces, we can use -A
helm list -l key1=value1,key2=value2 # Selector (label query) to filter on, supports '=', '==', and '!='
helm list --date                # Sort by release date
helm list --deployed            # Show deployed releases. If no other is specified, this will be automatically enabled
helm list --pending             # Show pending releases
helm list --failed              # Show failed releases
helm list --uninstalled         # Show uninstalled releases (if 'helm uninstall --keep-history' was used)
helm list --superseded          # Show superseded releases
helm list -o yaml               # Prints the output in the specified format. Allowed values: table, json, yaml (default table)
helm status <release>           # This command shows the status of a named release.
helm status <release> --revision <number>   # if set, display the status of the named release with revision
helm history <release>          # Historical revisions for a given release.
helm env                        # Env prints out all the environment information in use by Helm.
```

### Download Release Information

```sh
helm get all <release>      # A human readable collection of information about the notes, hooks, supplied values, and generated manifest file of the given release.
helm get hooks <release>    # This command downloads hooks for a given release. Hooks are formatted in YAML and separated by the YAML '---\n' separator.
helm get manifest <release> # A manifest is a YAML-encoded representation of the Kubernetes resources that were generated from this release's chart(s). If a chart is dependent on other charts, those resources will also be included in the manifest.
helm get notes <release>    # Shows notes provided by the chart of a named release.
helm get values <release>   # Downloads a values file for a given release. use -o to format output
```

### Plugin Management

```sh
helm plugin install <path/url1>     # Install plugins
helm plugin list                    # View a list of all installed plugins
helm plugin update <plugin>         # Update plugins
helm plugin uninstall <plugin>      # Uninstall a plugin
```

### helm ä½¿ç”¨ä¾‹å­

```sh
/usr/local/bin/helm

helm repo add ali-incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator
helm repo add ali-stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
#æ·»åŠ é˜¿é‡Œäº‘çš„ chart ä»“åº“
helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
# æ·»åŠ ç§æœ‰habor ä»“åº“
helm repo add --username=admin --password=xxxxxxxx my_harbor https://xxxxx:8443/chartrepo/library

#æŸ¥çœ‹é…ç½®çš„ chart ä»“åº“æœ‰å“ªäº›
helm repo list
#åˆ é™¤ chart ä»“åº“åœ°å€
helm repo remove aliyun
#ä»æŒ‡å®š chart ä»“åº“åœ°å€æœç´¢ chart
helm search repo aliyun
helm search repo gitlab-ce
# æŸ¥çœ‹ chart ä¿¡æ¯
helm show chart aliyun/memcached
# To see all configurable options with detailed comments
helm show values prometheus-community/kube-prometheus-stack
# Displays the contents of the values.yaml file. get a simple idea of the features of this chart
helm show chart prometheus-community/kube-prometheus-stack
# Inspect a chart and list its contents
helm show all prometheus-community/kube-prometheus-stack

helm fetch ali-stable/gitlab-ce

# Install charts
# helm install command can install from several sources
# A chart repository
helm install myrelease oci://<registry url>/<project>/<chart name> --version <version>
# A local chart archive
helm install foo foo-0.1.1.tgz
# An unpacked chart directory
helm install foo path/to/foo
# A full URL
helm install foo https://example.com/charts/foo-1.2.3.tgz

# å¯¼å‡ºé»˜è®¤å€¼æ–‡ä»¶ values.yaml
helm inspect values prometheus-community/kube-prometheus-stack > values.yaml
# ä½¿ç”¨é…ç½®æ–‡ä»¶ä¿®æ”¹é»˜è®¤å€¼
helm install kubernetes-dashboard /data/kubernetes-dashboard-7.5.tgz -f /data/value.yaml --create-namespace --namespace kubernetes-dashboard

helm uninstall gitlab

# To keep track of a release's state, or to re-read configuration information
helm status happy-panda

helm registry login <registry url>
```

ä¸‹è½½ chart åŒ…åˆ°æœ¬åœ°

```sh
# ä¸‹è½½ chart åŒ…åˆ°æœ¬åœ°
[root@master1 ~]# helm pull aliyun/memcached
[root@master1 ~]# tar zxvf memcached-2.0.1.tgz
[root@master1 ~]# cd memcached
[root@master1 memcached]# ls
Chart.yaml README.md templates values.yaml
# Chart.yamlï¼š chart çš„åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç‰ˆæœ¬åå­—ä¹‹ç±»
# templatesï¼šå­˜æ”¾ k8s çš„éƒ¨ç½²èµ„æºæ¨¡æ¿ï¼Œé€šè¿‡æ¸²æŸ“å˜é‡å¾—åˆ°éƒ¨ç½²æ–‡ä»¶
# values.yamlï¼šå­˜æ”¾å…¨å±€å˜é‡ï¼Œtemplates ä¸‹çš„æ–‡ä»¶å¯ä»¥è°ƒç”¨

[root@xianchaomaster1 memcached]# cd templates/
[root@xianchaomaster1 templates]# ls
_helpers.tpl NOTES.txt pdb.yaml statefulset.yaml svc.yaml
# _helpers.tpl å­˜æ”¾èƒ½å¤Ÿå¤ç”¨çš„æ¨¡æ¿
# NOTES.txt ä¸ºç”¨æˆ·æä¾›ä¸€ä¸ªå…³äº chart éƒ¨ç½²åä½¿ç”¨è¯´æ˜çš„æ–‡ä»¶
```

helm package

```sh
# æ‰§è¡Œhelm package -d <chartæ‰€åœ¨ç›®å½•> <chartåç§°>,ä¼šç”Ÿæˆä¸€ä¸ªå‹ç¼©åŒ…ï¼Œåå­—ä¸º<chartåç§°>-<chartæ–‡ä»¶å†…çš„version>.tgz
# ä¸‹é¢çš„å‘½ä»¤åœ¨ç›®å½•appchart-uat-cdä¼šç”Ÿæˆappchart-uat-cd-1.0.0.tgz
helm package -d appchart-uat-cd appchart-uat-cd
```

ä¸Šä¼  chart åŒ…åˆ°ä»“åº“

```sh
helm push mychart-1.0.0.tgz my-repo

# æ›´æ–°å­˜å‚¨åº“: ä¸Šä¼ æ–°çš„ Helm Chart åï¼Œä½ éœ€è¦æ›´æ–° Helm å­˜å‚¨åº“çš„ç´¢å¼•æ–‡ä»¶ã€‚
# ç”±äº Harbor ä¸»è¦ç”¨äºå®¹å™¨é•œåƒç®¡ç†ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ Helm Chart ä»“åº“ï¼Œå› æ­¤åœ¨æ›´æ–° Helm Chart æ—¶éœ€è¦æ‰‹åŠ¨é‡æ–°ä¸Šä¼ å¹¶æ›´æ–°ç´¢å¼•æ–‡ä»¶ã€‚
helm repo update

```

### helm chart æ¬è¿

[containers/skopeo: Work with remote images registries - retrieving information, images, signing content](https://github.com/containers/skopeo)

[ã€äº‘åŸç”Ÿå®ç”¨æŠ€å·§ã€‘ä½¿ç”¨ skopeo æ‰¹é‡åŒæ­¥ helm chart ä¾èµ–é•œåƒ-è…¾è®¯äº‘å¼€å‘è€…ç¤¾åŒº-è…¾è®¯äº‘](https://cloud.tencent.com/developer/article/2065531)
[sir5kong/helm-charts-hub: Kubernetes Helm Charts é•œåƒç«™ï¼Œä¸­å›½åŒºç½‘ç»œåŠ é€Ÿ](https://github.com/sir5kong/helm-charts-hub)

```sh
# [ç¦»çº¿é›†ç¾¤ | JuiceFS Document Center](https://juicefs.com/docs/zh/csi/administration/offline/)
# è·å–éœ€è¦æ¬è¿çš„é•œåƒåˆ—è¡¨
helm template kube-prometheus-stack-62.7.0.tgz | grep -E ' *image:' | sed 's/ *image: //' | sort | uniq > images.txt
```

### Migration helm chart

```sh
#!/bin/bash

# è„šæœ¬éœ€è¦ä¿®æ”¹ï¼Œæ‹‰å–çš„é•œåƒä¸æ­£ç¡®
# Set the private registry URL
private_registry_url="your_private_registry_url"

# Set the chart directory
chart_dir="charts/my-chart"

# Function to modify Chart.yaml and values.yaml
modify_chart() {
  chart_name=$(grep -m 1 "name:" $chart_dir/Chart.yaml | cut -d ":" -f 2 | sed 's/ //g')
  sed -i "s/repository:.*/repository: $private_registry_url\/$chart_name/" $chart_dir/values.yaml
  sed -i "s/registry:.*/registry: $private_registry_url/" $chart_dir/Chart.yaml
}

# Function to pull and push Docker images
pull_and_push_images() {
  chart_name=$(grep -m 1 "name:" $chart_dir/Chart.yaml | cut -d ":" -f 2 | sed 's/ //g')
  helm dependency update $chart_dir
  helm dep build $chart_dir
  for image in $(helm dep list $chart_dir | awk '{print $2}'); do
    docker pull $image
    docker tag $image $private_registry_url/$chart_name/$image
    docker push $private_registry_url/$chart_name/$image
  done
}

# Loop through multiple charts (optional)
for chart in charts/*; do
  cd $chart
  modify_chart
  pull_and_push_images
  cd ..
done

# Modify a single chart (optional)
# modify_chart
# pull_and_push_images
```

## Setup Cluster

```sh
# é»˜è®¤masterèŠ‚ç‚¹æ˜¯ä¸å‚ä¸podçš„è°ƒé…çš„ï¼Œå¦‚æœéœ€è¦è¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤

#æŸ¥çœ‹æ±¡ç‚¹
[root@master1 ~]# kubectl describe node k8s-master | grep Taints
Taints: node-role.kubernetes.io/master:NoSchedule
#å»é™¤æ±¡ç‚¹ï¼Œå…è®¸ master éƒ¨ç½² pod ,è¿™é‡ŒæŠ¥é”™ä¸ç”¨ç®¡
[root@master1 ~]# kubectl taint nodes --all node-role.kubernetes.io/master-
node/master1 untainted
error: taint "node-role.kubernetes.io/master" not found
#å†æ¬¡æŸ¥çœ‹ï¼Œæ— æ˜¾ç¤ºï¼Œè¯´æ˜æ±¡ç‚¹å»é™¤æˆåŠŸ
[root@master1 ~]# kubectl describe node master1 | grep Taints
Taints:    <none>
```

### Kubernetes Gateway API

Gateway API v1.0: GA Release October 31, 2023

[Kubernetes Gateway API æ­£å¼å‘å¸ƒå¹¶å¼•å…¥ ingress2gateway é¡¹ç›®ç”¨äºç®€åŒ– Gateway API å‡çº§ | äº‘åŸç”Ÿç¤¾åŒºï¼ˆä¸­å›½ï¼‰](https://cloudnative.to/blog/gateway-api-ingress2gateway/)

![Gateway API](image/Gateway-API.png)

[Gateway API | Kubernetes](https://kubernetes.io/docs/concepts/services-networking/gateway/)
[Getting started - Kubernetes Gateway API](https://gateway-api.sigs.k8s.io/guides/)

### Setup cli

[Supercharge your Kubernetes setup with OhMyZSH ğŸš€ğŸš€ğŸš€ + awesome command line tools](https://agrimprasad.com/post/supercharge-kubernetes-setup/)
[kube-ps1](https://github.com/jonmosco/kube-ps1)
`brew install kube-ps1 stern`
kube-shell `pip install kube-shell --user -U`
`brew install kubectx`
[Krew is a tool that makes it easy to use kubectl plugins](https://krew.sigs.k8s.io/docs/user-guide/setup/install/)

ç»ˆæå·¥å…·k9s

åˆ‡æ¢é›†ç¾¤ç”¨çš„å‘½ä»¤ [kubectx + kubens: Power tools for kubectl](https://github.com/ahmetb/kubectx)
git clone https://github.com/junegunn/fzf.git ~/.fzf
~/.fzf/install

### Setup prometheus + grafana


[Alerts firing right after setting up kube-prometheus-stack](https://groups.google.com/g/prometheus-users/c/_aI-HySJ-xM/m/kqrL1FYVCQAJ?pli=1)
[KubeControllerManagerDown & kubeSchedulerDown firing on kubeadm 1.18 cluster Â· Issue #718 Â· prometheus-operator/kube-prometheus](https://github.com/prometheus-operator/kube-prometheus/issues/718)

kube-scheduler -> bind-address=127.0.0.1
kube-controller-manager -> bind-address=127.0.0.1
etcd -> listen-metrics-urls=http://127.0.0.1:<port>

```sh
# ä¿®æ”¹ç›‘å¬çš„ip
kubectl -n kube-system edit configmap kube-proxy
# metricsBindAddress: "0.0.0.0:10249"

# åœ¨ä¸‰å° master èŠ‚ç‚¹ä¿®æ”¹æ–‡ä»¶ï¼Œåé‡å¯Pod
sed -e "s/- --bind-address=127.0.0.1/- --bind-address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-controller-manager.yaml
sed -e "s/- --bind-address=127.0.0.1/- --bind-address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-scheduler.yaml
sed -e "s#- --listen-metrics-urls=http://127.0.0.1:2381#- --listen-metrics-urls=http://0.0.0.0:2381#" -i /etc/kubernetes/manifests/etcd.yaml

```

## Best Practice

### å£°æ˜æ¯ä¸ªPodçš„resource

[é«˜å¯é æ¨èé…ç½®](https://help.aliyun.com/document_detail/128157.html)
Kubernetesé‡‡ç”¨é™æ€èµ„æºè°ƒåº¦æ–¹å¼ï¼Œå¯¹äºæ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„å‰©ä½™èµ„æºï¼Œå®ƒæ˜¯è¿™æ ·è®¡ç®—çš„ï¼šèŠ‚ç‚¹å‰©ä½™èµ„æº=èŠ‚ç‚¹æ€»èµ„æº-å·²ç»åˆ†é…å‡ºå»çš„èµ„æºï¼Œå¹¶ä¸æ˜¯å®é™…ä½¿ç”¨çš„èµ„æºã€‚å¦‚æœæ‚¨è‡ªå·±æ‰‹åŠ¨è¿è¡Œä¸€ä¸ªå¾ˆè€—èµ„æºçš„ç¨‹åºï¼ŒKuberneteså¹¶ä¸èƒ½æ„ŸçŸ¥åˆ°ã€‚

å¦å¤–æ‰€æœ‰Podä¸Šéƒ½è¦å£°æ˜resourcesã€‚å¯¹äºæ²¡æœ‰å£°æ˜resourcesçš„Podï¼Œå®ƒè¢«è°ƒåº¦åˆ°æŸä¸ªèŠ‚ç‚¹åï¼ŒKubernetesä¹Ÿä¸ä¼šåœ¨å¯¹åº”èŠ‚ç‚¹ä¸Šæ‰£æ‰è¿™ä¸ªPodä½¿ç”¨çš„èµ„æºã€‚å¯èƒ½ä¼šå¯¼è‡´èŠ‚ç‚¹ä¸Šè°ƒåº¦è¿‡å»å¤ªå¤šçš„Podã€‚

## Troubleshooting

### Pod exit code

[Docker Container Exit Codes Explained | hoangtrinhj.com](https://hoangtrinhj.com/docker-container-exit-codes)

[é€šè¿‡ Exit Code å®šä½ Pod å¼‚å¸¸é€€å‡ºåŸå›  | è…¾è®¯äº‘](https://intl.cloud.tencent.com/zh/document/product/457/35758)

[SIGTERM : Linux Graceful Termination | Exit code 143, Signal 15](https://komodor.com/learn/sigterm-signal-15-exit-code-143-linux-graceful-termination/)

Common exit codes (`128+x`) associated with docker containers are:

* Exit Code 0: Absence of an attached foreground process
* Exit Code 1: Indicates failure due to application error
* Exit Code 137: `128+9` Indicates failure as container received SIGKILL (Manual intervention or â€˜oom-killerâ€™ [OUT-OF-MEMORY])
* Exit Code 139: `128+11` Indicates failure as container received SIGSEGV
* Exit Code 143: `128+15` Indicates failure as container received SIGTERM

x: see `man signal`

### [Troubleshooting a failed certificate request | cert-manager](https://cert-manager.io/docs/faq/troubleshooting/)

There are several resources that are involved in requesting a certificate.
Automated Certificate Management Environment (ACME).

```text
  (  +---------+  )
  (  | Ingress |  ) Optional                                              ACME Only!
  (  +---------+  )
         |                                                     |
         |   +-------------+      +--------------------+       |  +-------+       +-----------+
         |-> | Certificate |----> | CertificateRequest | ----> |  | Order | ----> | Challenge |
             +-------------+      +--------------------+       |  +-------+       +-----------+
                                                               |
```

1. Checking the Certificate resource `kubectl get certificate` or `kubectl describe certificate <certificate-name>`
2. Checking the CertificateRequest `kubectl describe certificaterequest <CertificateRequest name>`
3. Check the issuer state
   1. `kubectl describe issuer <Issuer name>`
   2. `kubectl describe clusterissuer <ClusterIssuer name>`
4. [Troubleshooting Issuing ACME Certificates | cert-manager](https://cert-manager.io/docs/faq/acme/): ACME(e.g. Letâ€™s Encrypt)
   1. Check Orders `kubectl describe order example-com-2745722290-439160286`. If the Order is not completing successfully, you can debug the challenges for the Order
   2. Check Challenges `kubectl describe challenge example-com-2745722290-4391602865-0`
      1. [HTTP01 troubleshooting](https://cert-manager.io/docs/faq/acme/#http01-troubleshooting)
      2. [DNS01 troubleshooting](https://cert-manager.io/docs/faq/acme/#dns01-troubleshooting)
         1. [alidns-webhook/bundle.yaml](https://github.com/pragkent/alidns-webhook/blob/master/deploy/bundle.yaml)
      3. Check the pod status and log `kubectl log alidns-webhook-78df4cfddd-cnjsd`

## Example

### æš´éœ² TCP æœåŠ¡

#### service type = CluserIP é€šè¿‡Ingress

é˜¿é‡Œäº‘ kubernetes é…ç½®

1. é…ç½® configmap: å®¹å™¨æœåŠ¡ -> åº”ç”¨é…ç½®-> é…ç½®é¡¹ -> tcp-services -> æ·»åŠ  åç§°: config-name, å€¼: namespace/serviceName:service port, æ³¨æ„åç§°config-name éœ€è¦é…ç½®åˆ° ingress å®¹å™¨ç«¯å£, ä¾‹å¦‚ `5672:rabbitmq-system/rabbitmqcluster:5672` æˆ–è€…ç”¨å‘½ä»¤ `kubectl -n kube-system patch configmap tcp-services --type merge -p '{"data":{"5672": "rabbitmq-system/rabbitmqcluster:5672"}}'`
2. é…ç½® service: å®¹å™¨æœåŠ¡ -> ç½‘ç»œ -> æœåŠ¡ -> nginx-ingress-lb -> æ›´æ–° å¢åŠ ç«¯å£
   port æ˜¯æš´éœ²çš„å…¬ç½‘ç«¯å£(æ§åˆ¶å°å«åš æœåŠ¡ç«¯å£), targetPort æ˜¯ configmap åç§° (æ§åˆ¶å°å«åš å®¹å™¨ç«¯å£), è¿™é‡Œå®¹å™¨ç«¯å£åªèƒ½æ˜¯æ•°å­—, æ‰€ä»¥åè¿‡æ¥é™åˆ¶ç¬¬ä¸€æ­¥çš„ config-name åªèƒ½ç”¨æ•°å­—

ç„¶å `ingress` ä¼šåŠ¨æ€è¯»å–`tcp-services` æš´éœ²çš„ç«¯å£ `tcp-services-configmap=$(POD_NAMESPACE)/tcp-services`

reference:
[ç©è½¬Kubernetes TCP Ingress](https://developer.aliyun.com/article/603225)
[Exposing TCP and UDP services](https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services)

#### service type = NodePort

1. åœ¨è´Ÿè½½å‡è¡¡è®¾ç½®ç«¯å£æ˜ å°„, ç›‘å¬è™šæ‹ŸæœåŠ¡å™¨ç»„çš„ç«¯å£è®¾ç½®ä¸º NodePort 30022
2. å®‰å…¨ç»„æ”¾å¼€ nodePort 30022

## ä½¿ç”¨ kubecost åˆ†æ Kubernetes æˆæœ¬

kubecost æ˜¯ç›®å‰è¾ƒä¼˜ç§€çš„å¼€æº Kubernetes æˆæœ¬åˆ†æå·¥å…·ã€‚kubecost ç›®å‰æ”¯æŒ é˜¿é‡Œäº‘ã€AWS ç­‰äº‘å‚å•†å¯¹æ¥ï¼Œå®ƒèƒ½å¤Ÿæä¾›é›†ç¾¤ä¸­å‘½åç©ºé—´ã€åº”ç”¨ç­‰å„ç±»èµ„æºæˆæœ¬åˆ†é…ï¼Œç”¨æˆ·è¿˜å¯ä»¥åŸºäºè¿™äº›ä¿¡æ¯åœ¨ Kubecost ä¸­è®¾ç½®é¢„ç®—å’Œè­¦æŠ¥ï¼Œå¸®åŠ©è¿ç»´å’Œè´¢åŠ¡ç®¡ç†äººå‘˜è¿›ä¸€æ­¥å®ç°æˆæœ¬ç®¡ç†ã€‚

## containerd

### Docker vs. Containerd

[ä¸€æ–‡å¸¦ä½ äº†è§£Dockerä¸Containerdçš„åŒºåˆ«-è…¾è®¯äº‘å¼€å‘è€…ç¤¾åŒº-è…¾è®¯äº‘](https://cloud.tencent.com/developer/article/2327654)

[Docker, containerd, CRI-O and runcä¹‹é—´çš„åŒºåˆ«ï¼Ÿ - Zhai_David - åšå®¢å›­](https://www.cnblogs.com/chuanzhang053/p/16784668.html)

[å®¹å™¨æœåŠ¡ å¦‚ä½•é€‰æ‹© Containerd å’Œ Docker-å¸¸è§é—®é¢˜-æ–‡æ¡£ä¸­å¿ƒ-è…¾è®¯äº‘](https://cloud.tencent.com/document/product/457/35747)
[å¦‚ä½•é€‰æ‹©Dockerã€ContainerdåŠå®‰å…¨æ²™ç®±è¿è¡Œæ—¶_å®¹å™¨æœåŠ¡ Kubernetes ç‰ˆ ACK(ACK)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/comparison-of-docker-containerd-and-sandboxed-container)


Containerdï¼šè°ƒç”¨é“¾æ›´çŸ­ï¼Œç»„ä»¶æ›´å°‘ï¼Œæ›´ç¨³å®šï¼Œå ç”¨èŠ‚ç‚¹èµ„æºæ›´å°‘ã€‚å»ºè®®é€‰æ‹© Containerdã€‚

ä½œä¸º K8S å®¹å™¨è¿è¡Œæ—¶ï¼Œéƒ¨ç½²ç»“æ„å¯¹æ¯”

Docker: kubelet --> docker shim ï¼ˆåœ¨ kubelet è¿›ç¨‹ä¸­ï¼‰ --> dockerd --> containerd
Containerd: kubelet --> cri pluginï¼ˆåœ¨ containerd è¿›ç¨‹ä¸­ï¼‰ --> containerd

Containerd å’Œ Docker ç»„ä»¶å¸¸ç”¨å‘½ä»¤æ˜¯ä»€ä¹ˆï¼Ÿ

Containerd ä¸æ”¯æŒ docker API å’Œ docker CLIï¼Œä½†æ˜¯å¯ä»¥é€šè¿‡ cri-tool å‘½ä»¤å®ç°ç±»ä¼¼çš„åŠŸèƒ½ã€‚

ctr æ˜¯ containerd çš„ä¸€ä¸ªå®¢æˆ·ç«¯å·¥å…·ã€‚ crictl æ˜¯ CRI å…¼å®¹çš„å®¹å™¨è¿è¡Œæ—¶å‘½ä»¤è¡Œæ¥å£ï¼Œå¯ä»¥ä½¿ç”¨å®ƒæ¥æ£€æŸ¥å’Œè°ƒè¯• k8s èŠ‚ç‚¹ä¸Šçš„å®¹å™¨è¿è¡Œæ—¶å’Œåº”ç”¨ç¨‹åºã€‚ ctr -v è¾“å‡ºçš„æ˜¯ containerd çš„ç‰ˆæœ¬ï¼Œcrictl -v è¾“å‡ºçš„æ˜¯å½“å‰ k8s çš„ç‰ˆæœ¬ï¼Œä»ç»“æœæ˜¾è€Œæ˜“è§ä½ å¯ä»¥è®¤ä¸º crictl æ˜¯ç”¨äº k8s çš„ã€‚

|           | docker                  | ctrï¼ˆcontainerdï¼‰              | crictlï¼ˆkubernetesï¼‰       |
|-----------|-------------------------|------------------------------|--------------------------|
| åˆ›å»ºæ–°å®¹å™¨     | docker create           | ctr container create         | crictl create            |
| å¯åŠ¨/å…³é—­å®¹å™¨   | docker start/stop       | ctr task start/kill          | crictl start/stop        |
| è¿è¡Œæ–°å®¹å™¨     | docker run              | ctr run                      | æ— ï¼ˆæœ€å°å•å…ƒä¸º podï¼‰             |
| æŸ¥çœ‹è¿è¡Œçš„å®¹å™¨   | docker ps               | ctr task ls/ctr container ls | crictl ps                |
| åˆ é™¤å®¹å™¨      | docker rm               | ctr container rm             | crictl rm                |
| æŸ¥çœ‹å®¹å™¨æ—¥å¿—    | docker logs             | æ—                             | crictl logs              |
| æŸ¥çœ‹å®¹å™¨æ•°æ®    | docker inspect          | ctr container info           | crictl inspect           |
| æŸ¥çœ‹å®¹å™¨èµ„æº    | docker stats            | æ—                             | crictl stats             |
| å®¹å™¨å†…éƒ¨æ‰§è¡Œå‘½ä»¤  | docker exec             | æ—                             | crictl exec              |
| attach    | docker attach           | æ—                             | crictl attach            |
| ä¿®æ”¹é•œåƒæ ‡ç­¾    | docker tag              | ctr image tag                | æ—                         |
| å¯¼å…¥é•œåƒ      | docker load             | ctr image import             | æ—                         |
| å¯¼å‡ºé•œåƒ      | docker save             | ctr image export             | æ—                         |
| æŸ¥çœ‹é•œåƒ      | docker images           | ctr image ls                 | crictl images            |
| åˆ é™¤é•œåƒ      | docker rmi              | ctr image rm                 | crictl rmi               |
| æ‹‰å–é•œåƒ      | docker pull             | ctr image pull               | crictl pull              |
| æ¨é€é•œåƒ      | docker push             | ctr image push               | æ—                         |
| æŸ¥çœ‹é•œåƒè¯¦æƒ…    | docker inspect IMAGE-ID | ?                            | crictl inspect IMAGE-ID  |
| æ˜¾ç¤º POD åˆ—è¡¨ | æ—                        | æ—                             | crictl pods              |

### è®¾ç½® containerd æ‹‰å– http ç§æœ‰ä»“åº“

[How to pull docker image from a insecure private registry with latest Kubernetes - Stack Overflow](https://stackoverflow.com/questions/72419513/how-to-pull-docker-image-from-a-insecure-private-registry-with-latest-kubernetes)

```sh
# vi /etc/containerd/config.toml
      [plugins."io.containerd.grpc.v1.cri".registry.configs]

        [plugins."io.containerd.grpc.v1.cri".registry.configs."172.28.48.107:8081"] # edited line

          [plugins."io.containerd.grpc.v1.cri".registry.configs."172.28.48.107:8081".auth] # edited line
            username = "USERNAME"
            password = "PASSWORD"

          [plugins."io.containerd.grpc.v1.cri".registry.configs."172.28.48.107:8081".tls] # edited line
            ca_file = "" # edited line
            cert_file = "" # edited line
            insecure_skip_verify = true # edited line
            key_file = "" # edited line

      [plugins."io.containerd.grpc.v1.cri".registry.headers]

      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]

        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."172.28.48.107:8081"] # edited line
          endpoint = ["http://172.28.48.107:8081"] # edited line
```

### å®¢æˆ·ç«¯å·¥å…· nerdctl

https://github.com/containerd/nerdctl/releases

ç²¾ç®€ (nerdctl--linux-amd64.tar.gz): åªåŒ…å« nerdctl
å®Œæ•´ (nerdctl-full--linux-amd64.tar.gz): åŒ…å« containerd, runc, and CNI ç­‰ä¾èµ–

nerdctl çš„ç›®æ ‡å¹¶ä¸æ˜¯å•çº¯åœ°å¤åˆ¶ docker çš„åŠŸèƒ½ï¼Œå®ƒè¿˜å®ç°äº†å¾ˆå¤š docker ä¸å…·å¤‡çš„åŠŸèƒ½ï¼Œä¾‹å¦‚å»¶è¿Ÿæ‹‰å–é•œåƒï¼ˆlazy-pullingï¼‰ã€é•œåƒåŠ å¯†ï¼ˆimgcryptï¼‰ç­‰ã€‚å…·ä½“çœ‹ nerdctlã€‚

é€šè¿‡ nerdctl ç™»å½• harbor

```sh
echo Harbor12345 | nerdctl login --username "admin" --password-stdin  myharbor-minio.com:443
nerdctl login --username "admin" --password Harbor12345 myharbor-minio.com:443
# ç™»å‡º
nerdctl logout
```

## ip-netns management tool

[ip-netns(8) - Linux manual page](https://man7.org/linux/man-pages/man8/ip-netns.8.html)
[Tracing the path of network traffic in Kubernetes](https://learnk8s.io/kubernetes-network-packets)

The network namespaces can be managed by the ip-netns management tool,

```sh
# to list the namespaces on a host.
ip netns list

# run the exec command inside the namespace cni-0f226515
ip netns exec cni-ebbbed0d-7b1c-36fb-b412-ce337ff74778 ip a

# run the netstat command inside that namespace
# verify that the container listens for HTTP traffic from within the namespace
ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac netstat -lnp

# find the latest named network namespace
ls -lt /var/run/netns
```

`lsns` is a command for listing all available namespaces on a host.
  `-t, --type type` The supported types are mnt, net, ipc, user, pid and uts.
  `-p, --task pid` Display only the namespaces held by the process with this pid. `lsns -p 5777`

exampleï¼š

```sh
lsns -t net -t mnt

```
