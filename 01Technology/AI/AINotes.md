# AI News & Information

AIå¹²ä¸äº†ä½ å¹²ä¸äº†çš„äº‹ï¼Œå› ä¸ºä½ å¹²ä¸äº†ä½ å°±çœ‹ä¸å‡ºæ¥å®ƒæ˜¯èƒ¡å¹²è¿˜æ˜¯çœŸå¹²ã€‚AIçš„æœ‰æ•ˆèƒ½åŠ›èŒƒå›´ç­‰äºè·Ÿå®ƒé…å¯¹çš„äººçš„èƒ½åŠ›èŒƒå›´ã€‚

- [Humata - ChatGPT for all your files](https://www.humata.ai/)
- [Code Llama: Inference code for CodeLlama models](https://github.com/facebookresearch/codellama)

## GPT åˆ†ç±»

[Leaderboard | LMArena](https://lmarena.ai/leaderboard/)

[é«˜ç››å‘å¸ƒä¸­å›½AIæŠ¥å‘Šï¼šå…¨é¢å‰–æBATä¸‰å·¨å¤´ä»¥åŠ26ä¸ªæ ¸å¿ƒç©å®¶](https://36kr.com/p/5091725.html)

- ç”¨ [Perplexity](http://Perplexity.ai) ä»£æ›¿ Googleï¼›
- ç”¨ [Poe](https://poe.com/sage) ä»£æ›¿ ChatGPT.
- ç”¨ Promptable ä»£æ›¿ OpenAI Playground.
- ç”¨ [Elicit](https://elicit.org) ä»£æ›¿ Google Scholar

- [æå®¢æ—¶é—´ AI æŒ‡å—](https://zhinan.geekbang.org/)
- å†™æ–‡æ¡ˆç”¨ ChatGPTã€Notion AIæˆ– Jarvisï¼ŒClaudeï¼Œ
  - [Poe](https://poe.com/sage)
  - Slack Claude
  - [æ–‡å¿ƒä¸€è¨€ ç™¾åº¦](https://yiyan.baidu.com/)
  - [é€šä¹‰åƒé—® é˜¿é‡Œ](https://qianwen.com)
  - [Google AI Studio](https://aistudio.google.com/app/prompts/new_chat)
  - [Kimi.ai - å¸®ä½ çœ‹æ›´å¤§çš„ä¸–ç•Œ](https://kimi.moonshot.cn/)
  - [DeepSeek](https://platform.deepseek.com)
  - [lfnovo/open-notebook: An Open Source implementation of Notebook LM with more flexibility and features](https://github.com/lfnovo/open-notebook)
- å›¾åƒå¤„ç†
  - [Nano-Banana Pro Google æœ€å¼ºæ•™ç¨‹ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1978348406347961544) [Google AI Studio on X: "The Complete Guide to Nano Banana Pro: 10 Tips for Professional Asset Production" / X](https://x.com/GoogleAIStudio/status/1994480371061469306?t=c0LkdHNU0nYucaO2788Odg&s=09)
  - [æ–‡å¿ƒä¸€æ ¼ ç™¾åº¦](https://yige.baidu.com/)
  - [é€šä¹‰ä¸‡ç›¸ é˜¿é‡Œ](https://wanxiang.aliyun.com/creation)
  - Midjourney Discord æˆ– Getimg.aiï¼Œstable diffusionï¼Œdall-e
  - [OmniParser å›¾ç‰‡è§£æ](https://microsoft.github.io/OmniParser/)ï¼šOmniParser is a comprehensive method for parsing user interface screenshots into structured and easy-to-understand elements
  - [ç”Ÿæˆæ’ç”»ã€æ¼«ç”» Infinite AI Artboard - Recraft](https://www.recraft.ai/)
  - [FLUX.1 Kontext | Black Forest Labs](https://bfl.ai/models/flux-kontext)
  - [black-forest-labs/FLUX.1-Kontext-dev Â· Hugging Face](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev)
  - [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)
  - [DeepSeek-OCR: Contexts Optical Compression](https://github.com/deepseek-ai/DeepSeek-OCR)
- éŸ³è§†é¢‘è½¬æ–‡æœ¬
  - [é€šä¹‰å¬æ‚Ÿ - é˜¿é‡Œ](https://tingwu.aliyun.com)
  - [Aiko â€” Sindre Sorhus è¯­éŸ³è½¬æ–‡å­—](https://sindresorhus.com/aiko)
  - [MacWhisper](https://goodsnooze.gumroad.com/l/macwhisper) [MacWhisper](https://app.gumroad.com/d/29e33b796f6ce9bb186f87cdf2fadb16)
  - Buzz transcribes and translates audio offline on your personal computer. [Buzz](https://github.com/chidiwilliams/buzz)
  - [AudioPen](https://audiopen.ai/demo)
  - [alphacep/vosk-api: Offline speech recognition API for Android, iOS, Raspberry Pi and servers with Python, Java, C# and Node](https://github.com/alphacep/vosk-api)
- æ–‡æœ¬è½¬éŸ³è§†é¢‘
  - éŸ³é¢‘ç”Ÿæˆç”¨ voice.ai æˆ– Eleven Labsï¼Œè¿å­—å¹•éƒ½å¯ä»¥ç”¨è‡ªå·±é€šè¿‡AIâ€œè®­ç»ƒâ€çš„ä¸ªæ€§ä¸“å±å­—ä½“ã€‚
  - [AI Video Editor | Create viral videos with AI - Topview.ai](https://www.topview.ai/)
  -
- è§†é¢‘å¤„ç†
  - æŠŠç”»å‡ºæ¥çš„äººç‰©ï¼Œå–‚æ–‡æ¡ˆè®©ä»–å˜æˆé¢‘è§†çš„æ˜¯D-IDï¼›
  - è§†é¢‘åˆ¶ä½œç”¨ Descript æˆ– Runway
  - ä¸€ä¸ªé¢‘è§†ä¸­çš„äººç‰©æ›¿æ¢æˆè™šæ‹Ÿäººæˆ–CGçš„æ˜¯Wonderï¼›
  - ç»™é¢‘è§†é…éŸ³åˆ›å»ºéŸ³ä¹å°±ç”¨Soundrawï¼›
  - [Open-Sora è§†é¢‘ç”Ÿæˆ](https://github.com/hpcaitech/Open-Sora)
  - [Open Sora demo - a Hugging Face Space by hpcai-tech](https://huggingface.co/spaces/hpcai-tech/open-sora)
  - [å¯çµ AI - æ–°ä¸€ä»£ AI åˆ›æ„ç”Ÿäº§åŠ›å¹³å°](https://klingai.kuaishou.com/)
- æ™ºèƒ½åŒ–ç ”å‘
  - [Codeium Â· Free AI Code Completion & Chat](https://codeium.com/faq)
  - [é€šä¹‰çµç _æ™ºèƒ½ç¼–ç åŠ©æ‰‹ é˜¿é‡Œäº‘](https://tongyi.aliyun.com/lingma/download)
  - ç™¾åº¦ Comate 2.0ä»£ç åŠ©æ‰‹ [Baidu Comate Â· Coding mate](https://comate.baidu.com/)
  - [OpenDevin - OpenHands](https://github.com/All-Hands-AI/OpenHands)
  - [yetone/avante.nvim: Use your Neovim like using Cursor AI IDE!](https://github.com/yetone/avante.nvim)
  - [é˜¿é‡Œæ™ºèƒ½åŒ–ç ”å‘ä¸€å¹´å¤ç›˜](https://mp.weixin.qq.com/s/JTpLy8Z0klokHVcaDZm2RQ)
  - [CodeRabbit is an AI-powered code reviewer](https://docs.coderabbit.ai/)
  - [rikvermeulen/co-op-gitlab: Automate code reviews and feedback for GitLab Merge Requests using OpenAI GPT-3/4](https://github.com/rikvermeulen/co-op-gitlab)
  - [Diffblue Cover write Java unit tests](https://docs.diffblue.com/get-started/get-started/get-started-cover-plugin)
- æŠŠçœŸäººå˜æˆè™šæ‹Ÿäººçš„æ˜¯ Meta humanï¼Œæƒ³æ›´ç®€å•å˜è™šæ‹Ÿäººçš„æ˜¯Ready player meï¼›
- PPT:
  - [NotebookLM](https://notebooklm.google.com)
  - [AIPPT-å›¾ç‰‡è½¬ PPT](https://aippt.wps.cn/aippt/convert-ppt/home)
  - chatgptç”Ÿæˆçš„markdownè¯­æ³•å†…å®¹å¯¼å…¥[MindShow](https://www.mindshow.fun/#/home)
  - [AI PPT Maker - Best Online Free (No Sign up)](https://aipptmaker.ai/en)
  - [å¿«é€ŸåšPPT Gamma - Presentations and Slide Decks with AI | Gamma](https://gamma.app/)
  - [Anionex/banana-slides: ä¸€ä¸ªåŸºäºnano banana proğŸŒçš„åŸç”ŸAI PPTç”Ÿæˆåº”ç”¨ï¼Œè¿ˆå‘çœŸæ­£çš„ï¼‚Vibe PPTï¼‚; æ”¯æŒä¸Šä¼ ä»»æ„æ¨¡æ¿å›¾ç‰‡ï¼›ä¸Šä¼ ä»»æ„ç´ æ&æ™ºèƒ½è§£æï¼›ä¸€å¥è¯/å¤§çº²/é¡µé¢æè¿°è‡ªåŠ¨ç”ŸæˆPPTï¼›å£å¤´ä¿®æ”¹æŒ‡å®šåŒºåŸŸã€ä¸€é”®å¯¼å‡º - An AI-native PPT generator based on nano banana proğŸŒ](https://github.com/Anionex/banana-slides)
- æ‰‹æœºè‡ªåŠ¨åŒ–
  - [æ™ºè°±å¼€æºæ¨¡å‹ Open-AutoGLM](https://github.com/zai-org/Open-AutoGLM) Phone Agent æ˜¯ä¸€ä¸ªåŸºäº AutoGLM æ„å»ºçš„æ‰‹æœºç«¯æ™ºèƒ½åŠ©ç†æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä»¥å¤šæ¨¡æ€æ–¹å¼ç†è§£æ‰‹æœºå±å¹•å†…å®¹ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æ“ä½œå¸®åŠ©ç”¨æˆ·å®Œæˆä»»åŠ¡ã€‚
- ç¬¬ä¸‰æ–¹ç¤¾ç¾¤ï¼Œcivitai
- æ•°å­¦ [MathGPT å­¦è€Œæ€](https://www.mathgpt.com/)
- åŒ»ç–—é—®é¢˜å›ç­”[MediSearch](https://medisearch.io/zh)
- æŸ¥æ‰¾æ³•å¾‹æ¡ˆä¾‹ã€æ¡æ–‡ [MetaLaw ç±»æ¡ˆæ£€ç´¢ï¼Œä¸€é”®ç›´è¾¾ï¼Œè®©ä½ çš„æ³•å¾‹ç ”ç©¶æ•ˆç‡å¿«äºº10å€](https://meta.law/)
- åŠå…¬å°æµ£ç†Š:åˆ¶ä½œexcelè¡¨æ ¼ã€‚
- æµè§ˆå™¨è‡ªåŠ¨åŒ–
  - Scribe AI chrome æ’ä»¶ï¼šè‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–æ“ä½œæŒ‡å—, åˆ›å»ºé€æ­¥æŒ‡å—çš„åœºæ™¯ï¼ŒåŒ…æ‹¬æ“ä½œè¯´æ˜ã€æ ‡å‡†æ“ä½œè§„ç¨‹ã€åŸ¹è®­æ‰‹å†Œç­‰ã€‚
  - [Skyvern-AI/skyvern: Automate browser based workflows with AI](https://github.com/Skyvern-AI/skyvern)
- ç¼–æ’
  - [Dify github](https://github.com/langgenius/dify)
  - [TEN Agent is a conversational AI](https://github.com/TEN-framework/TEN-Agent)
  - [RAGä¸ƒåäºŒå¼ï¼š2024å¹´åº¦RAGæ¸…å•](https://mp.weixin.qq.com/s/_pnezCv-sKmzhho7Xw3D2g)
- UI è®¾è®¡å·¥å…·
  - [Stitch - Design with AI](https://stitch.withgoogle.com/?pli=1)

[claude-code-proxy: Run Claude Code on OpenAI models](https://github.com/1rgs/claude-code-proxy)
[AI å·¥å…·åˆ—è¡¨ é£ä¹¦æ–‡æ¡£](https://zl49so8lbq.feishu.cn/wiki/wikcn6YTN3CrZTS8RhrEca8c8Eg)
[æå®¢æ—¶é—´ AIGC çŸ¥è¯†åº“](https://gp477l8icq.feishu.cn/wiki/JUXnwzSuviL5E9kh6jUc8FRinHe)
[ModelScopeé­”æ­ç¤¾åŒº æ—¨åœ¨æ‰“é€ ä¸‹ä¸€ä»£å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ï¼Œè®©æ¨¡å‹åº”ç”¨æ›´ç®€å•ã€‚](https://community.modelscope.cn/)
[ima.copilot-è…¾è®¯æ™ºèƒ½å·¥ä½œå°](https://ima.qq.com/) ima.copilotï¼ˆç®€ç§°imaï¼‰æ˜¯ä¸€æ¬¾ç”±è…¾è®¯æ··å…ƒå¤§æ¨¡å‹æä¾›æŠ€æœ¯æ”¯æŒçš„ï¼Œé¢å‘å­¦ä¹ ã€åŠå…¬åœºæ™¯ï¼Œä»¥çŸ¥è¯†åº“ä¸ºæ ¸å¿ƒçš„AIæ™ºèƒ½å·¥ä½œå°ï¼Œæ˜¯è¯»ã€æœã€å†™ä¸€ä½“çš„æ•ˆç‡å·¥å…·

[xtekky/gpt4free: decentralising the Ai Industry, just some language model api's...](https://github.com/xtekky/gpt4free)

| Website s | Model(s) |
| --- | --- |
| [forefront.ai](https://chat.forefront.ai) | GPT-4/3.5 |
| [poe.com](https://poe.com) | GPT-4/3.5 |
| [writesonic.com](https://writesonic.com) | GPT-3.5 / Internet |
| [t3nsor.com](https://t3nsor.com) | GPT-3.5 |
| [you.com](https://you.com) | GPT-3.5 / Internet / good search |
| [sqlchat.ai](https://sqlchat.ai) | GPT-3.5 |
| [bard.google.com](https://bard.google.com) | custom / search |
| [bing.com/chat](https://bing.com/chat) | GPT-4/3.5 |
| [italygpt.it](https://italygpt.it) | GPT-3.5 |

[åˆ†äº«ä¸€ä¸‹æˆ‘ç°åœ¨æ¯å¤©æœ€å¸¸ç”¨çš„AIäº§å“](https://mp.weixin.qq.com/s/ES-9RTkiwIk99hphoUbfyg)

1. AIçŸ¥è¯†é—®ç­”ï¼š OpenAI-o3ï¼Œè±†åŒ…ï¼ˆå›½å†…ï¼‰
2. å†™ä½œå’Œå†™å†…å®¹ï¼š GPT-4.5ï¼ˆå…¶ä»–éƒ½æ˜¯åƒåœ¾ï¼‰
3. é•¿æ–‡æœ¬å¤„ç†ï¼š Gemini 2.5 Proï¼ˆå…¶ä»–éƒ½æ˜¯åƒåœ¾ï¼‰
4. æ·±åº¦ç ”ç©¶ï¼š ChatGPT DeepResearchï¼Œç§˜å¡”AIï¼ˆå›½å†…ï¼‰
5. å›¾ç‰‡ç”Ÿæˆï¼š Midjourneyï¼Œå³æ¢¦ï¼ˆå›½å†…ï¼‰ï¼ŒFluxï¼ˆå¼€æºï¼‰
6. ä¸­æ–‡æµ·æŠ¥ç”Ÿæˆï¼š å³æ¢¦3.0ï¼ˆå›½å†…ï¼Œæµ·å¤–éƒ½æ˜¯åƒåœ¾ï¼‰
7. å›¾ç‰‡ä¿®æ”¹ï¼š Flux Contextã€è±†åŒ…ï¼ˆå›½å†…ï¼‰
8. AIé€šç”¨Agentï¼š MiniMax Agentï¼ˆå›½å†…ï¼‰
9. AIè®¾è®¡Agentï¼š Lovartï¼Œæ˜Ÿæµï¼ˆå›½å†…ï¼‰
10. AIç¼–ç¨‹Agentï¼š Claude code
11. AIç¼–ç¨‹IDEï¼š Cursorï¼ŒTraeï¼ˆå›½å†…ï¼‰
12. AIéŸ³ä¹ç”Ÿæˆï¼š Suno 4.5+ã€Mureka v7ï¼ˆå›½å†…ï¼‰
13. AIè§†é¢‘ç”Ÿæˆï¼ˆå¸¦å°è¯ï¼‰ï¼š Veo3ï¼Œå³æ¢¦å¯¹å£å‹å¤§å¸ˆç‰ˆï¼ˆå›½å†…ï¼‰
14. AIè§†é¢‘ç”Ÿæˆï¼ˆæ— å°è¯ï¼‰ï¼š Veo3ï¼Œå…¶ä»–çš„éƒ½æ˜¯å›½å†…çš„ã€‚å³æ¢¦ï¼ˆæœ€å¸¸ç”¨ï¼Œåšå„ç§é£æ ¼åŒ–ï¼‰ï¼Œå¯çµï¼ˆåšé«˜è´¨é‡è¿åŠ¨ï¼‰ï¼Œhailuoï¼ˆåŠ¨ä½œè¡¨æ¼”ã€é«˜é€Ÿæ‰“æ–—ï¼‰
15. AIè¯­éŸ³ç”Ÿæˆï¼š 11Labsï¼ŒMinimaxï¼ˆå›½å†…ï¼‰
16. AI 3Dç”Ÿæˆï¼š Tripo
17. AIå·¥ä½œæµï¼š é£ä¹¦å¤šç»´è¡¨æ ¼
18. AIç¿»è¯‘æ’ä»¶ï¼š æ²‰æµ¸å¼ç¿»è¯‘
19. AIç¬”è®°å’ŒçŸ¥è¯†åº“ï¼š Getç¬”è®°

### prompt æç¤ºè¯

æç¤ºè¯åˆ†ä¸ºç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯ï¼Œç”¨æˆ·æç¤ºè¯å°±æ˜¯æˆ‘ä»¬çš„é—®é¢˜ã€‚ç³»ç»Ÿæç¤ºè¯ï¼Œæ˜¯agentçš„èƒŒæ™¯/è§’è‰²ï¼Œè®¾ç½®äº†agentéœ€è¦å®Œæˆä»€ä¹ˆç±»å‹çš„ä»»åŠ¡ã€‚ç³»ç»Ÿæç¤ºè¯ä¸»è¦åŒ…æ‹¬ï¼šèº«ä»½ï¼ˆRoleï¼‰+ ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰+ Â ä¾‹å­ï¼ˆExamplesï¼‰ + è¾“å‡ºè§„èŒƒï¼ˆOutput Formatï¼‰ã€‚

ç°åœ¨å·²ç»æœ‰äº†å¾ˆå¤šå¸®åŠ©æˆ‘ä»¬ç”Ÿäº§æç¤ºè¯çš„å·¥å…·ï¼Œå¦‚ï¼š
https://prompt.always200.com/
https://prompts.chat/
æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å·¥å…·ç®€å•ç”Ÿæˆåˆç‰ˆï¼Œå†è¿›è¡Œåç»­ä¼˜åŒ–ã€‚

#### [å¸¸ç”¨çš„Promptå¿ƒæ³•](https://mp.weixin.qq.com/s/a_-EMSjN0ldsrKCIk6nxuQ)

1. è®©AIé€‰å®šè§’è‰²åå†å›ç­”

åœ¨ä½ å¯¹ä½ è¦æé—®çš„é—®é¢˜éå¸¸æ¸…æ¥šçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ç›´æ¥è®¾å®šè§’è‰²ï¼Œè€Œä¸”è¿™ä¸ªè§’è‰²è¶Šå…·ä½“è¶ŠçœŸå®ï¼Œå…¶å®æ•ˆæœå°±è¶Šå¥½ï¼Œæ¯”å¦‚ä½ è®¾å®šä¹”å¸ƒæ–¯çš„è§’è‰²ï¼Œå¾ˆå¤šæ—¶å€™ï¼Œå°±æ¯”ä¸€ä¸ª10å¹´çš„äº§å“ç»ç†è¿™ç§è§’è‰²ï¼Œæ¥çš„æ›´åŠ æœ‰å¥‡æ•ˆã€‚

```sh
æˆ‘æƒ³æ¢è®¨ã€é¢†åŸŸã€‘é‡Œçš„ã€é—®é¢˜ç±»å‹/åœºæ™¯ã€‘ã€‚å…ˆåˆ«å›ç­”ã€‚
è¯·ä½ å…ˆé€‰ä¸€ä½æœ€é€‚åˆçš„é¢†åŸŸé¡¶å°–åäººä¸“å®¶æ¥æ€è€ƒå®ƒã€‚å¯ä»¥æ˜¯æ´»äººæˆ–å†å²äººç‰©ï¼Œåå­—å¯ä»¥å°ä¼—ï¼Œä½†å¿…é¡»åœ¨è¯¥ç»†åˆ†é¢†åŸŸå¾ˆä¸“ä¸šã€‚å¦‚æœä½ ä¸ç¡®å®šè¯¥é€‰è°ï¼Œå¯ä»¥å…ˆåé—®æˆ‘2ä¸ªå®šä½é—®é¢˜å†é€‰ã€‚
å…ˆè¾“å‡º1.ä½ é€‰è°ï¼Œä»–å¯¹åº”çš„ç»†åˆ†é¢†åŸŸ2.ä¸ºå•¥é€‰ä»–ï¼Œä¸‰å¥è¯ç„¶åå†è®©æˆ‘æè¿°è¯¦ç»†çš„é—®é¢˜ã€‚
```

2. ç»™ç­”æ¡ˆå‰å…ˆè®©AIè¿½é—®

```sh
ã€ä½ çš„é—®é¢˜/éœ€æ±‚ã€‘è¯·ä½ åœ¨å›ç­”å‰ï¼Œå…ˆé—®æˆ‘é—®é¢˜ã€‚è¦æ±‚ï¼šä¸€æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜ã€‚æ ¹æ®æˆ‘çš„å›ç­”ï¼Œç»§ç»­è¿½é—®ã€‚ç›´åˆ°ä½ æœ‰95%çš„ä¿¡å¿ƒç†è§£æˆ‘çš„çœŸå®éœ€æ±‚å’Œç›®æ ‡ã€‚ç„¶åæ‰ç»™å‡ºæ–¹æ¡ˆ
```

3. ä¸AIè¾©è®º

```sh
æˆ‘é©¬ä¸Šè¦å‚åŠ ä¸€åœºè¾©è®ºèµ›ï¼Œä¼šæœ‰å¾ˆå¤šäººæ¥æŒ‘æˆ˜æˆ‘çš„è§‚ç‚¹ã€‚æˆ‘çš„è§‚ç‚¹æ˜¯ã€è§‚ç‚¹ã€‘æˆ‘å¸Œæœ›è¿™ä¸ªç†è®ºå¿…é¡»å˜å¾—æ— æ‡ˆå¯å‡»ã€‚å¦‚æœä½ æ˜¯ä¸€ä¸ªå­¦è€…ï¼Œä½ éœ€è¦ç”¨å°½ä¸€åˆ‡è®ºæ®ã€ç»†èŠ‚å’Œé€»è¾‘ï¼Œæ¥æŒ‘æˆ˜æˆ‘ã€åé©³æˆ‘ã€‚ä½ çš„å”¯ä¸€ç›®æ ‡ï¼Œå°±æ˜¯è¯æ˜æˆ‘æ˜¯é”™çš„ã€‚ä½ ä¼šæ€ä¹ˆåé©³å‘¢ï¼Ÿ
```

å½“ç„¶æ€è·¯éƒ½å·®ä¸å¤šï¼Œè¿™ç©æ„ä¸éœ€è¦èƒŒä¸‹æ¥ï¼Œç®€åŒ–ç‰ˆçš„ä¹Ÿè¡Œã€‚

```sh
ã€æˆ‘çš„æƒ³æ³•/è§‚ç‚¹ã€‘è¯·ä½ ç°åœ¨æ‰®æ¼”ä¸€ä¸ª"åå¯¹è€…è§’è‰²"ï¼Œä»ä¸åŒè§’åº¦æ”»å‡»æˆ‘çš„æƒ³æ³•ï¼Œå¸®æˆ‘å®Œå–„æˆ‘çš„è§‚ç‚¹ã€‚è¦æ±‚ï¼šä¸ç”¨å®¢æ°”ï¼Œç›´æ¥æŒ‡å‡ºæ¼æ´ã€‚
```

4. è®©AIæå‰é¢„æ¼”å¤±è´¥

æˆ‘ä»¬äººåšè®¡åˆ’çš„æ—¶å€™å°±å¾ˆå®¹æ˜“çƒ­è¡€ï¼ŒAIåšè®¡åˆ’çš„æ—¶å€™ä¹Ÿå®¹æ˜“ä¹è§‚ã€‚è¿™ä¿©å‡‘ä¸€å—ï¼Œç»å¸¸ä¼šå¾ˆå®¹æ˜“æå‡ºé‚£ç§å¬èµ·æ¥å¾ˆç‡ƒä½†æ˜¯è½åœ°å…¨é å‘½çš„æ–¹æ¡ˆã€‚æ‰€ä»¥å‘¢ï¼Œæˆ‘è‡ªå·±å¾ˆå–œæ¬¢åœ¨ä¸€äº›è®¡åˆ’æˆ–è€…æ–¹æ¡ˆå¼€å§‹å‰ï¼Œå…ˆåšä¸€æ¬¡â€œé¢„æ¼”å¤±è´¥â€ã€‚

```sh
ã€æˆ‘çš„é¡¹ç›®/æƒ³æ³•ã€‘
è¯·å‡è®¾è¿™ä¸ªé¡¹ç›®åˆ°æ—¶å€™å¤±è´¥äº†æ‹‰äº†å¤§è·¨ã€‚
ç„¶åå›ç­”ï¼š
ä»€ä¹ˆæ—¶é—´ç‚¹å¼€å§‹å‡ºç°è¡°é€€ä¿¡å·ï¼Ÿ
æœ€è‡´å‘½çš„å†³ç­–é”™è¯¯æ˜¯ä»€ä¹ˆï¼Ÿ
ä½ å¿½è§†çš„æ ¸å¿ƒé£é™©æ˜¯ä»€ä¹ˆï¼Ÿ
å¦‚æœèƒ½é‡æ¥ï¼Œç¬¬ä¸€ä¸ªåº”è¯¥æ”¹çš„æ˜¯ä»€ä¹ˆï¼Ÿ
è¦æ±‚ï¼šå†™ä¸€ç¯‡"å¤±è´¥å¤ç›˜æ–‡ç« "ï¼Œè¦åŸºäºçœŸå®çš„ç±»ä¼¼é¡¹ç›®å¤±è´¥æ¡ˆä¾‹ã€‚
```

5. åå‘æç¤º

å°±æ˜¯æœ‰äº›æ—¶å€™ï¼Œä½ æ ¹æœ¬ä¸çŸ¥é“æ€ä¹ˆé—®ï¼Œä½ åªçŸ¥é“ä½ æƒ³è¦ä»€ä¹ˆæ ·çš„ç»“æœã€‚æ¯”å¦‚ä½ çœ‹åˆ°ä¸€ç¯‡å¾ˆç‰›çš„æ–‡æ¡ˆï¼Œæˆ–è€…å¾ˆç‰›é€¼çš„å›¾ï¼Œä½ æƒ³è¦åŒæ ·çš„ç»“æ„å’ŒèŠ‚å¥ï¼Œå¯ä½ è¯´ä¸å‡ºæ¥ã€‚é‚£å°±æŠŠæˆå“ç»™å®ƒï¼Œè®©å®ƒå€’æ¨æç¤ºè¯ã€‚
```sh
è¿™æ˜¯æˆ‘æƒ³è¦çš„æˆå“èŒƒä¾‹ã€‚
è¯·ä½ å€’æ¨ä¸€ä¸ªæç¤ºè¯ï¼Œè®©æˆ‘ç”¨å®ƒèƒ½ç¨³å®šç”ŸæˆåŒé£æ ¼çš„å†…å®¹ã€‚
å¹¶è¯´æ˜è¿™ä¸ªæç¤ºè¯é‡Œæ¯ä¸€å¥çš„ä½œç”¨ã€‚
```

6. åŒå±‚è§£é‡Šæ³•

å°±æ˜¯æˆ‘ä»¬å¹³æ—¶ç”¨AIäº†è§£ä¸€äº›æˆ‘ä»¬ä¸ç†Ÿæ‚‰çš„é¢†åŸŸï¼Œæˆ–è€…ä¸å¤ªæ‡‚çš„åè¯ï¼Œå¾ˆå¤šæ—¶å€™ï¼Œå¤§å®¶éƒ½ä¼šç”¨æˆ‘æ˜¯ä¸€ä¸ªå…­å¹´çº§å°å­¦ç”Ÿï¼Œè¯·ç”¨æˆ‘å¬å¾—æ‡‚çš„æ–¹å¼æ¥ç»™æˆ‘è§£é‡Šã€‚è¿™ä¸ªæ–¹æ³•å½“ç„¶æœ‰ç”¨ï¼Œèƒ½è®©ä½ æœ€å¿«é€Ÿåº¦äº†è§£å¤§æ¦‚æ˜¯ä¸ªå•¥ï¼Œä½†æ˜¯å…¶å®å¹¶ä¸æ˜¯ç‰¹åˆ«åˆ©äºåç»­çš„å­¦ä¹ ã€‚

æ‰€ä»¥æˆ‘ä¸€èˆ¬ï¼Œä¼šè®©ä»–ç›´æ¥å‡ºä¸¤ä¸ªç‰ˆæœ¬ï¼Œ
ä¸€ä¸ªç‰ˆæœ¬å°±æ˜¯åˆå­¦è€…ç‰ˆæœ¬ï¼Œèƒ½ç”¨æœ€å¿«çš„ç±»æ¯”å½“æˆ‘æ²¡æœ‰åŸºç¡€ï¼Œä¹Ÿèƒ½é€šä¿—æ˜“æ‡‚çš„ç†è§£ã€‚
å¦ä¸€ä¸ªç‰ˆæœ¬å°±æ˜¯æ›´ä¸ºä¸“ä¸šçš„ç‰ˆæœ¬ï¼Œè·Ÿåˆå­¦è€…ç‰ˆæœ¬è¿›è¡Œå¯¹ç…§å­¦ä¹ ï¼Œå¦‚æœä¸­é—´æœ‰ä¸æ‡‚çš„åœ°æ–¹ï¼Œç»§ç»­ç”¨åŒå±‚è§£é‡Šæ³•å‘ä¸‹æŒ–æ˜ã€‚

```sh
è¯·å¸®æˆ‘è§£é‡Šä¸€ä¸‹ã€ä½ çš„é—®é¢˜ã€‘ã€‚
è¯·ç”¨ä¸¤ç§æ–¹å¼è¿›è¡Œå›ç­”ï¼š
1.Â åˆå­¦è€…ç‰ˆæœ¬ï¼šé¢å‘å¯¹è±¡æ˜¯æ´—è„šåŸçš„å¤§çˆ·ï¼Œç”¨å¤§çˆ·ä¹Ÿèƒ½å¬å¾—æ‡‚çš„è¯è¯­ä¸ºä»–è¿›è¡Œè¯¦ç»†è§£é‡Šã€‚
2.Â æ·±åº¦ä¸“ä¸šç‰ˆæœ¬ï¼š é¢å‘å¯¹è±¡æ˜¯ä¸“ä¸šäººç¾¤ï¼Œä¸€å®šä¸èƒ½å‡ºç°äº‹å®é”™è¯¯ã€‚
```

## AI åœ¨ç ”å‘åœºæ™¯è½åœ°çš„ç°çŠ¶

- **æ™ºèƒ½ç ”å‘æ’ä»¶**ï¼šä»¥ Github Copilot/ é€šä¹‰çµç  /Comate ä¸ºä»£è¡¨ï¼Œä¸»è¦ä»¥ JetBrainsã€VSCode ä¸ºæ’ä»¶å½¢å¼ä¸ºç”¨æˆ·æä¾›ä»£ç è¡¥å…¨ä¸ºä¸»çš„æ™ºèƒ½ç¼–ç æœåŠ¡
- **AI Native çš„ IDE**ï¼šä»¥ Cursorã€Windsurfã€MarsCode ä¸ºä»£è¡¨ï¼Œä»¥ç‹¬ç«‹ IDE çš„æ–¹å¼ä¸ºå¼€å‘è€…æä¾›æœåŠ¡ï¼Œè€Œæœ‰ä¸€äº›å…¬å¸å¦‚ PearAI å·²ç»å¼€å§‹èµ°å¼€æºè·¯å¾„ï¼Œä»–ä»¬çš„å…±åŒç‰¹ç‚¹æ˜¯ä»¥ VSCode ä¸ºæŠ€æœ¯åº•åº§è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œå¥½å¤„æ˜¯èƒ½æå¤§ç¨‹åº¦ä¸Šåˆ©ç”¨ VSCode çš„æ’ä»¶å’Œå¼€æºç”Ÿæ€
- **CodeReview æ™ºèƒ½åŒ–**ï¼šè¿™ä¸ªé¢†åŸŸèµ·æ­¥æ¯”è¾ƒæ—©ï¼Œä½†æ•ˆæœå§‹ç»ˆä¸€èˆ¬ï¼Œè¿˜éœ€è¦å¾ˆé•¿æ—¶é—´çš„æ‘¸ç´¢ï¼Œé˜¿é‡Œå†…éƒ¨å¾ˆæ—©å°±å¯åŠ¨äº†è¿™ä¸ªé¡¹ç›®ï¼Œä½†æ•ˆæœå¹¶ä¸æ˜¾è‘—ï¼Œè¿™é‡Œæ—¢å­˜åœ¨æ¨¡å‹çš„èƒ½åŠ›çš„é—®é¢˜ï¼Œä¹Ÿå­˜åœ¨å·¥ç¨‹åŒ–ä¸è¶³çš„é—®é¢˜
- **RAG æœç´¢åœºæ™¯**ï¼šRAG å…¶å®è§£å†³çš„æ˜¯æœç´¢å’Œ Summary çš„é—®é¢˜ï¼Œä¾‹å¦‚çŸ¥è¯†æœç´¢ï¼Œæ™ºèƒ½ç­”ç–‘ï¼Œä½†ä¹Ÿå­˜åœ¨éå¸¸å¤§çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç”¨æˆ·é—®é¢˜çš„ä¸Šä¸‹æ–‡ä¸è¶³ï¼ŒçŸ¥è¯†ä¸ä¿é²œï¼Œä¿¡æ¯ä¸å®Œæ•´ï¼Œå¾ˆéš¾è¯„æµ‹ï¼Œç­‰ç­‰ï¼Œä½†ç”±äºå…¶é—¨æ§›æ¯”è¾ƒä½ï¼Œåè€Œæ˜¯å¤§å¤šæ•°å›¢é˜Ÿä¼šé¦–å…ˆæ¶‰è¶³çš„é¢†åŸŸ
- **å…¶ä»–çš„åœºæ™¯**ï¼šä¾‹å¦‚æ™ºèƒ½è§£å†³ä»£ç å†²çªï¼Œè‡ªåŠ¨è§£å†³ç¼–è¯‘é—®é¢˜ç­‰ä¹Ÿéƒ½åœ¨é˜¿é‡Œå†…éƒ¨å¹³å°æ—©å·²ä¸Šçº¿ï¼Œæ™ºèƒ½è¯Šæ–­ï¼Œæ™ºèƒ½ç›‘æ§ç­‰å‡æœ‰äººåœ¨è°ƒç ”ä¸­
- **å±€éƒ¨æ™ºèƒ½åŒ–çš„ Agent**ï¼šä»¥ Gru.ai ç­‰äº§å“ä¸ºä»£è¡¨å¸®åŠ©ç”¨æˆ·ç”Ÿæˆå•å…ƒæµ‹è¯•ï¼Œä»¥ readme-ai ä¸ºä»£è¡¨å¸®åŠ©å¼€å‘è€…ç”Ÿæˆ Readmeï¼Œä»¥ RepoAgent ä¸ºä»£è¡¨å¸®ç”¨æˆ·è¡¥å……æ³¨é‡Šç­‰ç­‰ï¼Œè€Œé˜¿é‡Œåœ¨å†…éƒ¨ä¹Ÿè¿˜å®ç°äº†å¸®åŠ©ç”¨æˆ·æŒ‰æ•´ä¸ªä»“åº“ç”Ÿæˆæ³¨é‡Šï¼Œç”Ÿæˆå•å…ƒæµ‹è¯•çš„ Agent ï¼Œè¿™ç±» Agent çš„ç‰¹ç‚¹æ˜¯åœºæ™¯æ¯”è¾ƒæ¯”è¾ƒå‚ç›´ç®€å•ï¼Œé—®é¢˜ä¸å‘æ•£ï¼ŒæˆåŠŸç‡æ¯”è¾ƒé«˜
- **å¹¿æ³›è‡ªåŠ¨åŒ–çš„ Agent**ï¼šä»¥ Devinã€OpenDevin ä¸ºä»£è¡¨ï¼Œä»¥ SWE-bench ä¸ºä¸»è¦è¯„æµ‹é›†çš„æ–¹å¼ï¼Œåˆ©ç”¨å¤§æ¨¡å‹ç”Ÿæˆå®ç°ä¸€ä¸ªä»»åŠ¡çš„ planï¼Œå¹¶è°ƒç”¨å·¥å…·ï¼Œåœ¨ä¸€ä¸ªç‹¬ç«‹çš„å®¹å™¨å†…æ‰§è¡Œï¼Œå¹¶ä¸”èƒ½å’Œç”¨æˆ·äº¤äº’çš„æ–¹å¼æ¥å®ç°ä¸€äº›ç®€å•çš„ issue æˆ–è€…éœ€æ±‚

[ä¸€ä¸ªéå¸¸è¯¦ç»†çš„Vibe CodingæŒ‡å—](https://mp.weixin.qq.com/s/qNiAym5a70mWqXsBBLnitw)

[é¢å‘ AI Codingçš„ç ”å‘ä½“ç³»](https://mp.weixin.qq.com/s/sQ8zVNuaL7UxqGep_nCvjg)

AI è¾…åŠ©å¼€å‘ç¯å¢ƒ â€”â€” MCP é©±åŠ¨çš„é«˜æ•ˆç¼–ç è¿™æ˜¯ç ”å‘çš„ä¸»æˆ˜åœºã€‚

å¼€å‘è€…ä½¿ç”¨æ”¯æŒ MCP çš„ IDEï¼ˆå¦‚ TRAEï¼‰ã€‚

- åç«¯å¼€å‘ï¼šIDE é€šè¿‡ApiFox MCPè¯»å–å·²å†»ç»“çš„å¥‘çº¦ï¼ŒAI ä¼šè‡ªåŠ¨ç”Ÿæˆç²¾å‡†çš„ Controller éª¨æ¶ã€å…¥å‚æ ¡éªŒé€»è¾‘å’Œæ•°æ®æ¨¡å‹ã€‚å¼€å‘è€…åªéœ€å¡«å……æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ã€‚
- å‰ç«¯å¼€å‘ï¼šIDE åŒæ—¶æ¿€æ´»Figma MCPå’ŒApiFox MCPã€‚AI å·¦æ‰‹æ‹¿ç€ UI è®¾è®¡ç¨¿ï¼Œå³æ‰‹æ‹¿ç€ API æ•°æ®ç»“æ„ï¼Œèƒ½å¤Ÿç”ŸæˆåŒ…å«æ ·å¼ã€äº¤äº’å’Œç½‘ç»œè¯·æ±‚çš„å…¨æ ˆç»„ä»¶ä»£ç ã€‚

è¿™ä¸€æ­¥æå¤§åœ°å‡å°‘äº†é‡å¤åŠ³åŠ¨ï¼Œå¹¶ç¡®ä¿äº†ä»£ç ä¸éœ€æ±‚ã€è®¾è®¡çš„é«˜åº¦ä¸€è‡´æ€§ã€‚

## AI ç¼–ç åŠ©æ‰‹

[What else should determine my model use in Cline? : r/ChatGPTCoding](https://www.reddit.com/r/ChatGPTCoding/comments/1hsx76e/what_else_should_determine_my_model_use_in_cline/)

[lst97/claude-code-sub-agents: Collection of specialized AI subagents for Claude Code for personal use.](https://github.com/lst97/claude-code-sub-agents)

### Continue

[Context providers | Continue](https://docs.continue.dev/customize/context-providers#gitlab-merge-request)

add context: Ctrl+I

### Cline

[cline: Autonomous coding agent](https://github.com/cline/cline)

[VSCode + Cline + VLLM + Qwen2.5 = Fast : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1gbb2de/vscode_cline_vllm_qwen25_fast/)

[set up cline and LLM](https://www.reddit.com/r/LocalLLaMA/comments/1gbb2de/comment/ltkf1z3)

```sh
docker run --runtime nvidia --gpus all \
-v ~/.cache/huggingface:/root/.cache/huggingface \
--ipc=host -p 8000:8000 \
vllm/vllm-openai \
--model Qwen/Qwen2.5-32B-Instruct-AWQ  --tensor-parallel-size 2 \
--quantization awq_marlin --enable-auto-tool-choice --tool-call-parser hermes \
--kv-cache-dtype fp8_e5m2 \
--rope-scaling '{ "factor": 4.0, "original_max_position_embeddings": 32768, "type": "yarn" }'
```

[Install the Cline extension onto VSCode](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)

Cline ä½¿ç”¨ GitHub Copilotï¼šAPI Provider > vscode lm api > Language Model > github copilot

### Roo Code

[Roo Code (prev. Roo Cline) gives you a whole dev team of AI agents in your code editor.](https://github.com/RooVetGit/Roo-Code)

[[Poweruser Guide] Level Up Your RooCode: Become a Roo Poweruser! [Memory Bank] : r/RooCode](https://www.reddit.com/r/RooCode/comments/1jfx9mk/poweruser_guide_level_up_your_roocode_become_a/)

[GreatScottyMac/RooFlow: RooFlow - Enhanced Memory Bank System with â˜¢ï¸Footgun Powerâ˜¢ï¸ Next-gen Memory Bank system with five integrated modes and system-level customization. Uses Roo Code's experimental "Footgun" feature for deep AI assistant customization while maintaining efficient token usage!](https://github.com/GreatScottyMac/RooFlow/tree/main)

Available Keyboard Commands:

- roo-cline.focusInput: ctrl+alt+r
- Add to Context: ctrl+alt+c
- roo.acceptInput: ctrl+enter

### Google Gemini Code Assist

[Gemini Code Assist for business | Google Cloud](https://codeassist.google/products/business)
[google-gemini/gemini-cli: An open-source AI agent that brings the power of Gemini directly into your terminal.](https://github.com/google-gemini/gemini-cli)

Toggle integration: After the one-time setup, you can easily manage the integration:
[Gemini CLI + VS Code: Native diffing and context-aware workflows - Google Developers Blog](https://developers.googleblog.com/en/gemini-cli-vs-code-native-diffing-context-aware-workflows/)

To activate it: /ide enable
To deactivate it: /ide disable

## AI ä½¿ç”¨æ€è·¯

### Review code

[GitLab AI Code Review å®è·µåˆ†äº« Â· Issue #38 Â· hewking/blog](https://github.com/hewking/blog/issues/38)
[GitLab - åˆ©ç”¨ç”Ÿæˆå¼ AI å®ç°ä»£ç å®¡æ ¸è‡ªåŠ¨åŒ–](https://codelabs.developers.google.com/genai-for-dev-gitlab-code-review?hl=zh-cn)
[sunmh207/AI-Codereview-Gitlab: åŸºäºå¤§æ¨¡å‹(DeepSeek,OpenAIç­‰)çš„ GitLab è‡ªåŠ¨ä»£ç å®¡æŸ¥å·¥å…·ï¼›æ”¯æŒé’‰é’‰/ä¼ä¸šå¾®ä¿¡/é£ä¹¦æ¨é€æ¶ˆæ¯å’Œç”Ÿæˆæ—¥æŠ¥ï¼›æ”¯æŒDockeréƒ¨ç½²ï¼›å¯è§†åŒ– Dashboardã€‚](https://github.com/sunmh207/AI-Codereview-Gitlab)

[AI Code Review Tools for GitLab Merge Requests (2025 Guide)](https://www.getpanto.ai/blog/ai-code-review-tools-gitlab-merge-requests)

### æŠŠ AI å½“å·¥å…·çš„ä¸‰ç§æ€è·¯

[å¤§æ¨¡å‹æ—¶ä»£çš„å­¦ä¹ ï¼Œå¯ä¸åªæ˜¯å†™å†™ prompt](https://mp.weixin.qq.com/s/C-RHduSapy23Y1QQUeDImg)

æˆ‘ç®€å•ç»™å¤§å®¶ä»‹ç»ä¸‹ï¼Œæ ¸å¿ƒæ€è·¯å…¶å®å°±æ˜¯ï¼šAI è½»å®è·µã€‚ä¸æ˜¯æ¨èå¤§å®¶å•ƒå¤§éƒ¨å¤´æˆ–è€…æ­»ç£•æºä»£ç ï¼Œçœ‹è®ºæ–‡ï¼Œè€Œæ˜¯ç”¨ AI å½“æ‹æ£ï¼Œåœ¨åŠ¨æ‰‹çš„è¿‡ç¨‹èƒ½åŠ›å­¦ä¸œè¥¿ã€‚

ç¬¬ä¸€ä¸ªæ–¹æ³•å«â€œæç®€å¤åˆ»â€ã€‚é‡åˆ°å¤æ‚çš„å¼€æºé¡¹ç›®æˆ–è®ºæ–‡ï¼Œä¸ç”¨æ­»ç£•ç»†èŠ‚ï¼Œç›´æ¥æ‰”ç»™ AI ç¼–ç¨‹å·¥å…·ï¼Œè®©å®ƒç”¨ä½ ç†Ÿæ‚‰çš„è¯­è¨€åšä¸€ä¸ªæç®€ç‰ˆ Demoã€‚æ¯”å¦‚åŸé¡¹ç›®æ˜¯ .NET å†™çš„ï¼Œä½ ç”¨ Python å¤åˆ»ä¸ªç®€åŒ–ç‰ˆï¼Œè·‘èµ·æ¥çœ‹çœ‹æ ¸å¿ƒé€»è¾‘ï¼›è®ºæ–‡é‡Œçš„ç®—æ³•æµç¨‹ï¼Œä¹Ÿèƒ½è®© AI ç”Ÿæˆä¸ªæ¼”ç¤º demoã€‚å¥½å¤„æ˜¯èƒ½å¿«é€Ÿåœ¨è‡ªå·±çš„ç¯å¢ƒé‡Œè·‘é€šï¼Œä»å®è§‚åˆ°ç»†èŠ‚ä¸€å±‚å±‚æŒ–ï¼Œæ¯”å¯¹ç€æ–‡æ¡£ç©ºæƒ³å®åœ¨å¤šäº†ã€‚å½“ç„¶äº†ï¼Œè¿™ç§å¾—æœ‰ç‚¹ç¼–ç¨‹çš„åº•å­ã€‚

ç¬¬äºŒä¸ªæ–¹æ³•è—åœ¨å¾®ä¿¡é‡Œâ€”â€”ç”¨â€œå…ƒå®â€ AI åŠ©ç†ç»ƒä¹ æ‰¹åˆ¤æ€§æ€ç»´ã€‚çœ‹åˆ°å…¬ä¼—å·æ–‡ç« ã€ç¾¤èŠè§‚ç‚¹ï¼Œç›´æ¥è½¬ç»™å®ƒï¼Œå¤šé—®å‡ å¥â€œè¿™ç»“è®ºæœ‰å•¥å±€é™â€ï¼Œæˆ–è€…â€œå’Œå¦ä¸€ä¸ªè§‚ç‚¹å†²çªåœ¨å“ªâ€ï¼Œç­‰ç­‰ã€‚å…³é”®æ˜¯åˆ«å½“ä¼¸æ‰‹å…šï¼Œå¸¦ç€è´¨ç–‘å»èŠï¼Œå¾—åˆ°çš„ç­”æ¡ˆè¿˜èƒ½å­˜æˆçŸ¥è¯†åº“ã€‚å¾®ä¿¡æœ¬æ¥å°±æ˜¯é«˜é¢‘å·¥å…·ï¼Œé¡ºæ‰‹å°±èƒ½æææ·±åº¦å­¦ä¹ ï¼Œæ¯”åˆ·ä¿¡æ¯æµå¼ºå¤ªå¤šäº†ã€‚

ç¬¬ä¸‰ä¸ªæ–¹æ³•å«â€œä¸‡ç‰©çš†å¯ Vibe Codingâ€ã€‚æœ‰ä¸ªæƒ³æ³•å°±èµ¶ç´§è®© AI å¸®ä½ å¼„æˆåŸå‹ï¼Œæ¯”å¦‚ç»™å¨ƒåšä¸ªæœ‰å£°ç»˜æœ¬ç½‘ç«™ï¼Œæˆ–è€…å¤åˆ»æŸä¸ªä½é…ç‰ˆçš„äº§å“ã€‚ä¸ç”¨è¿½æ±‚å®Œç¾ï¼Œæä¸ªâ€œç”¨å®Œå³å¼ƒâ€çš„ç®€æ˜“ç‰ˆå°±è¡Œã€‚ä»æ¶ˆè´¹è€…å˜æˆåˆ›é€ è€…ï¼Œå¯¹æŠ€æœ¯å’Œäº§å“çš„ç†è§£ç«‹é©¬ä¸ä¸€æ ·ï¼Œä¿æŒå¥½å¥‡å¿ƒï¼Œæ°¸è¿œä¸è€ã€‚

è¿™ä¸‰ä¸ªæ–¹æ³•ï¼Œä½œè€…ç»™å‡ºäº†å®æ“æŒ‡å—ï¼Œå¤§å®¶å¯ä»¥å»å¢¨é—®é‡Œå­¦ä¹ ï¼Œå…¶å®å°±æ˜¯â€œç”¨ AI é™ä½å®è·µé—¨æ§›ï¼Œåœ¨åŠ¨æ‰‹é‡Œæ‰¾ä½“æ„Ÿâ€ã€‚ä¸ç”¨ç­‰å­¦é€äº†æ‰å¼€å§‹ï¼Œè¾¹åšè¾¹è¿­ä»£ï¼Œåè€Œå­¦å¾—æ›´æ‰å®ã€‚

## AI åº”ç”¨æ„å»º

### å›´ç»•AIèƒ½åŠ›æ„å»ºæœ‰ä»·å€¼çš„AIäº§å“

[å¤§è¯­è¨€æ¨¡å‹é€‚ç”¨ä¸šåŠ¡åœºæ™¯åŠè½åœ°æ¡ˆä¾‹æ¢³ç†-çŸ¥ä¹](https://www.zhihu.com/xen/market/training/training-video/1893664728515068172/1893665727908644714?education_channel_code=ZHZN-cd8085beea05e6d)

[å›´ç»•AIèƒ½åŠ›æ„å»ºæœ‰ä»·å€¼çš„AIäº§å“-çŸ¥ä¹](https://www.zhihu.com/xen/market/training/training-video/1890413236689564620/1890414867116177303?education_channel_code=ZHZN-cd8085beea05e6d)

[â€Œã€ŠAIå¤§æ¨¡å‹è§£å†³æ–¹æ¡ˆä¸“å®¶åŸ¹å…»è®¡åˆ’ã€‹è¯¾ç¨‹å¤§çº² - é£ä¹¦äº‘æ–‡æ¡£](https://ncnmfdan85y5.feishu.cn/wiki/ODZiw9uoCiHfnQkvOF4cQksRnbd)

å¦‚ä½•æå‡æ€§èƒ½

æå‡çŠ¶æ€åˆ¤æ–­å‡†ç¡®åº¦ã€æå‡Function Call å‡†ç¡®åº¦ã€æå‡RAG å‡†ç¡®åº¦ã€æå‡Agent å¯æ§æ€§

| æ­¥éª¤ | å½±å“å‡†ç¡®ç‡çš„å…³é”® |
| --- | --- |
| å¤„ç†ç”¨æˆ·çš„è¯·æ±‚ åˆ¤æ–­æ˜¯å¦å”¤èµ·å·¥ä½œæµ | LLMçš„ç†è§£èƒ½åŠ› |
| ç¡®å®šåˆ†ææ¡†æ¶ | LLMçš„ç†è§£èƒ½åŠ›ã€Promptã€Agent |
| é‡å†™ç”¨æˆ·è¯·æ±‚ | LLMçš„ç†è§£èƒ½åŠ›+ç”Ÿæˆèƒ½åŠ› 0 |
| æ£€ç´¢èµ„æ–™ | RAGã€Embedding |
| æ•´åˆç”Ÿæˆå›å¤ | LLMçš„ç†è§£èƒ½åŠ›+ç”Ÿæˆèƒ½åŠ› |

æœ€åˆçº§ï¼šåªä¼šç”¨Prompt
åˆçº§ï¼šAgentåæ€+çº é”™
ä¸­çº§ï¼šå¤šAgentåä½œã€RAG
é«˜çº§ï¼šFine tuneã€Embeddingå®šåˆ¶
é¡¶çº§ï¼šè®­ç»ƒå‚ç›´è¡Œä¸šLLM

æ¨¡å‹èƒ½åŠ›ä¸åº”ç”¨åœºæ™¯ æ¨¡å‹èƒ½åŠ›å¦‚ä½•åº”ç”¨åœ¨å®é™…äº§å“

åšAäº§å“ï¼Œåº”è¯¥ä»å“ªé‡Œå‡ºå‘

1ã€é€‰æ‹©æœ‰ä»·å€¼çš„åº”ç”¨åœºæ™¯ï¼Œç„¶åç ”ç©¶å¦‚ä½•é€šè¿‡æ¨¡å‹èƒ½åŠ›å®ç°åŠŸèƒ½
2ã€ç ”ç©¶å„ç±»æ¨¡å‹èƒ½åŠ›è¾¹ç•Œï¼Œç„¶åç ”ç©¶æ¨¡å‹èƒ½åŠ›æœ‰ä½•å®é™…åº”ç”¨åœºæ™¯

ä¸€äº›å…¬å¸AåŒ–é¡¹ç›®çš„æ–¹å‘

1ã€ä¸è¦ä¸€ä¸Šæ¥å°±æƒ³åšä¸€ä¸ªå•ç‹¬çš„Aäº§å“
2ã€ä»ä¸€ä¸ªä¼˜åŒ–ç‚¹å¼€å§‹
3ã€2Cäº§å“å»ºè®®ä»ç•™å­˜ã€æ´»è·ƒã€äº’åŠ¨ç‡ä¸Šè€ƒè™‘
4ã€2Bä»å¤§è§„æ¨¡æ•ˆç‡å‡ºå‘
5ã€å†…éƒ¨ç³»ç»Ÿä¸›ç”¨æˆ·æ“ä½œå¤æ‚åº¦

### é˜¿é‡Œäº‘å¤§æ¨¡å‹ è¯¾ç¨‹

[é˜¿é‡Œäº‘å¤§æ¨¡å‹å·¥ç¨‹å¸ˆACAè®¤è¯å…è´¹è¯¾ç¨‹](https://edu.aliyun.com/course/3126500/)
[é˜¿é‡Œäº‘å¤§æ¨¡å‹é«˜çº§å·¥ç¨‹å¸ˆACPè®¤è¯è¯¾ç¨‹](https://edu.aliyun.com/course/3130200/)

[10åˆ†é’Ÿæ­å»ºä¸€ä¸ªæ‹¥æœ‰å¤§æ¨¡å‹èƒ½åŠ›ä»¥åŠä¸“å±çŸ¥è¯†åº“çš„é’‰é’‰æœºå™¨äºº\_å¤§æ¨¡å‹æœåŠ¡å¹³å°ç™¾ç‚¼(Model Studio)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/model-studio/use-cases/add-an-ai-assistant-to-your-dingtalk-in-10-minutes)
[10åˆ†é’Ÿè®©å¾®ä¿¡å…¬ä¼—å·æˆä¸ºæ™ºèƒ½å®¢æœ\_å¤§æ¨¡å‹æœåŠ¡å¹³å°ç™¾ç‚¼(Model Studio)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/model-studio/use-cases/add-an-ai-assistant-to-your-wechat-in-10-minutes)

### åº”ç”¨æ¡ˆä¾‹

æ™ºèƒ½ç®€å†ç­›é€‰ Agent ç³»ç»Ÿ

1. è®¾è®¡ [å®æˆ˜æ¡ˆä¾‹ï¼šä» 0 åˆ° 1 æ­å»º LLM æ™ºèƒ½ç®€å†ç­›é€‰ Agent ç³»ç»Ÿï¼ˆè®¾è®¡+å®ç°ï¼‰](https://mp.weixin.qq.com/s/66zlftFwb2FmmkEddki1Xg)
2. å®ç°æ€è·¯[å¼€æº LLM æ™ºèƒ½ç®€å†ç­›é€‰ Agent ç³»ç»Ÿï¼ˆè®¾è®¡+å®ç°ï¼‰- code by claude code + kimi - å¼€å‘è°ƒä¼˜ - LINUX DO](https://linux.do/t/topic/861309)
3. github ä»£ç  [liangdabiao/LLM-Agent-Resume: å®æˆ˜æ¡ˆä¾‹ï¼šä» 0 åˆ° 1 æ­å»º LLM æ™ºèƒ½ç®€å†ç­›é€‰ Agent ç³»ç»Ÿï¼ˆè®¾è®¡+å®ç°ï¼‰- code by claude code + kimi](https://github.com/liangdabiao/LLM-Agent-Resume)

#### çŸ¥è¯†åº“

[NirDiamant/RAG\_Techniques: This repository showcases various advanced techniques for Retrieval-Augmented Generation (RAG) systems.](https://github.com/NirDiamant/RAG_Techniques)

[hefeng6500/UltimateRAG: ä¸€ä¸ªæŒ‰è·¯çº¿å›¾é€æ­¥å®ç°çš„ RAG å­¦ä¹ é¡¹ç›®ï¼šä» Naive RAG â†’ Advanced RAG â†’ Agentic RAG â†’ GraphRAG & Fine-tuning â†’ RAGOpsï¼Œåœ¨æœ¬åœ°ç”¨ LangChain + å‘é‡åº“æŠŠâ€œæ–‡æ¡£é—®ç­”â€è·‘é€šï¼Œå¹¶é€æ­¥åŠ å…¥è¯­ä¹‰åˆ†å—ã€æ··åˆæ£€ç´¢ã€é‡æ’åºã€è·¯ç”±ã€è‡ªåæ€ä¸å·¥å…·è°ƒç”¨ç­‰èƒ½åŠ›ã€‚ ä»æœ€ç®€å•çš„ Demo è¿›åŒ–åˆ°ä¼ä¸šçº§ã€ç”šè‡³ç§‘ç ”çº§çš„ RAG ç³»ç»Ÿã€‚](https://github.com/hefeng6500/UltimateRAG)

- **Stage 1ï¼šNaive RAG**
    æœ€å°å¯ç”¨é—­ç¯ï¼šæ–‡æ¡£ â†’ åˆ†å— â†’ å‘é‡æ£€ç´¢ â†’ é—®ç­”
    è§£å†³â€œåˆ°åº•åœ¨è·‘ä»€ä¹ˆâ€
- **Stage 2ï¼šAdvanced RAG**
    è¯­ä¹‰åˆ†å—ã€æ··åˆæ£€ç´¢ã€é‡æ’åºã€è·¯ç”±
    è§£å†³â€œä¸ºä»€ä¹ˆæ•ˆæœä¸ç¨³å®šâ€
- **Stage 3ï¼šAgentic RAG**
    å¤šæ­¥æ¨ç†ã€å·¥å…·è°ƒç”¨ã€è‡ªåæ€
    è§£å†³â€œå¤æ‚é—®é¢˜æ€ä¹ˆæ‹†è§£â€
- **Stage 4ï¼šGraphRAG / Fine-tuning**
    ç»“æ„åŒ–çŸ¥è¯†ã€å…³ç³»æ¨ç†ã€æ¨¡å‹é€‚é…
    å¼€å§‹æ¥è¿‘ç§‘ç ”ä¸å¤æ‚ä¸šåŠ¡åœºæ™¯
- **Stage 5ï¼šRAGOps**
    è¯„æµ‹ã€å›å½’ã€ç›‘æ§ã€æŒç»­ä¼˜åŒ–
    é¢å‘é•¿æœŸè¿è¡Œçš„ç³»ç»Ÿ

æ¯ä¸€é˜¶æ®µéƒ½æ˜¯**å¯è¿è¡Œã€å¯å•ç‹¬ç†è§£ã€å¯ç»§ç»­æ‰©å±•**çš„ã€‚

[ä»å·¥å•ã€æ–‡æ¡£åˆ°ç»“æ„åŒ–çŸ¥è¯†åº“ï¼šä¸€å¥—å¯å¤ç”¨çš„ Agent çŸ¥è¯†é‡‡é›†æ–¹æ¡ˆ](https://mp.weixin.qq.com/s/8iOvDIgOJlxi7LmcuuhCQQ)

çŸ¥è¯†åº“ç›´æ¥å¬å›æ•ˆæœå·®

åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œå‘é‡å¬å›ç»å¸¸ä¼šé‡åˆ°ä¸€ä¸ªç°å®é—®é¢˜ï¼š
ç”¨æˆ·é—®çš„æ˜¯å£è¯­åŒ–ã€å¤šè§’åº¦é—®é¢˜ï¼›
çŸ¥è¯†åº“é‡Œå­˜çš„æ˜¯é«˜åº¦ä¹¦é¢åŒ–ã€å•ä¸€è¡¨è¾¾çš„æ–‡æ¡£æè¿°ï¼›
ç»“æœå°±æ˜¯ï¼šè¯­ä¹‰è·ç¦»ç®—å‡ºæ¥ä¸å¤Ÿè¿‘ï¼Œå¬å›æ•ˆæœåå¼±ï¼›

æˆ‘ç›®å‰åœ¨ç”¨çš„ä¸¤ç±»è§£å†³æ€è·¯ï¼š
å‰ç½®æ³›åŒ–(æœ¬æ–‡æ–¹æ¡ˆ)ï¼š
åœ¨çŸ¥è¯†å…¥åº“å‰ï¼Œè®© AI å¯¹æ¯æ¡çŸ¥è¯†åšâ€œé—®æ³•æ³›åŒ–â€ï¼ŒæŠŠç”¨æˆ·å¯èƒ½ä½¿ç”¨çš„è¯´æ³•é¢„å…ˆå±•å¼€ã€‚
è¿™æ ·çŸ¥è¯†åº“å°±ä¸å†æ˜¯â€œä¸€é—®ä¸€ç­”â€ï¼Œè€Œæ˜¯â€œä¸€ç­”å¯¹åº”å¤šé—®â€ï¼Œæœ‰æ•ˆæå‡å¬å›ç‡ã€‚

å¬å›åè´¨æ£€ï¼š
åœ¨ Agent ä¸­å•ç‹¬è®¾ä¸€ä¸ªâ€œçŸ¥è¯†è´¨æ£€èŠ‚ç‚¹â€ï¼Œè´Ÿè´£ï¼šä¿®æ”¹/é‡å†™ç”¨æˆ·é—®é¢˜ï¼Œå¤šæ¬¡å°è¯•å¬å›ï¼›
å¯¹å¬å›çŸ¥è¯†è¿›è¡Œç›¸å…³æ€§åˆ¤æ–­å’Œâ€œå¯ä¿¡åº¦æ‰“åˆ†â€ï¼›
å¿…è¦æ—¶è¿‡æ»¤æ‰ç›¸å…³æ€§ä¸å¤Ÿé«˜çš„çŸ¥è¯†ï¼›

è¿™ä¸¤å±‚å¤„ç†å åŠ åï¼Œå®é™… RAG ä½“éªŒä¼šå¥½å¾ˆå¤šã€‚

## æ¨¡å‹

### deepseek

vllm + ray

deepseek janus pro å¤šæ¨¡æ€å¤§æ¨¡å‹ç‚¸è£‚å‡ºåœºï¼Œtransformeræ¶æ„ï¼Œæ²¡æœ‰èµ°diffusionè·¯çº¿

[deepseek-ai/DeepSeek-R1 Â· Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-R1)

[DeepSeek Token ç”¨é‡è®¡ç®— | DeepSeek API Docs](https://api-docs.deepseek.com/zh-cn/quick_start/token_usage)

[KTransformers 4090å•å¡è·‘671B DeepSeek-R1 - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/23212558318)
[å•å¡RTX4090éƒ¨ç½²R1æ»¡è¡€ç‰ˆä¹‹KTransformersç¯‡-é˜¿æœ±](https://mp.weixin.qq.com/s/g3JsrLUuMXDX-8lSSzb06A)

[4090å•å¡éƒ¨ç½²QWen2.5-VLè§†è§‰æ¨¡å‹ - é˜¿æœ±](https://mp.weixin.qq.com/s/Ha-J5uUKk7XUqMfW_VEqHg)

[ä¸åˆ° 4 ä¸‡å…ƒçš„ DeepSeek-R1-671B-Q8 éƒ¨ç½²æ–¹æ¡ˆ - è…¾è®¯ç„æ­¦å®éªŒå®¤](https://mp.weixin.qq.com/s/vIrvbVJ6Nv00Ehre1zZwMw)

[Deepseek V3 0324 modelfile : r/ollama](https://www.reddit.com/r/ollama/comments/1jpk3ty/deepseek_v3_0324_modelfile)

#### quantized DeepSeek-R1

[Deployment-ready reasoning with quantized DeepSeek-R1 models | Red Hat Developer](https://developers.redhat.com/articles/2025/03/03/deployment-ready-reasoning-quantized-deepseek-r1-models#)

INT4 models recover 97%+ accuracy for 7B and larger models, with the 1.5B model maintaining ~94%.

#### å†…å­˜ä½¿ç”¨é‡è®¡ç®—

[DeepSeek æœ¬åœ°åŒ–éƒ¨ç½²æŒ‡å—ï¼šç¡¬ä»¶é€‚é…å…¨è§£æ-DeepSeekæŠ€æœ¯ç¤¾åŒº](https://deepseek.csdn.net/67c14dbab8d50678a2421282.html)

å†…å­˜ä½¿ç”¨è®¡ç®—

fp16: (16/8)*70B = 140GB
fp16: (16/8)*671B = 1342 GB
int8: (8/8)*671B = 671 GB
int4: (4/8)*671B = 335.5 GB
int1: (1/8)*671B = 83.875 GB

å¯¹äº DeepSeek-R1-32B æ¨¡å‹ï¼Œè‹¥ä»¥å¸¸è§„çš„ fp16 ç²¾åº¦è®¡ç®—ï¼Œæ¯ä¸ªå‚æ•°å ç”¨ 2 å­—èŠ‚ï¼ˆ16/8ï¼‰ï¼ŒåŸºç¡€å‚æ•°å ç”¨ä¸º 320 äº¿ Ã— 2 å­—èŠ‚ = 640 äº¿å­—èŠ‚ï¼Œçº¦ 64GBã€‚ä¹˜ä»¥å®‰å…¨ç³»æ•° 1.3 åï¼ŒåŸºç¡€å‚æ•°å ç”¨æå‡è‡³ 83.2GBã€‚åœ¨å®é™…è¿è¡Œä¸­ï¼Œæ¯å¤„ç†ä¸€å®šæ•°é‡çš„ä¸Šä¸‹æ–‡ tokenï¼Œä¼šäº§ç”Ÿé¢å¤–çš„ä¸Šä¸‹æ–‡å¼€é”€ã€‚å‡è®¾å¤„ç† 4096 tokens çš„ä¸Šä¸‹æ–‡ä¼šå¢åŠ  2GB çš„ä¸Šä¸‹æ–‡å¼€é”€ï¼ˆå…·ä½“æ•°å€¼ä¼šå› æ¨¡å‹å’Œè¿è¡Œç¯å¢ƒç•¥æœ‰å·®å¼‚ï¼‰ï¼Œå½“å¤„ç† 8192 ä¸ªä¸Šä¸‹æ–‡ token æ—¶ï¼Œä¸Šä¸‹æ–‡æ‰©å±•é‡ä¸º 2GB Ã— 2 = 4GBã€‚è‹¥å†è€ƒè™‘ç³»ç»Ÿç¼“å­˜å¯èƒ½å ç”¨ 3GBï¼ˆå®é™…ä¼šå› ç³»ç»Ÿé…ç½®ä¸åŒè€Œå˜åŒ–ï¼‰ï¼Œåˆ™ æ€»æ˜¾å­˜éœ€æ±‚ = 83.2GB + 4GB + 3GB = 90.2GBã€‚è¿™è¡¨æ˜åœ¨éƒ¨ç½² DeepSeek-R1-32B æ¨¡å‹æ—¶ï¼Œå•å¡æ˜¾å­˜è‹¥ä½äº 90.2GBï¼Œå¯èƒ½æ— æ³•ç¨³å®šè¿è¡Œ

1. æ¯ä¸ªå‚æ•°å ç”¨ 2 å­—èŠ‚ï¼ŒåŸºç¡€å‚æ•°å ç”¨ä¸º 320 äº¿ Ã— 2 å­—èŠ‚ = 640 äº¿å­—èŠ‚ï¼Œçº¦ 64GB
2. ä¹˜ä»¥å®‰å…¨ç³»æ•° 1.3 * 64GB = 83.2GB
3. å¤„ç† 8192 ä¸ªä¸Šä¸‹æ–‡ token æ—¶ï¼Œä¸Šä¸‹æ–‡æ‰©å±•é‡ä¸º 2GB Ã— 2 = 4GB
4. æ€»æ˜¾å­˜éœ€æ±‚ = 83.2GB + 4GB + 3GB = 90.2GB

[Run DeepSeek-R1 Dynamic 1.58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)
DeepSeek R1 has 61 layers

n (offload) = VRAM(GB) / Filesize(GB) Ã— n (layers) âˆ’ 4

#### The Temperature Parameter

[The Temperature Parameter | DeepSeek API Docs](https://api-docs.deepseek.com/quick_start/parameter_settings)

The default value of temperature is 1.0. We recommend users to set the temperature according to their use case listed in below.

Coding / Mathâ€ƒâ€ƒâ€ƒ                  0.0
Data Cleaning / Data Analysis     1.0
General Conversation              1.3
Translation                       1.3
Creative Writing / Poetry         1.5

#### æ¨¡å‹èƒ½åŠ›å¯¹æ¯”

| å¯¹æ¯”é¡¹ | GPT (OpenAI) | DeepSeek-R1 |
| --- | --- | --- |
| æ¨¡å‹æ¶æ„ | Transformerè§£ç å™¨æ¶æ„ (å…¨å‚æ•°æ¿€æ´») | æ··åˆä¸“å®¶æ¨¡å‹ (MoEï¼Œéƒ¨åˆ†å‚æ•°æ¿€æ´») |
| å‚æ•°è§„æ¨¡ | GPT-4ï¼š1.76ä¸‡äº¿å‚æ•°ï¼ˆå…¨å‚æ•°è®¡ç®—ï¼‰01æœª å…¬å¼€ | æ€»å‚æ•°6,710äº¿ï¼Œæ¯æ¬¡ä»…æ¿€æ´»370äº¿ |
| è®­ç»ƒæ–¹æ³• | D ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰+å¼ºåŒ–å­¦ä¹ (RLHF) | çº¯å¼ºåŒ–å­¦ä¹ (RL)ï¼Œä¸ä¾èµ– SFT |
| æ¨ç†èƒ½åŠ› | å¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ› | ä¼˜ç§€çš„æ¨ç†èƒ½åŠ›ï¼Œå…·å¤‡è‡ªæˆ‘éªŒè¯å’Œåæ€ |
| è®¡ç®—èµ„æº | éœ€è¦é«˜è®¡ç®—æˆæœ¬å’Œèƒ½è€— | MoE æœºåˆ¶é™ä½è®¡ç®—æ¶ˆè€— |
| å¹¶è¡Œè®­ç»ƒ | åŸºäºæ ‡å‡†çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ | é‡‡ç”¨ HAI-LLM å¹¶è¡Œæ¡†æ¶(16è·¯æµæ°´çº¿å¹¶è¡Œã€64è·¯ä¸“å®¶å¹¶è¡Œ) |
| å¼€æºæƒ…å†µ | GPTä¸ºé—­æº | DeepSeek-R1ä¸ºå¼€æº |
| é€‚ç”¨åœºæ™¯ | é€šç”¨å¯¹è¯ã€ä»£ç ç”Ÿæˆã€å¤æ‚æ¨ç† | é€‚ç”¨äºé«˜æ•ˆæ¨ç†ä»»åŠ¡ï¼Œä½æˆæœ¬å¤§è§„æ¨¡éƒ¨ç½² |

## GPT é¡¹ç›®

GPT (Generative Pre-trained Transformer)

[ChatGPT](https://chat.openai.com/chat)
[What Is ChatGPT Doing â€¦ and Why Does It Work?â€”Stephen Wolfram Writings](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
[ChatGPTä¸ºå•¥è¿™ä¹ˆå¼ºï¼šä¸‡å­—é•¿æ–‡è¯¦è§£ by WolframAlphaä¹‹çˆ¶-ä»Šæ—¥å¤´æ¡](https://www.toutiao.com/article/7200604582392087095)
[ChatGPTå­¦ä¹ èµ„æ–™åˆé›† Original å•å»ºä¼Ÿ é˜¿æœ±è¯´](https://mp.weixin.qq.com/s/ZApy_d873Y1DEmGc6NjplQ)

[å¦‚ä½•ä½¿ç”¨ChatGPT APIè®­ç»ƒè‡ªå®šä¹‰çŸ¥è¯†åº“AIèŠå¤©æœºå™¨äºº - é—ªç”µåš](https://www.wbolt.com/how-to-train-ai-chatbot.html)
[How to Train an AI Chatbot With Custom Knowledge Base Using ChatGPT API | Beebom](https://beebom.com/how-train-ai-chatbot-custom-knowledge-base-chatgpt-api/)
[Fine-tuning - OpenAI API](https://platform.openai.com/docs/guides/fine-tuning)

[äººäººéƒ½èƒ½æ‡‚çš„ChatGPTè§£è¯»_AI_å¼ æ°_InfoQç²¾é€‰æ–‡ç« ](https://www.infoq.cn/article/VWrPIRvRg6E3O74q7PtL)

[zhayujie/chatgpt-on-wechat: ä½¿ç”¨ChatGPTæ­å»ºå¾®ä¿¡èŠå¤©æœºå™¨äººï¼ŒåŸºäºChatGPT3.5 APIå’Œitchatå®ç°ã€‚Wechat robot based on ChatGPT, which using OpenAI api and itchat library.](https://github.com/zhayujie/chatgpt-on-wechat)

[ConnectAI-E/Dingtalk-OpenAI: ğŸ”” é’‰é’‰ & ğŸ¤– GPT-3.5 è®©ä½ çš„å·¥ä½œæ•ˆç‡ç›´æ¥èµ·é£ ğŸš€ ç§èŠç¾¤èŠæ–¹å¼ã€å•èŠä¸²èŠæ¨¡å¼ã€è§’è‰²æ‰®æ¼”ã€å›¾ç‰‡åˆ›ä½œ ğŸš€](https://github.com/ConnectAI-E/Dingtalk-OpenAI)

### AI åº”ç”¨å¼€å‘

[â€Œâ€â¤â€‹ä¸ªäººå¼€æº AI çŸ¥è¯†åº“ - é£ä¹¦äº‘æ–‡æ¡£](https://tffyvtlai4.feishu.cn/wiki/OhQ8wqntFihcI1kWVDlcNdpznFf)
[å¤§æ¨¡å‹ AI åº”ç”¨å…¨æ ˆå¼€å‘çŸ¥è¯†ä½“ç³» v1.3.1 - é£ä¹¦äº‘æ–‡æ¡£](https://agiclass.feishu.cn/docx/Z3Aed6qXboiF8gxGuaccNHxanOc)
[GitHub - deepseek-ai/awesome-deepseek-integration: Integrate the DeepSeek API into popular softwares](https://github.com/deepseek-ai/awesome-deepseek-integration)
[ç«å±±å¼•æ“ é«˜ä»£ç  Python SDK Arkitect volcengine/ai-app-lab](https://github.com/volcengine/ai-app-lab/)

[AI å…¨æ ˆå­¦å‘˜éƒ¨åˆ†ä½œå“é›† - é£ä¹¦äº‘æ–‡æ¡£](https://agiclass.feishu.cn/docx/M5xydPVjWovB9exHBjDc7IMYnub)

[çŸ¥ä¹ã€ŠAI å¤§æ¨¡å‹å…¨æ ˆå·¥ç¨‹å¸ˆã€‹è¯¾ç¨‹å¤§çº²ï¼ˆç¬¬ 05 æœŸï¼‰ - é£ä¹¦äº‘æ–‡æ¡£](https://agiclass.feishu.cn/docx/KjFSdqxTZoDDfcxzikHcjjx0nDg)

[AIå¤§æ¨¡å‹é¢ è¦†ç¨‹åºå‘˜çš„ä»·å€¼ - ç¨‹åºå‘˜çš„AIå¤§æ¨¡å‹è¿›é˜¶ä¹‹æ—…0122æœŸ](https://www.zhihu.com/xen/market/training/training-video/1730906752995045376/1730906966715797506?education_channel_code=ZHZN-d62bb90dfad9e02) è®²è§£äº†è‡ªè®­ç»ƒ å¾®è°ƒç­‰æ–¹æ³•
[å¤§æ¨¡å‹åº”ç”¨å¼€å‘æŠ€æœ¯ä½“ç³»ä¸²è®² - ç¨‹åºå‘˜çš„AIå¤§æ¨¡å‹è¿›é˜¶ä¹‹æ—…0125æœŸ](https://www.zhihu.com/xen/market/training/training-video/1731335160308744192/1731335415037206528?education_channel_code=ZHZN-cd8085beea05e6d) å­™å¿—åˆšè€å¸ˆå›ç­”é—®é¢˜
[ä½¿â½¤ Assistants APIå¿«é€Ÿæ­å»ºé¢†åŸŸä¸“å±AI - ç¨‹åºå‘˜çš„AIå¤§æ¨¡å‹è¿›é˜¶ä¹‹æ—…0122æœŸ](https://www.zhihu.com/xen/market/training/training-video/1730906752995045376/1730907032264232960?education_channel_code=ZHZN-d62bb90dfad9e02)

å¤§æ¨¡å‹é¢†åŸŸå²—ä½æ¨è
 Â· å¤§æ¨¡å‹è®­ç»ƒå¸ˆ
 Â· å¤§æ¨¡å‹ç®—æ³•å·¥ç¨‹å¸ˆ
 Â· æç¤ºè¯å·¥ç¨‹å¸ˆ
 Â· å¤§æ¨¡å‹å…¨æ ˆå¼€å‘å·¥ç¨‹å¸ˆ
 Â· å¤§æ¨¡å‹æ–¹å‘äº§å“ç»ç†
 Â· å¤§æ¨¡å‹æ–¹å‘é¡¹ç›®ç»ç†

ç»§ç»­æœ¬å²—ä½ Â· å¤§å¤§æå‡æ•ˆç‡ï¼Œæ¨ªå‘å·åŒè¡Œï¼Œçºµå‘å·ä¸Šä¸‹æ¸¸
æˆä¸ºè¶…çº§ä¸ªä½“ ç‹¬ç«‹å¼€å‘è€…ï¼Œåšè‡ªå·±çš„å°è€æ¿
æˆä¸ºå¤§æ¨¡å‹è®­ç»ƒå¸ˆ åšå…¬å¸çš„æŠ€æœ¯æ ¸å¿ƒ
ç‹¬ç«‹åˆ›ä¸š å‡­å¤§æ¨¡å‹å‚ç›´è½åœ°èƒ½åŠ›è§£å†³ç‹¬æœ‰åœº

ä½¿ç”¨ Assistants API å¿«é€Ÿæ­å»ºé¢†åŸŸä¸“å±AIåŠ©æ‰‹
Demoæ¡†æ¶ åŠå…·ä½“å®ç° Streamlit ç®€ä»‹â€” A faster way to build and share data apps

[ChatTTS-ui: ä¸€ä¸ªç®€å•çš„æœ¬åœ°ç½‘é¡µç•Œé¢ï¼Œç›´æ¥ä½¿ç”¨ChatTTSå°†æ–‡å­—åˆæˆä¸ºè¯­éŸ³ï¼ŒåŒæ—¶æ”¯æŒå¯¹å¤–æä¾›APIæ¥å£ã€‚](https://github.com/jianchang512/ChatTTS-ui)

### Text-to-SQL

[Canner/WrenAI: Open-source GenBI AI Agent that empowers data-driven teams to chat with their data to generate Text-to-SQL, charts, spreadsheets, reports, and BI](https://github.com/Canner/WrenAI)

[CodePhiliaX/Chat2DB: AI-driven database tool and SQL client, The hottest GUI client, supporting MySQL, Oracle, PostgreSQL, DB2, SQL Server, DB2, SQLite, H2, ClickHouse, and more.](https://github.com/codePhiliaX/Chat2DB)

[vanna-ai/vanna: Chat with your SQL database ğŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG.](https://github.com/vanna-ai/vanna)

## Text to Image

[DALLÂ·E 2](https://openai.com/product/dall-e-2)

[comfyanonymous/ComfyUI: The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.](https://github.com/comfyanonymous/ComfyUI)

[ComfyUIï¼šæ­ç§¯æœ¨ä¸€æ ·æ„å»ºä¸“å±äºè‡ªå·±çš„AIGCå·¥ä½œæµï¼ˆä¿å§†çº§æ•™ç¨‹ï¼‰](https://mp.weixin.qq.com/s/sr4Cpd6UAyCf75Jm2D8YTQ)

[10æ¬¾AIç»˜ç”»ç”Ÿæˆå™¨ï¼Œäººäººéƒ½æ˜¯æ’ç”»å¸ˆï¼](https://pixso.cn/designskills/10-ai-paint-builders/)

[Text To Image - AI Image Generator API | DeepAI](https://deepai.org/machine-learning-model/text2img)

[æ–‡å¿ƒä¸€æ ¼ - AIè‰ºæœ¯å’Œåˆ›æ„è¾…åŠ©å¹³å°](https://yige.baidu.com/creation)

[22ä¸ªå›½å†…AIç»˜ç”»ç½‘ç«™æ±‡æ€»](https://zl49so8lbq.feishu.cn/wiki/MdzYw0mtki3OPGkPk03cnd9hnfg#Uaaidf4Mro82FFx8ucic8MUOnBf)

### image prompt

[Guide for prompt writing | BoostPixels](https://boostpixels.com/guide)
[midjourneyå²ä¸Šæœ€å…¨æ•™ç¨‹-æŒç»­æ›´æ–° - Feishu Docs](https://nw3t0riwqkt.feishu.cn/docx/NCVdd118toPLkRxKBexcQZiunJZ)

[å¢¨æœ¬å…³é”®è¯åŠ©æ‰‹](https://www.mbprompt.com/#/)
[MidJourney Prompt Tool](https://prompt.noonshot.com/)

### Stable Diffusion

[Stable Diffusion Models: a beginner's guide - Stable Diffusion Art](https://stable-diffusion-art.com/models/)
[ControlNet: A Complete Guide - Stable Diffusion Art](https://stable-diffusion-art.com/controlnet/)
[å¸¸ç”¨çš„ControlNetä»¥åŠå¦‚ä½•åœ¨Stable Diffusion WebUIä¸­ä½¿ç”¨ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/620074109)

[Install Stable Diffusion on Mac](https://uxplanet.org/install-stable-diffusion-ui-on-mac-beginners-guide-351e40a9e8e2)
[webui Online Services Â· AUTOMATIC1111/stable-diffusion-webui Wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)
[Stability-AI/generative-models: Generative Models by Stability AI](https://github.com/Stability-AI/generative-models)
[Mikubill/sd-webui-controlnet: WebUI extension for ControlNet](https://github.com/Mikubill/sd-webui-controlnet)

### MidJourney æ¡ˆä¾‹

[ã€Midjourneyæ•™ç¨‹ã€‘è®¾è®¡éº»ç“œä¹Ÿèƒ½10åˆ†é’Ÿä¸Šæ¶ä¸€å¥—è¡¨æƒ…åŒ…](https://mp.weixin.qq.com/s/FagQ3HdAnx-HLfJK4NRMBQ)

### Mistral-7B

æ¥è‡ªæ³•å›½çš„å¼€æºå¤§æ¨¡å‹

[æœ€å¥½çš„7Bæ¨¡å‹æ˜“ä¸»ï¼Œå…è´¹å¼€æºå¯å•†ç”¨ï¼Œæ¥è‡ªâ€œæ¬§æ´²çš„OpenAIâ€-ä»Šæ—¥å¤´æ¡](https://www.toutiao.com/article/7287811935905546763/)
[mistralai (Mistral AI_)](https://huggingface.co/mistralai)

## OpenAI doc

[API Reference - OpenAI API](https://platform.openai.com/docs/api-reference/introduction)

[Playground - OpenAI API](https://platform.openai.com/playground)

[Assistants overview - OpenAI API](https://platform.openai.com/docs/assistants/overview)## prompt

[Maximizing the Potential of LLMs: A Guide to Prompt Engineering](https://www.ruxu.dev/articles/ai/maximizing-the-potential-of-llms/)

[f/awesome-chatgpt-prompts: This repo includes ChatGPT prompt curation to use ChatGPT better.](https://github.com/f/awesome-chatgpt-prompts)

[æç¤ºè¯æŠ€å·§](https://mp.weixin.qq.com/s/eqIqbbyqlgkCU78SKuZCMw)

```sh
curl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \
-H "Authorization: Bearer $DASHSCOPE_API_KEY" \
-H "Content-Type: application/json" \
-d '{
    "model": "qwen-plus",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "ä½ æ˜¯è°ï¼Ÿ"
        }
    ]
}'
```

### [Learn Prompting](https://learnprompting.org/zh-Hans/docs/intro)

#### Learn Prompting Sample

[ğŸŸ¡ Coding Assistance | Learn Prompting](https://learnprompting.org/docs/basic_applications/coding_assistance)

1. act like a senior developer
2. as a very junior developer
3. You can also dictate that it have a certain area of expertise (e.g., sorting algorithms) or number of years of experience
4. Act as Microsoft SQL Server.

##### English translator and Improver

Prompt: I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is "lovin istanbul and the city"

##### Interviewer

Prompt: I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the position position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is "Hi"

##### English Pronunciation Helper

Prompt: I want you to act as an English pronunciation assistant for Turkish speaking people. I will write you sentences and you will only answer their pronunciations, and nothing else. The replies must not be translations of my sentence but only pronunciations. Pronunciations should use Turkish Latin letters for phonetics. Do not write explanations on replies. My first sentence is "how the weather is in Istanbul?"

##### Travel Guide

Prompt: I want you to act as a travel guide. I will write you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion request is ""I am in Istanbul/BeyoÄŸlu and I want to visit only museums."

## ç†è®ºçŸ¥è¯†

[Deepseekå¤§æ¨¡å‹æ¨ç†ç®—æ³•å…¶å®å¾ˆç®€å• | é™ˆç»](https://mp.weixin.qq.com/s/SaK9mlj6NCKxEFig6KFGVQ)

1. Function Call:åŸºäºLLMçš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡ç†è§£è¯­ä¹‰ï¼Œè‡ªä¸»å†³ç­–ä½¿ç”¨æŸé¡¹å·¥å…·ï¼Œå¹¶ç»“æ„åŒ–è°ƒç”¨
2. RAG:é€šè¿‡å¥å­æˆ–æ®µè½çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ¯”è¾ƒï¼Œæ£€ç´¢ç›¸å…³èµ„æ–™ï¼Œåœ¨èµ„æ–™æ”¯æŒä¸‹ç”Ÿæˆå›å¤(Retrieval-Augmented Generation)

### é—®é¢˜

é¢äº†ä¸ªåŠè°ƒå­åç«¯è½¬AI Agentæ–¹å‘ çœŸçš„æ— è¯­
åç«¯ç¨‹åºå‘˜è½¬AI Agentæœ¬æ˜¯ä¼˜åŠ¿ç»„åˆâ€”â€”å·¥ç¨‹æ¶æ„ã€åˆ†å¸ƒå¼å¤„ç†èƒ½åŠ›éƒ½æ˜¯è½åœ°å…³é”®ã€‚ä½†è¿‘æœŸé¢è¯•ä¸­ï¼Œå¤šæ•°å€™é€‰äººçš„è¡¨ç°å´å·®è·æ˜æ˜¾ï¼Œæš´éœ²å‡ºè½¬å²—çš„å…±æ€§è¯¯åŒºã€‚

ğŸ“Œ æ¶æ„è®¤çŸ¥æµ®äºè¡¨é¢ï¼Œæ ¸å¿ƒé€»è¾‘ç©ºç™½
å€™é€‰äººå¤šèƒ½å¤è¿°â€œæ„ŸçŸ¥-å†³ç­–-æ‰§è¡Œâ€æ¶æ„ï¼Œå¯è¿½é—®â€œçŸ­æœŸä¸é•¿æœŸè®°å¿†å¦‚ä½•è”åŠ¨â€â€œå·¥å…·è°ƒç”¨å†²çªæ€ä¹ˆè°ƒåº¦â€æ—¶ï¼Œä¾¿éš¾ä»¥ç»™å‡ºæœ‰æ•ˆæ–¹æ¡ˆã€‚å®åˆ™è¿™äº›é—®é¢˜å¯ç±»æ¯”åç«¯â€œç¼“å­˜-æ•°æ®åº“è”åŠ¨â€â€œåˆ†å¸ƒå¼ä»»åŠ¡ä¼˜å…ˆçº§æ§åˆ¶â€ï¼Œä½†å¤šæ•°äººæœªå»ºç«‹çŸ¥è¯†è¿ç§»æ„è¯†ï¼Œæ¶æ„è®¤çŸ¥ä»…åœç•™åœ¨åè¯å±‚é¢ã€‚

ğŸ“Œ é¡¹ç›®æ­¢äºDemoçº§ï¼Œå·¥ç¨‹å±æ€§ç¼ºå¤±
â€œç”¨LangChainåšè¿‡AIåŠ©æ‰‹â€æ˜¯é«˜é¢‘å›ç­”ï¼Œä½†æ·±ç©¶ä¾¿éœ²é¦…ï¼šAPIè¶…æ—¶æ— é‡è¯•æœºåˆ¶ã€â€œæ•´ç†çºªè¦å¹¶å‘é‚®ä»¶â€çš„ä»»åŠ¡æ‹†åˆ†é€»è¾‘æ··ä¹±ã€æ— ä¸çº¯å¤§æ¨¡å‹çš„æ•ˆæœå¯¹æ¯”æ•°æ®ã€‚åç«¯æ“…é•¿çš„å®¹é”™è®¾è®¡ã€é“¾è·¯ç›‘æ§ç­‰èƒ½åŠ›ï¼Œæœªåœ¨Agenté¡¹ç›®ä¸­ä½“ç°ï¼Œé¡¹ç›®ä»…å®Œæˆå·¥å…·è°ƒç”¨çš„è¡¨å±‚æ‹¼æ¥ã€‚

ğŸ“Œ å·¥ç¨‹æ€ç»´æœªè¿ç§»ï¼Œè½åœ°èƒ½åŠ›è–„å¼±
è°ˆåŠ1000ç”¨æˆ·å¹¶å‘ï¼Œå›ç­”å¤šä¸ºâ€œç›´æ¥æ‰©å®¹â€ï¼Œå¿½è§†Agentä¸Šä¸‹æ–‡çŠ¶æ€ç®¡ç†ä¸èµ„æºå¤ç”¨ï¼›æåˆ°ä¸Šä¸‹æ–‡çª—å£ä¸è¶³ï¼Œä»…è¯´â€œæˆªæ–­å¤„ç†â€ï¼Œæœªè€ƒè™‘è®°å¿†ä¸¢å¤±å¯¹å†³ç­–çš„å½±å“ã€‚æ›´æœ‰å€™é€‰äººåœ¨å¤šAgentåä½œé¡¹ç›®ä¸­çœç•¥æƒé™æ ¡éªŒï¼Œè¿èƒŒåç«¯å®‰å…¨å¼€å‘åŸºçº¿ã€‚

ğŸ“Œ åç«¯è½¬å²—çš„æ ¸å¿ƒæˆé•¿è·¯å¾„
1. è¡¥AIåŸºç¡€ï¼šèšç„¦å¤§æ¨¡å‹å‡½æ•°è°ƒç”¨ã€æç¤ºè¯å·¥ç¨‹ï¼Œå°†å…¶è§†ä¸ºâ€œAIç‰ˆæ¥å£å¼€å‘â€ï¼Œå»ºç«‹æŠ€æœ¯å…³è”ï¼›
2. æ·±ç ”Agentæ ¸å¿ƒï¼šç”¨åç«¯é€»è¾‘ç†è§£è®°å¿†æœºåˆ¶ï¼ˆçŸ­æœŸ=ç¼“å­˜ã€é•¿æœŸ=æŒä¹…åŒ–å­˜å‚¨ï¼‰ã€ä»»åŠ¡è§„åˆ’ï¼ˆç±»æ¯”åˆ†å¸ƒå¼è°ƒåº¦ï¼‰ï¼›
3. å°é¡¹ç›®ç»ƒè½åœ°ï¼šä»â€œPDFè§£æ+é—®ç­”â€åˆ‡å…¥ï¼ŒåµŒå…¥é‡è¯•ã€æ—¥å¿—ç›‘æ§ç­‰åç«¯å®è·µï¼›
4. èåˆèƒ½åŠ›ï¼šå°†å¹¶å‘æ§åˆ¶ã€æƒé™ç®¡ç†ç­‰åç«¯æŠ€èƒ½ï¼Œè¿ç§»è‡³å¤šAgentåä½œåœºæ™¯ã€‚
åç«¯è½¬AI Agentçš„å…³é”®ï¼Œæ˜¯è®©å·¥ç¨‹ä¼˜åŠ¿ä¸AIè®¤çŸ¥å½¢æˆåˆåŠ›ã€‚é¿å¼€æ¦‚å¿µå †ç Œè¯¯åŒºï¼Œæ‰èƒ½çœŸæ­£å®ç°é«˜æ•ˆè½¬å²—ã€‚


å¾ˆå¤šäººä»¥ä¸ºä¼šè°ƒä¸ªOpenAIçš„æ¥å£ã€è·‘é€šDemoå°±æ˜¯AIå¼€å‘äº†ï¼Œå®é™…ä¸Šè¿AIå¼€å‘çš„é—¨æ§›éƒ½æ²¡æ‘¸åˆ°

æ€»ç»“ 4 ç±»å…¸å‹â€œé€å‘½é¢˜â€

1. æ¦‚å¿µèƒŒå¾—æ»šç“œçƒ‚ç†Ÿï¼Œè½åœ°ç»†èŠ‚ä¸€é—®ä¸‰ä¸çŸ¥

æ ¸å¿ƒç—›ç‚¹ï¼šæŠŠLangChainæ–‡æ¡£èƒŒå¾—æŒºç†Ÿï¼ŒçœŸåˆ°äº†ä¸šåŠ¡åœºæ™¯å°±éœ²é¦…ã€‚

å…·ä½“åä¾‹ï¼šé—®ä»–RAGï¼Œè‡ªä¿¡è¯´â€œè¿™æˆ‘ç†Ÿâ€ã€‚æ·±é—®ä¸€å¥ï¼šâ€œä½ çš„æ–‡æ¡£åˆ‡ç‰‡ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿæ€ä¹ˆè§£å†³é•¿ä¸‹æ–‡æˆªæ–­ä¸¢å¤±è¯­ä¹‰çš„é—®é¢˜ï¼Ÿâ€ æ²‰é»˜..åªè·‘è¿‡å®˜æ–¹Demoï¼Œæ²¡å¤„ç†è¿‡çœŸå®è„æ•°æ®ã€‚

2. åªæœ‰Demoæ€ç»´ï¼Œæ²¡æœ‰å·¥ç¨‹æ€ç»´

æ ¸å¿ƒç—›ç‚¹ï¼šåšå‡ºæ¥çš„ä¸œè¥¿æ˜¯ä¸ªç©å…·ï¼Œæ ¹æœ¬æ²¡æ³•ä¸Šçº¿ã€‚

åä¾‹ï¼š åšçš„Agentæ¼”ç¤ºæ•ˆæœä¸é”™ï¼Œæˆ‘é—®ä»–ï¼šâ€œå¦‚æœå¹¶å‘ä¸Šæ¥æ€ä¹ˆæ§åˆ¶Tokenæˆæœ¬ï¼ŸAPIå“åº”è¶…æ—¶æ€ä¹ˆåšé™çº§ç†”æ–­ï¼Ÿæ¨¡å‹å‡ºç°å¹»è§‰æ€ä¹ˆåšåå¤„ç†éªŒè¯ï¼Ÿâ€ ä»–ä¸€è„¸èŒ«ç„¶ï¼Œä»¿ä½›è¿™äº›æ˜¯è¿ç»´çš„äº‹ã€‚ä½†åœ¨å¼€å‘é‡Œï¼Œç¨³å®šæ€§æ¯”Featureæ›´éš¾åšã€‚

3. æŠŠPrompt Engineeringå½“æˆâ€œè·Ÿæœºå™¨äººèŠå¤©â€

æ ¸å¿ƒç—›ç‚¹ï¼š ä»¥ä¸ºä¼šæ‰“å­—å°±ä¼šå†™Promptï¼Œå®Œå…¨ä¸æ‡‚ç»“æ„åŒ–æŒ‡ä»¤ã€‚

åä¾‹ï¼šè®¾è®¡å¤„ç†å¤æ‚ä»»åŠ¡çš„Promptï¼Œä»–ç”¨å¤§ç™½è¯è·ŸAIè°ˆå¿ƒï¼Œä¸æ‡‚ä»€ä¹ˆæ˜¯CoTã€Few-Shotï¼Œæ›´ä¸æ‡‚æ€ä¹ˆçº¦æŸè¾“å‡ºæ ¼å¼ã€‚ç»“æœå°±æ˜¯æ¨¡å‹è¾“å‡ºæä¸ç¨³å®šï¼Œè¿˜è¦é åç«¯ä»£ç å†™ä¸€å †æ­£åˆ™å»ä¿®è¡¥ã€‚

4. æ‹¿ç€é”¤å­æ‰¾é’‰å­ï¼Œä¸ºäº†AIè€ŒAI

æ ¸å¿ƒç—›ç‚¹ï¼šæŠ€æœ¯é€‰å‹ç”±äºç¼ºä¹åœºæ™¯æ„Ÿï¼Œå¯¼è‡´æ–¹æ¡ˆæå…¶ä½æ•ˆã€‚

åä¾‹ï¼šå­—ç¬¦ä¸²æå–ï¼Œæ˜æ˜å†™ä¸ªæ­£åˆ™è¡¨è¾¾å¼0.01ç§’å°±èƒ½æå®šï¼Œä»–éè¦è°ƒç”¨GPT-4å»å¤„ç†ï¼Œä»–è¯´â€œè¿™æ ·æ›´æ™ºèƒ½â€ã€‚è¿™ä¸å«æ™ºèƒ½ï¼Œè¿™å«çƒ§é’±ä¸”ä½æ•ˆã€‚

é¢è¯•å®˜çš„çœŸå®æ„Ÿæ…¨

ç°åœ¨çš„å¸‚åœºä¸Šä¸ç¼ºä¼šè°ƒç”¨APIçš„ç¨‹åºå‘˜ï¼Œç¼ºçš„æ˜¯çœŸæ­£ç†è§£å¤§æ¨¡å‹è¾¹ç•Œã€èƒ½æŠŠâ€˜æ¦‚ç‡æ€§æ¨¡å‹â€™é©¯æœæˆâ€˜ç¡®å®šæ€§äº§å“â€™çš„å·¥ç¨‹å¸ˆ

è½¬å‹äººåŠ©åŠ›æŒ‰è¿™ä¸ªæ¢¯åº¦å»æ¶è¡¥ï¼š

1ã€è„±ç¦»APIè°ƒåŒ…ä¾ 

åˆ«å…‰çœ‹LangChainï¼Œå»æ‰‹å†™ä¸€ä¸ªç®€å•çš„RAGæµç¨‹ã€‚ææ‡‚Embeddingã€å‘é‡æ•°æ®åº“åˆ°åº•æ˜¯æ€ä¹ˆå­˜ã€æ€ä¹ˆæŸ¥çš„ã€‚

2ã€ç²¾é€šæç¤ºè¯å·¥ç¨‹

æŒæ¡CoTã€ReActæ¡†æ¶ï¼Œå­¦ä¼šè®©æ¨¡å‹æŒ‰ä½ çš„é€»è¾‘å»æ€è€ƒï¼Œç”¨Promptè§£å†³æ¨¡å‹å˜ç¬¨çš„é—®é¢˜ã€‚

3ã€æŒæ¡Agentç¼–æ’æ¶æ„

å­¦ä¹ Multi-Agentåä½œï¼Œç†è§£è§„åˆ’ã€è®°å¿†ã€å·¥å…·ä½¿ç”¨è¿™ä¸‰å¤§ä»¶æ€ä¹ˆåœ¨ä»£ç å±‚æµè½¬ã€‚

4ã€æ­»ç£•è½åœ°ä¸è¯„æµ‹

é«˜è–ªåˆ†æ°´å²­ã€‚å»ºç«‹ä¸€å¥—è‡ªåŠ¨åŒ–è¯„æµ‹é›†ï¼Œå…³æ³¨Tracingï¼Œæ‡‚å¾—åœ¨æ¨¡å‹æ™ºå•†å’Œæ¨ç†æˆæœ¬ä¹‹é—´åšTrade-offã€‚

### é‡åŒ–

[æç®€æ•™ç¨‹ï¼Œå¤§æ¨¡å‹é‡åŒ–å®è·µï¼Œ1å¼ 4090è·‘QwQï¼Ÿ](https://mp.weixin.qq.com/s/27EsyfQNXk_A73I8FMz61A)

é‡åŒ–æ˜¯ä¸€ç§å°†æ¨¡å‹çš„æµ®ç‚¹æƒé‡ï¼ˆé€šå¸¸æ˜¯ 32 ä½æˆ– 16 ä½ï¼‰è½¬æ¢ä¸ºä½ä½æ•´æ•°ï¼ˆå¦‚ 2 ä½ã€4 ä½ã€8 ä½ç­‰ï¼‰çš„æŠ€æœ¯ï¼Œç›®çš„æ˜¯å‡å°‘æ¨¡å‹çš„å­˜å‚¨ç©ºé—´å’Œè®¡ç®—èµ„æºéœ€æ±‚ï¼ŒåŒæ—¶å°½å¯èƒ½ä¿æŒæ¨¡å‹çš„æ€§èƒ½ã€‚

æœ€ä¸»æµçš„æœ‰ä»¥ä¸‹é‡åŒ–çš„æ–¹æ³•ï¼š

| æ–¹æ³•ç±»å‹  | ä»£è¡¨æŠ€æœ¯      | æ ¸å¿ƒç‰¹å¾              | é€‚ç”¨åœºæ™¯      |
|-------|-----------|-------------------|-----------|
| è®­ç»ƒåé‡åŒ– | GPTQ      | 4bit æƒé‡é‡åŒ–ï¼ŒåŠ¨æ€åé‡åŒ–æ¨ç† | GPU åŠ é€Ÿæ¨ç†  |
| æ„ŸçŸ¥é‡åŒ–  | AWQ       | æ¿€æ´»å€¼å¼•å¯¼çš„æ™ºèƒ½é‡åŒ–        | ç²¾åº¦æ•æ„Ÿå‹ä»»åŠ¡   |
| æ··åˆæ¨ç†  | GGUF/GGML | CPU-GPU å¼‚æ„è®¡ç®—æ¡†æ¶    | è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²    |

ç„¶åè¿™é‡Œè¿˜è®¾è®¡ä¸åŒçš„é‡åŒ–ä½å®½ï¼Œå…¶å®å¸¸è§ä¹Ÿå°±K-Quants å¢å¼ºç³»åˆ—

| é‡åŒ–ç±»å‹ | ä½å®½ç­–ç•¥               | å­˜å‚¨æ•ˆç‡   | æŠ€æœ¯ç‰¹ç‚¹      |
|------|--------------------|--------|-----------|
| Q2_K | 2.56bit/æƒé‡         | è¶…é«˜å‹ç¼©ç‡  | 16 å—è¶…å—ç»“æ„  |
| Q3_K | 3.44bit/æƒé‡ï¼ˆtype-0ï¼‰ | æ€§èƒ½ä¼˜å…ˆ   | å¹³è¡¡å‹ç¼©ä¸ç²¾åº¦   |
| Q4_K | 4.5bit/æƒé‡ï¼ˆtype-1ï¼‰  | é€šç”¨å‹    | ä¸»æµéƒ¨ç½²æ–¹æ¡ˆ    |
| Q5_K | 5.5bit/æƒé‡          | ç²¾åº¦å¢å¼º   | å…³é”®å±‚ä¿æŠ¤     |
| Q6_K | 6.56bit/æƒé‡         | å‡†æ— æŸå‹ç¼©  | å¤æ‚ä»»åŠ¡ä¿ç•™    |
| Q8_K | 8bit ä¸­é—´ç»“æœé‡åŒ–        | èµ„æºå……è¶³åœºæ™¯ | æ¢¯åº¦è®¡ç®—ä¼˜åŒ–    |

è¿˜æœ‰å¸¸è§åˆ°çš„ IQ ç³»åˆ—é‡åŒ–æ–¹æ³•

- IQ4_NLï¼š4 ä½é‡åŒ–ï¼Œè¶…å—åŒ…å« 256 ä¸ªæƒé‡ï¼Œæƒé‡ w é€šè¿‡ super_block_scale å’Œ importance matrix è®¡ç®—å¾—åˆ°
- IQ4_XSï¼š4 ä½é‡åŒ–ï¼Œè¶…å—åŒ…å« 256 ä¸ªæƒé‡ï¼Œæ¯ä¸ªæƒé‡å ç”¨ 4.25 ä½ï¼Œé€šè¿‡ super_block_scale å’Œ importance matrix è®¡ç®—å¾—åˆ°

ç›®å‰æœ€æµè¡Œçš„æ˜¯æ··åˆé‡åŒ–ç­–ç•¥ï¼Œä¸»æ‰“ä¸€ä¸ªåŠ¨æ€ç²¾åº¦åˆ†é…

- K_M æ··åˆç­–ç•¥ï¼šå¯¹ attention.wv ç­‰å…³é”®å¼ é‡é‡‡ç”¨ Q6_Kï¼Œå…¶ä½™ä½¿ç”¨ Q4_K
- K_S å‡è´¨ç­–ç•¥ï¼šå…¨æ¨¡å‹ç»Ÿä¸€é‡åŒ–é…ç½®ï¼Œæ‰€æœ‰å¼ é‡å‡ä½¿ç”¨ Q4_K

å…¶ä¸­

- Kï¼šè¡¨ç¤º k-quants é‡åŒ–æ–¹æ³•
- S (Small)ï¼šç®€å•é‡åŒ–ï¼Œæ‰€æœ‰å¼ é‡å‡ä½¿ç”¨ç›¸åŒä½æ•°é‡åŒ–
- M (Mixed)ï¼šæ··åˆé‡åŒ–ï¼Œå¯¹å…³é”®å¼ é‡ä½¿ç”¨æ›´é«˜ç²¾åº¦çš„é‡åŒ–ï¼Œå…¶ä½™ä½¿ç”¨æ ‡å‡†ç²¾åº¦

åœ¨ç›¸åŒä½æ•°ä¸‹ï¼ŒK_M ç³»åˆ—æ¨¡å‹ï¼ˆæ¯”å¦‚ä»Šå¤©æˆ‘ä»¬è¦æ¼”ç¤ºçš„ QwQ-32B-Q4_K_Mï¼‰é€šå¸¸åœ¨æ¨¡å‹å¤§å°å’Œæ€§èƒ½ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ï¼Œæ˜¯æ¨èçš„é€‰æ‹©ã€‚

### å¸¸ç”¨æ•°æ®ç±»å‹

[å¤§æ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼ˆQuantizationï¼‰å¯è§†åŒ–æŒ‡å—](https://mp.weixin.qq.com/s/L162LDMXXzAlTQhEcuOiMw)

é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹æ¯”åˆ†æå¸¸è§„æ•°æ®ç±»å‹ä¸32ä½ï¼ˆå³ å…¨ç²¾åº¦ æˆ– FP32ï¼‰è¡¨ç¤ºæ–¹å¼çš„å·®å¼‚ï¼š

- FP32 æµ®ç‚¹æ ¼å¼ å…¨ç²¾åº¦
- FP16 æµ®ç‚¹æ ¼å¼ åŠç²¾åº¦
- BF16ï¼šè™½ç„¶ä¸FP16å ç”¨ç›¸åŒçš„å­˜å‚¨ç©ºé—´ï¼ˆ16ä½ï¼‰ï¼Œä½†å…¶æ•°å€¼è¡¨ç¤ºèŒƒå›´æ›´å¹¿ï¼Œå› æ­¤è¢«å¹¿æ³›åº”ç”¨äºæ·±åº¦å­¦ä¹ é¢†åŸŸã€‚
- INT8ï¼šå½“æˆ‘ä»¬å°†æ¯”ç‰¹ä½æ•°è¿›ä¸€æ­¥é™ä½æ—¶ï¼Œä¾¿ä¼šè¿›å…¥åŸºäºæ•´æ•°çš„è¡¨ç¤ºæ–¹æ³•èŒƒç•´ï¼Œè€Œä¸å†ä½¿ç”¨æµ®ç‚¹è¡¨ç¤ºæ³•ã€‚ä»¥FP32æµ®ç‚¹æ ¼å¼è½¬æ¢ä¸º8æ¯”ç‰¹çš„INT8ä¸ºä¾‹ï¼Œå…¶æ¯”ç‰¹ä½æ•°å°†ç¼©å‡è‡³åŸå§‹å€¼çš„å››åˆ†ä¹‹ä¸€

å…·ä½“ç¡¬ä»¶æ¡ä»¶ä¸‹ï¼ŒåŸºäºæ•´æ•°çš„è¿ç®—é€Ÿåº¦å¯èƒ½ä¼˜äºæµ®ç‚¹è¿ç®—ï¼Œä½†è¿™ä¸€ä¼˜åŠ¿å¹¶éç»å¯¹æˆç«‹ã€‚ç„¶è€Œï¼Œé€šå¸¸å½“ä½¿ç”¨è¾ƒå°‘æ¯”ç‰¹ä½æ•°æ—¶ï¼Œè®¡ç®—é€Ÿåº¦ä¼šæ˜¾è‘—æå‡ã€‚

å®é™…ä¸Šï¼Œæˆ‘ä»¬æ— éœ€å°†å®Œæ•´çš„FP32èŒƒå›´[-3.4e38, 3.4e38]æ˜ å°„åˆ°INT8ï¼Œåªéœ€æ‰¾åˆ°å°†æ¨¡å‹å‚æ•°çš„å®é™…æ•°æ®èŒƒå›´é€‚é…åˆ°INT8çš„æ–¹æ³•å³å¯ã€‚

å¸¸è§çš„å‹ç¼©/æ˜ å°„æ–¹æ³•åŒ…æ‹¬ **å¯¹ç§°é‡åŒ–** å’Œ **éå¯¹ç§°é‡åŒ–** ï¼Œè¿™äº›éƒ½å±äº çº¿æ€§æ˜ å°„ çš„èŒƒç•´ã€‚

### å†…å­˜è®¡ç®—

è®¡ç®—æ¨¡å‹åœ¨ç»™å®šæ•°å€¼æ—¶æ‰€éœ€çš„å†…å­˜ç©ºé—´: memory = nr_bits/8 * nr_params

æ³¨æ„ï¼šå®é™…åº”ç”¨ä¸­ï¼Œæ¨ç†è¿‡ç¨‹æ‰€éœ€çš„(V)RAMå®¹é‡è¿˜éœ€è€ƒè™‘å…¶ä»–å› ç´ ï¼Œä¾‹å¦‚ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ¨¡å‹æ¶æ„è®¾è®¡ã€‚

å¤§å¤šæ•°æ¨¡å‹åŸç”Ÿé‡‡ç”¨32ä½æµ®ç‚¹æ•°ï¼ˆå¸¸ç§°ä¸º å…¨ç²¾åº¦ï¼‰è¡¨ç¤º, DeepSeekæ¨¡å‹æ˜¯ 16ä½æµ®ç‚¹æ•°æ¨¡å‹ï¼Œå¯¹äº70Bï¼ˆ700äº¿ï¼‰å‚æ•°çš„æ¨¡å‹ï¼ŒåŠ è½½æ¨¡å‹éœ€è¦å ç”¨140GBå†…å­˜ç©ºé—´ã€‚è®¡ç®—å…¬å¼ä¸º

fp16: (16/8)*70B ~= 140GB
fp16: (16/8)*671B = 1342 GB å†…å­˜
int8: (8/8)*671B = 671 GB å†…å­˜
int4: (4/8)*671B = 335.5 GB å†…å­˜

## å­¦ä¹ æ•™ç¨‹

Christopher M. Bishopçš„ã€Šæ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ ã€‹ï¼ˆPRML: Pattern Recognition and Machine Learningï¼‰åˆç‰ˆåœ¨2006å¹´çš„ï¼Œ2025å¹´è¢«é¦–æ¬¡å¼•è¿›å›½å†…

[Getting Started With MachineLearning (all in one)](https://pan.baidu.com/s/1tNXYQNadAsDGfPvuuj7_Tw)

[microsoft/ML-For-Beginners: 12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all](https://github.com/microsoft/ML-For-Beginners)

[microsoft/AI-For-Beginners: 12 Weeks, 24 Lessons, AI for All!](https://github.com/microsoft/AI-For-Beginners)

1. [ç¬¬ä¸€ç« ï¼šå…¨ä¸–ç•Œæœ€ç®€å•çš„æœºå™¨å­¦ä¹ å…¥é—¨æŒ‡å—](https://zhuanlan.zhihu.com/p/24339995)
2. [ç¬¬äºŒç« ï¼šç”¨æœºå™¨å­¦ä¹ åˆ¶ä½œè¶…çº§é©¬é‡Œå¥¥çš„å…³å¡](https://zhuanlan.zhihu.com/p/24344720)
3. [ç¬¬ä¸‰ç« :å›¾åƒè¯†åˆ«ã€é¸Ÿoré£æœºã€‘ï¼Ÿæ·±åº¦å­¦ä¹ ä¸å·ç§¯ç¥ç»ç½‘ç»œ] https://zhuanlan.zhihu.com/p/24524583
4. [ç¬¬å››ç« ï¼šç”¨æ·±åº¦å­¦ä¹ è¯†åˆ«äººè„¸](https://zhuanlan.zhihu.com/p/24567586)
5. [ç¬¬äº”ç« ï¼šGoogle ç¿»è¯‘èƒŒåçš„é»‘ç§‘æŠ€ï¼šç¥ç»ç½‘ç»œå’Œåºåˆ—åˆ°åºåˆ—å­¦ä¹ ](https://zhuanlan.zhihu.com/p/24590838)
6. [ç¬¬å…­ç« ï¼šå¦‚ä½•ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Ÿ](https://zhuanlan.zhihu.com/p/24703268)

### å¬å›ç‡å’Œç²¾ç¡®ç‡

[ç®—æ³•å·¥ç¨‹å¸ˆè¯´çš„å¬å›æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ - æ‹’æµ·ç©ºé—´](https://refusea.com/?p=1546)

åœ¨ç®—æ³•å·¥ç¨‹ä¸­ï¼Œ"å¬å›"æ˜¯ä¸€ä¸ªé‡è¦çš„æ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯æ£€ç´¢å’Œæœºå™¨å­¦ä¹ é¢†åŸŸã€‚

å¬å›ç‡ï¼ˆRecallï¼‰æ˜¯ä¸€ç§è¡¡é‡æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„æŒ‡æ ‡ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹è¯†åˆ«å‡ºç›¸å…³å®ä¾‹çš„èƒ½åŠ›ã€‚

å…·ä½“æ¥è¯´ï¼Œ**å¬å›ç‡æ˜¯æŒ‡æ¨¡å‹æ­£ç¡®è¯†åˆ«å‡ºçš„æ­£ä¾‹ï¼ˆçœŸæ­£ä¾‹ï¼‰å æ‰€æœ‰å®é™…æ­£ä¾‹ï¼ˆçœŸæ­£ä¾‹+å‡åä¾‹ï¼‰çš„æ¯”ä¾‹**ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯æ¨¡å‹æ‰¾åˆ°çš„ç›¸å…³å®ä¾‹å æ‰€æœ‰ç›¸å…³å®ä¾‹çš„æ¯”ä¾‹ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªç”¨äºæ£€æµ‹åƒåœ¾é‚®ä»¶çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå¬å›ç‡å°±æ˜¯æ¨¡å‹æ­£ç¡®æ ‡è®°ä¸ºåƒåœ¾é‚®ä»¶çš„é‚®ä»¶æ•°é‡å æ‰€æœ‰å®é™…åƒåœ¾é‚®ä»¶æ•°é‡çš„æ¯”ä¾‹ã€‚

å¬å›ç‡å’Œç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰é€šå¸¸ä¸€èµ·ä½¿ç”¨ï¼Œä»¥è·å¾—æ¨¡å‹æ€§èƒ½çš„å…¨é¢è§†å›¾ã€‚

**ç²¾ç¡®ç‡æ˜¯æ¨¡å‹é¢„æµ‹ä¸ºæ­£ä¾‹çš„å®ä¾‹ä¸­å®é™…ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹**ï¼Œè€Œå¬å›ç‡åˆ™å…³æ³¨æ¨¡å‹èƒ½æ‰¾åˆ°å¤šå°‘å®é™…çš„æ­£ä¾‹ã€‚

ä¾‹å­
å¦‚æœæœ‰ 1000 é‚®ä»¶éœ€è¦æ£€æµ‹ï¼Œç®—æ³•æ£€æµ‹å‡ºæœ‰ 800 åƒåœ¾é‚®ä»¶ï¼Œå®é™…è¿™ 800 é‡ŒçœŸæ­£çš„åƒåœ¾é‚®ä»¶æ˜¯ 600ï¼ŒåŒæ—¶ç®—æ³•è¿˜é—æ¼äº† 50 åƒåœ¾é‚®ä»¶ã€‚é‚£ä¹ˆå¬å›ç‡å’Œç²¾ç¡®ç‡æ˜¯å¤šå°‘ï¼Ÿæ€ä¹ˆè®¡ç®—çš„ï¼Ÿ

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆå®šä¹‰ä»¥ä¸‹å‡ ä¸ªæ¦‚å¿µï¼š

çœŸæ­£ä¾‹ï¼ˆTrue Positiveï¼ŒTPï¼‰ï¼šç®—æ³•æ­£ç¡®åœ°é¢„æµ‹ä¸ºåƒåœ¾é‚®ä»¶çš„é‚®ä»¶æ•°é‡ï¼Œå³600å°ã€‚
å‡æ­£ä¾‹ï¼ˆFalse Positiveï¼ŒFPï¼‰ï¼šç®—æ³•é”™è¯¯åœ°é¢„æµ‹ä¸ºåƒåœ¾é‚®ä»¶çš„é‚®ä»¶æ•°é‡ï¼Œå³800ï¼ˆç®—æ³•é¢„æµ‹ä¸ºåƒåœ¾é‚®ä»¶çš„æ•°é‡ï¼‰- 600ï¼ˆçœŸæ­£çš„åƒåœ¾é‚®ä»¶æ•°é‡ï¼‰= 200å°ã€‚
å‡åä¾‹ï¼ˆFalse Negativeï¼ŒFNï¼‰ï¼šç®—æ³•é”™è¯¯åœ°é¢„æµ‹ä¸ºéåƒåœ¾é‚®ä»¶çš„é‚®ä»¶æ•°é‡ï¼Œå³é—æ¼çš„åƒåœ¾é‚®ä»¶æ•°é‡ï¼Œå³50å°ã€‚
æ ¹æ®è¿™äº›å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å¬å›ç‡å’Œç²¾ç¡®ç‡ï¼š

å¬å›ç‡ï¼ˆRecallï¼‰= çœŸæ­£ä¾‹ / (çœŸæ­£ä¾‹ + å‡åä¾‹) = 600 / (600 + 50) = 0.923ï¼Œæˆ–è€…è¯´92.3%ã€‚
ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰= çœŸæ­£ä¾‹ / (çœŸæ­£ä¾‹ + å‡æ­£ä¾‹) = 600 / (600 + 200) = 0.75ï¼Œæˆ–è€…è¯´75%ã€‚
æ‰€ä»¥ï¼Œè¿™ä¸ªåƒåœ¾é‚®ä»¶æ£€æµ‹ç®—æ³•çš„å¬å›ç‡æ˜¯92.3%ï¼Œç²¾ç¡®ç‡æ˜¯75%ã€‚

### ç®€ä»‹

çº¿æ€§å›å½’ä¸»è¦ç”¨æ¥è§£å†³è¿ç»­å€¼é¢„æµ‹çš„é—®é¢˜ï¼Œé€»è¾‘å›å½’ç”¨æ¥è§£å†³åˆ†ç±»çš„é—®é¢˜ï¼Œè¾“å‡ºçš„å±äºæŸä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œå·¥ä¸šç•Œç»å¸¸ä¼šç”¨é€»è¾‘å›å½’æ¥åšæ’åº

## LLM ä»‹ç»

[Getting Started With Large Language Models - DZone Refcardz](https://dzone.com/refcardz/getting-started-with-large-language-models)

æ–‡å¿ƒå¤§æ¨¡å‹ERNIEæ˜¯ç™¾åº¦å‘å¸ƒçš„äº§ä¸šçº§çŸ¥è¯†å¢å¼ºå¤§æ¨¡å‹ï¼Œæ¶µç›–äº†NLPå¤§æ¨¡å‹å’Œè·¨æ¨¡æ€å¤§æ¨¡å‹ã€‚

https://github.com/PaddlePaddle/ERNIE

## AI Agent åŸºç¡€è®¾æ–½

[AI Agent åŸºç¡€è®¾æ–½](https://mp.weixin.qq.com/s/xp1f1BistZxy9rES3We3sA)

AI Agentæ˜¯åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯ä»¥å®ç°ç‰¹å®šç›®æ ‡å¹¶ä¸ºç”¨æˆ·å®Œæˆä»»åŠ¡çš„è½¯ä»¶ç³»ç»Ÿã€‚å®ƒä»¬å±•ç°å‡ºæ¨ç†ã€è§„åˆ’ã€è®°å¿†ä»¥åŠä¸€å®šç¨‹åº¦çš„è‡ªä¸»æ€§ï¼Œèƒ½å¤Ÿè¿›è¡Œå†³ç­–ã€å­¦ä¹ å’Œé€‚åº”ç¯å¢ƒ ã€‚è¿™äº›Agentèƒ½å¤ŸåŒæ—¶å¤„ç†åŒ…æ‹¬æ–‡æœ¬ã€è¯­éŸ³ã€è§†é¢‘ã€éŸ³é¢‘å’Œä»£ç åœ¨å†…çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶å…·å¤‡å¯¹è¯ã€æ¨ç†ã€å­¦ä¹ å’Œå†³ç­–çš„èƒ½åŠ› ã€‚

**AI Agentçš„æ ¸å¿ƒåŠŸèƒ½ç»„ä»¶**
AI Agentçš„å¼ºå¤§åŠŸèƒ½æºäºå…¶å†…éƒ¨å¤šä¸ªæ ¸å¿ƒç»„ä»¶çš„ååŒå·¥ä½œã€‚è¿™äº›ç»„ä»¶å…±åŒæ„æˆäº†Agentçš„æ„ŸçŸ¥ã€æ€è€ƒã€å†³ç­–å’Œè¡ŒåŠ¨èƒ½åŠ›ã€‚

1. å¤§è„‘ï¼šæ ¸å¿ƒLLMã€æ¨ç†ä¸è§„åˆ’
2. æ„ŸçŸ¥ä¸è¡ŒåŠ¨æ¨¡å—ï¼šä¸ç¯å¢ƒäº¤äº’
   1. ç¯å¢ƒæ„ŸçŸ¥æ¨¡å—ï¼šæ„ŸçŸ¥æ¨¡å—è´Ÿè½½æŠŠéœ€è¦çš„ä¸Šä¸‹æ–‡ã€ç¯å¢ƒä¿¡æ¯å¬å›ï¼Œä¼ é€’ç»™å¤§æ¨¡å‹ã€‚åœ¨æ„ŸçŸ¥æ¨¡å—ä¸­ï¼Œè¯­ä¹‰æœç´¢ã€NL2SQLç­‰èƒ½åŠ›æ˜¯åŸºç¡€ï¼Œè¿™äº›æ¨¡å—çš„èƒ½åŠ›æŠŠLLMæ„ŸçŸ¥ç¯å¢ƒçš„éœ€æ±‚è½¬æ¢ä¸ºå…·ä½“çš„è·å–æ•°æ®çš„æ“ä½œ
   2. è¡ŒåŠ¨æ¨¡å— ï¼šè´Ÿè´£æ‰§è¡ŒAgentçš„å†³ç­–ï¼Œè¿™å¯èƒ½åŒ…æ‹¬è°ƒç”¨APIã€ä¸å¤–éƒ¨å·¥å…·äº¤äº’ã€ç”Ÿæˆæ–‡æœ¬æˆ–ä»£ç ï¼Œç”šè‡³åœ¨æœºå™¨äººæŠ€æœ¯ä¸­æ‰§è¡Œç‰©ç†åŠ¨ä½œ
3. Memoryï¼šå­¦ä¹ ä¸ç»´æŠ¤ä¸Šä¸‹æ–‡
   1. çŸ­æœŸè®°å¿† (Short-Term Memory)
   2. é•¿æœŸè®°å¿† (Long-Term Memory)
   3. åˆ†å±‚è®°å¿†ç³»ç»Ÿï¼šå¦‚å·¥ä½œè®°å¿†ã€çŸ­æœŸè®°å¿†å’Œé•¿æœŸè®°å¿†çš„ç»„åˆï¼Œå¯ä»¥æé«˜æ£€ç´¢é€Ÿåº¦å’Œä¸Šä¸‹æ–‡ä¿çœŸåº¦ ã€‚
4. å·¥å…·é›†æˆä¸ä½¿ç”¨ï¼šæ‰©å±•Agentèƒ½åŠ›
   1. äº¤äº’åè®®å±‚: MCP, A2A
   2. å·¥å…·å‘ç°å’Œæ•´åˆ
   3. æ²™ç®±ï¼šæ²™ç®±æ˜¯ä¿éšœå·¥å…·å®‰å…¨è¿è¡Œçš„å‰æ
5. è·¯ç”±å™¨/æ§åˆ¶å™¨ï¼šç®¡ç†å¤æ‚å·¥ä½œæµ
   1. æ¶‰åŠå¤šä¸ªå·¥å…·æˆ–å­Agentçš„å¤šæ­¥éª¤ä»»åŠ¡æ—¶ï¼Œä¸€ä¸ªæœ‰æ•ˆçš„æ§åˆ¶å™¨å¯¹äºåè°ƒè¿™äº›ç»„ä»¶å˜å¾—è‡³å…³é‡è¦

**Agentç³»ç»Ÿè¿ç»´åŸºç¡€è®¾æ–½**
åŒ…æ‹¬LLM APIç½‘å…³ã€ç¼“å­˜ç­–ç•¥ä»¥åŠå®‰å…¨çš„å·¥å…·æ‰§è¡Œç¯å¢ƒã€‚

1. LLM APIç½‘å…³ï¼šç»Ÿä¸€è®¿é—®ã€å®‰å…¨ä¸å¯è§‚æµ‹æ€§
2. LLMå“åº”çš„ç¼“å­˜ç­–ç•¥ï¼šæ€§èƒ½ä¸æˆæœ¬ä¼˜åŒ–
   1. ç²¾ç¡®é”®ç¼“å­˜ (Exact Key Caching)
   2. è¯­ä¹‰ç¼“å­˜ (Semantic Caching)
3. å®‰å…¨çš„å·¥å…·æ‰§è¡Œç¯å¢ƒï¼šæ²™ç®±ä¸å‡­è¯ç®¡ç†

**Agentç¼–æ’ä¸åä½œ**
Agentçš„æ™ºèƒ½ä¸ä»…ä»…ä½“ç°åœ¨å…¶ä¸ªä½“èƒ½åŠ›ä¸Šï¼Œæ›´åœ¨äºå®ƒä»¬å¦‚ä½•ç»„ç»‡è‡ªå·±çš„â€œæ€ç»´â€è¿‡ç¨‹ä»¥åŠå¦‚ä½•ä¸å…¶ä»–Agentæˆ–ç³»ç»Ÿè¿›è¡Œåä½œã€‚

- æ€ç»´é“¾ (Chain-of-Thought, CoT)ï¼šå°†ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„æ­¥éª¤ï¼Œä»¥é€æ­¥æ±‚è§£ã€‚éå¸¸é€‚åˆéœ€è¦é€»è¾‘æˆ–å¤šæ­¥éª¤æ¨ç†çš„ä»»åŠ¡ ã€‚å®ƒå¸®åŠ©æ¨¡å‹åˆ†è§£ä»»åŠ¡ï¼Œä½¿å…¶æ€è€ƒè¿‡ç¨‹æ›´æ˜“äºç†è§£ ã€‚
- ReAct (Reasoning and Acting, æ¨ç†ä¸è¡ŒåŠ¨)ï¼šå°†CoTæ¨ç†ä¸å¤–éƒ¨å·¥å…·ä½¿ç”¨ç›¸ç»“åˆ ã€‚å®ƒæ¶‰åŠä¸€ä¸ªâ€œæ€è€ƒ (Thought) -> è¡ŒåŠ¨ (Action) -> è§‚å¯Ÿ (Observation)â€çš„å¾ªç¯ ã€‚è¿™ä½¿å¾—Agentèƒ½å¤Ÿæ ¹æ®æ–°ä¿¡æ¯æˆ–å‰ä¸€æ­¥éª¤çš„ç»“æœåŠ¨æ€è°ƒæ•´å…¶æ–¹æ³• ï¼Œä»è€Œå¢å¼ºLLMåœ¨Agentå·¥ä½œæµä¸­å¤„ç†å¤æ‚ä»»åŠ¡å’Œå†³ç­–çš„èƒ½åŠ› ã€‚
- Reflexion (åæ€)ï¼šåˆ©ç”¨åé¦ˆå¾ªç¯ï¼Œä½¿LLMèƒ½å¤Ÿåæ€è¿‡å»çš„è¾“å‡ºå¹¶è¿­ä»£åœ°æ”¹è¿›å…¶æ€§èƒ½ ã€‚è¿™ç§æ¨¡å¼éå¸¸é€‚ç”¨äºéœ€è¦å¤šæ¬¡å°è¯•è¿›è¡Œä¼˜åŒ–å’Œå¤æ‚æ¨ç†çš„ä»»åŠ¡

**å¼€æºAgentæ¡†æ¶**
å¼€æºAgentæ¡†æ¶æ—¨åœ¨é€šè¿‡æä¾›é¢„æ„å»ºçš„ç»„ä»¶å’ŒæŠ½è±¡æ¥ç®€åŒ–Agentçš„å¼€å‘è¿‡ç¨‹

- LangChainï¼šé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œé€‚ç”¨äºå…·æœ‰ç›´æ¥å·¥ä½œæµçš„ç®€å•Agentã€‚å®ƒæ”¯æŒå‘é‡æ•°æ®åº“å’Œè®°å¿†åŠŸèƒ½ï¼Œå…¶LangSmithå¹³å°å¯ç”¨äºè°ƒè¯•å’Œç›‘æ§ ã€‚ç„¶è€Œï¼Œä¸€äº›å¼€å‘è€…è®¤ä¸ºå®ƒè¿‡åº¦æŠ½è±¡ï¼Œéš¾ä»¥ä½¿ç”¨ï¼Œç”šè‡³æœ‰äº›è¿‡åº¦å·¥ç¨‹åŒ– ã€‚
- LangGraphï¼šä½œä¸ºLangChainç”Ÿæ€ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ï¼ŒLangGraphé‡‡ç”¨å›¾æ¶æ„ï¼ˆèŠ‚ç‚¹ä»£è¡¨ä»»åŠ¡/åŠ¨ä½œï¼Œè¾¹ä»£è¡¨è½¬æ¢ï¼‰ï¼Œå¹¶åŒ…å«ä¸€ä¸ªçŠ¶æ€ç»„ä»¶æ¥ç»´æŠ¤ä»»åŠ¡åˆ—è¡¨ã€‚å®ƒéå¸¸é€‚åˆå‘¨æœŸæ€§ã€æ¡ä»¶æ€§æˆ–éçº¿æ€§å·¥ä½œæµ ã€‚LangGraphæä¾›äº†æ¯”å…¶ä»–æ¡†æ¶æ›´é«˜çš„å¯æ§æ€§ï¼Œå¹¶æœ‰æ„ä¿æŒå…¶åº•å±‚å’Œé›†æˆæ— å…³æ€§ ã€‚
- AutoGenï¼šç”±å¾®è½¯æ¨å‡ºçš„å¤šAgent AIæ¡†æ¶ï¼Œé‡‡ç”¨åˆ†å±‚æ¶æ„ï¼ˆæ ¸å¿ƒå±‚ã€AgentChatå±‚ã€æ‰©å±•å±‚ï¼‰ã€‚å®ƒæ”¯æŒå¼‚æ­¥æ¶ˆæ¯ä¼ é€’å’Œå¯¹è¯å¼AIåŠ©æ‰‹çš„æ„å»ºï¼Œå¹¶æä¾›AutoGen Benchå’ŒAutoGen Studioç”¨äºå¼€å‘å’ŒåŸºå‡†æµ‹è¯• ã€‚
- CrewAIï¼šä¸€ä¸ªç”¨äºå¤šAgentè§£å†³æ–¹æ¡ˆçš„ç¼–æ’æ¡†æ¶ï¼Œé‡‡ç”¨åŸºäºè§’è‰²çš„æ¶æ„ï¼ˆAgentã€ä»»åŠ¡ã€æµç¨‹â€”â€”é¡ºåºæˆ–åˆ†å±‚ï¼‰ã€‚å®ƒåº•å±‚ä½¿ç”¨äº†LangChain ï¼Œä½†æœ¬èº«æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ¡†æ¶ ã€‚å…¶å±€é™æ€§åœ¨äºç›®å‰ä¸»è¦æ”¯æŒé¡ºåºç¼–æ’ï¼Œå¹¶ä¸”å¯èƒ½äº§ç”Ÿä¸å®Œæ•´çš„è¾“å‡º ã€‚

## ollama

[ollama/ollama: Get up and running with large language models.](https://github.com/ollama/ollama)
[ollama/ollama - Docker Image | Docker Hub](https://hub.docker.com/r/ollama/ollama)
[ollama Importing a model](https://github.com/ollama/ollama/blob/main/docs/import.md)
[ollama api](https://github.com/ollama/ollama/blob/main/docs/api.md)

```sh
# è®¾ç½®å¥½ç¯å¢ƒå˜é‡åï¼Œè¿è¡Œ ollama run å‘½ä»¤å³å¯è®© Ollama ä½¿ç”¨æŒ‡å®šçš„ GPU
export CUDA_VISIBLE_DEVICES=0,1
# Docker ä½¿ç”¨æ‰€æœ‰gpu --gpus=all
# Docker åªä½¿ç”¨ç¬¬ 0 å’Œç¬¬ 1 å¼  GPU å¡ --gpus=0,1
docker run -d --env OLLAMA_HOST=0.0.0.0:11434 -v /data/docker/ollama/ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
alias ollama='docker exec -it ollama ollama'

# Show model information
ollama show llama3.2
# List models on your computer
ollama list
# List which models are currently loaded
ollama ps
# Start Ollama
ollama serve

# Run a model
ollama run modelName
# Show more info: duration and eval rate
ollama run modelName --verbose
# Stop a running model
ollama stop modelName
# Remove a model
ollama rm modelName

#
# Run with parameter: num_ctx Context Window
ollama run llama3 â€“set parameter num_ctx 4096 --temperature 0.7 --top-p 0.9 --memory-limit 8GB --batch-size 8
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
ollama run llama2 --debug
# ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·
ollama run llama2 --profile

# ä½¿ç”¨curlæµ‹è¯•API
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello, how are you?"
}'

curl http://localhost:11434/api/tags

# æ‰¹å¤„ç†
ollama run llama2 < batch_prompts.txt > responses.txt

# [ollama Model library](https://ollama.com/library)

# deepseek
# deepseek 1.5b ~ 32b ä¸Šä¸‹æ–‡é•¿åº¦ 32,768  æœ€å¤§è¾“å…¥ 32,768
# [DeepSeek R1å’ŒDeepSeek V3 API_å¤§æ¨¡å‹æœåŠ¡å¹³å°ç™¾ç‚¼(Model Studio)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/model-studio/developer-reference/deepseek#94082c580cot9)
# [deepseek-r1:1.5b](https://ollama.com/library/deepseek-r1:1.5b)
# DeepSeek-R1-Distill-Qwen-1.5B
ollama run deepseek-r1:1.5b
# DeepSeek-R1-Distill-Qwen-7B
ollama run deepseek-r1:7b
# DeepSeek-R1-Distill-Qwen-32B
ollama run deepseek-r1:32b

# DeepSeek-R1-Distill-Llama-70B
# Error: model requires more system memory (37.3 GiB) than is available (24.7 GiB)
ollama run deepseek-r1:70b

# 34b æ‰§è¡Œå¤ªæ…¢
ollama run codellama:34b

# 70b Error: model requires more system memory (31.2 GiB) than is available (27.3 GiB)
ollama run codellama:70b

# Cline uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.5 Sonnet for its advanced agentic coding capabilities.
ollama run qwen2.5:32b

# [DeepSeek Coder](https://deepseekcoder.github.io/)
# [DeepSeek-Coder/Evaluation/HumanEval - deepseek-ai/DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder/tree/main/Evaluation/HumanEval)
#
# DeepSeek-Coder-V2 comes in two primary types: Instruct and Base.
# Base model
# A base model is a general-purpose language model trained on a large corpus of text (e.g., code, documentation, and natural language). It has no specific fine-tuning for instruction-following or task-oriented behaviour.
#
# Instruct model
# An instruct model is a fine-tuned version of a base model, optimized to follow instructions and perform specific tasks. It is trained on datasets containing instruction-response pairs (e.g., â€œWrite a SQL query to find duplicatesâ€ â†’ â€œSELECT â€¦â€). Excels at task-oriented interactions (e.g., debugging, refactoring, answering questions).

# [second-state-DeepSeek-Coder-V2-Lite-Instruct-GGUF: Mirror of https://huggingface.co/second-state/DeepSeek-Coder-V2-Lite-Instruct-GGUF](https://gitee.com/hf-models/second-state-DeepSeek-Coder-V2-Lite-Instruct-GGUF)
# DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf  Quant method: Q4_K_M
# medium, balanced quality - recommended
# DeepSeek-Coder-V2-Instruct 236B
# DeepSeek-Coder-V2-Lite-Instruct 16B
ollama run deepseek-coder-v2:16b-lite-instruct-q4_K_M

# [MFDoom/deepseek-r1-tool-calling:8b](https://ollama.com/MFDoom/deepseek-r1-tool-calling:8b)
# DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen. With Tool Calling support.
ollama run MFDoom/deepseek-r1-tool-calling:8b
```

### ollama å‘½ä»¤

[ollamaçš„å‘½ä»¤æ³¨è§£_ollamaå‘½ä»¤è¡Œ-CSDNåšå®¢](https://blog.csdn.net/sunyuhua_keyboard/article/details/141174683)

```sh
>>> /?
Available Commands:
  /set            Set session variables
  /show           Show model information
  /load <model>   Load a session or model
  /save <model>   Save your current session
  /clear          Clear session context
  /bye            Exit
  /?, /help       Help for a command
  /? shortcuts    Help for keyboard shortcuts

Use """ to begin a multi-line message.

>>> /set
Available Commands:
  /set parameter ...     Set a parameter
  /set system <string>   Set system message
  /set template <string> Set prompt template
  /set history           Enable history
  /set nohistory         Disable history
  /set wordwrap          Enable wordwrap
  /set nowordwrap        Disable wordwrap
  /set format json       Enable JSON mode
  /set noformat          Disable formatting
  /set verbose           Show LLM stats
  /set quiet             Disable LLM stats
```

ä¸»å‘½ä»¤
/set: ç”¨äºè®¾ç½®ä¼šè¯å‚æ•°å’Œé…ç½®ã€‚ä¾‹å¦‚ï¼Œè®¾ç½®æ¶ˆæ¯æ ¼å¼ã€å¯ç”¨æˆ–ç¦ç”¨å†å²è®°å½•ç­‰ã€‚
/show: æ˜¾ç¤ºæ¨¡å‹çš„ç›¸å…³ä¿¡æ¯ï¼Œå¦‚å½“å‰åŠ è½½çš„æ¨¡å‹çš„åç§°ã€ç‰ˆæœ¬ç­‰ã€‚
/load : åŠ è½½ä¸€ä¸ªç‰¹å®šçš„æ¨¡å‹æˆ–ä¼šè¯ã€‚ä½ å¯ä»¥æŒ‡å®šä¸€ä¸ªæ¨¡å‹çš„åç§°æˆ–è·¯å¾„æ¥åŠ è½½å®ƒã€‚
/save : ä¿å­˜å½“å‰çš„ä¼šè¯çŠ¶æ€æˆ–æ¨¡å‹ã€‚ä½ å¯ä»¥å°†å½“å‰ä¼šè¯æˆ–æ¨¡å‹çš„é…ç½®ä¿å­˜ä¸ºä¸€ä¸ªæ–‡ä»¶ï¼Œä»¥ä¾¿ä»¥åä½¿ç”¨ã€‚
/clear: æ¸…é™¤ä¼šè¯ä¸Šä¸‹æ–‡ã€‚è¿™å°†åˆ é™¤å½“å‰ä¼šè¯ä¸­çš„æ‰€æœ‰å†å²è®°å½•æˆ–å¯¹è¯å†…å®¹ã€‚
/bye: é€€å‡ºä¼šè¯ã€‚è¿™ä¸ªå‘½ä»¤å°†ç»“æŸå½“å‰ä¸æ¨¡å‹çš„å¯¹è¯ï¼Œå¹¶é€€å‡ºç¨‹åºã€‚
/? æˆ– /help: æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯ã€‚å¦‚æœä½ éœ€è¦å…³äºæŸä¸ªå‘½ä»¤çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥ä½¿ç”¨è¿™äº›å‘½ä»¤ã€‚
/? shortcuts: æ˜¾ç¤ºé”®ç›˜å¿«æ·é”®çš„å¸®åŠ©ä¿¡æ¯ã€‚è¿™å¯ä»¥å¸®åŠ©ä½ æ›´å¿«é€Ÿåœ°è¿›è¡Œæ“ä½œã€‚

/set å­å‘½ä»¤
/set parameter â€¦: è®¾ç½®æŸä¸ªå‚æ•°ã€‚è¿™å¯èƒ½åŒ…æ‹¬ä¸€äº›ç‰¹å®šçš„é…ç½®é¡¹ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºæˆ–è¾“å‡ºæ–¹å¼ã€‚
/set system : è®¾ç½®ç³»ç»Ÿæ¶ˆæ¯ã€‚ä½ å¯ä»¥æä¾›ä¸€ä¸ªå­—ç¬¦ä¸²ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯ï¼Œè¿™é€šå¸¸ç”¨äºåœ¨å¯¹è¯å¼€å§‹æ—¶å‘æ¨¡å‹ä¼ è¾¾èƒŒæ™¯ä¿¡æ¯æˆ–ç‰¹å®šæŒ‡ä»¤ã€‚
/set template : è®¾ç½®æç¤ºæ¨¡æ¿ã€‚è¿™å…è®¸ä½ å®šä¹‰ä¸€ä¸ªæ¨¡æ¿ï¼Œç”¨äºæ ¼å¼åŒ–ä½ ä¸æ¨¡å‹çš„å¯¹è¯ã€‚
/set history: å¯ç”¨å†å²è®°å½•ã€‚è¿™æ„å‘³ç€æ¨¡å‹ä¼šä¿å­˜ä½ å½“å‰ä¼šè¯ä¸­çš„å¯¹è¯å†å²ï¼Œä»¥ä¾¿ç¨åå‚è€ƒæˆ–ä½¿ç”¨ã€‚
/set nohistory: ç¦ç”¨å†å²è®°å½•ã€‚ä½¿ç”¨è¿™ä¸ªå‘½ä»¤åï¼Œæ¨¡å‹å°†ä¸ä¼šä¿å­˜ä¼šè¯å†å²ã€‚
/set wordwrap: å¯ç”¨è‡ªåŠ¨æ¢è¡Œã€‚è¿™åœ¨é•¿æ–‡æœ¬æ¶ˆæ¯çš„æƒ…å†µä¸‹éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥è®©æ–‡æœ¬è‡ªåŠ¨æ¢è¡Œä»¥ä¾¿äºé˜…è¯»ã€‚
/set nowordwrap: ç¦ç”¨è‡ªåŠ¨æ¢è¡Œã€‚å¦‚æœä¸éœ€è¦è‡ªåŠ¨æ¢è¡Œï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ã€‚
/set format json: å¯ç”¨JSONæ¨¡å¼æ ¼å¼åŒ–è¾“å‡ºã€‚è¿™ä¼šå°†æ¨¡å‹çš„å“åº”æ ¼å¼åŒ–ä¸ºJSONæ ¼å¼ï¼Œæ–¹ä¾¿ç»“æ„åŒ–æ•°æ®çš„å¤„ç†ã€‚
/set noformat: ç¦ç”¨æ ¼å¼åŒ–è¾“å‡ºã€‚å¦‚æœä¸éœ€è¦ä»»ä½•ç‰¹å®šæ ¼å¼çš„è¾“å‡ºï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ã€‚
/set verbose: å¯ç”¨è¯¦ç»†æ¨¡å¼ï¼Œè¿™ä¼šæ˜¾ç¤ºä¸LLMç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚å“åº”æ—¶é—´ã€æ¶ˆè€—èµ„æºç­‰ã€‚
/set quiet: ç¦ç”¨è¯¦ç»†æ¨¡å¼ã€‚å¯ç”¨åï¼Œå°†ä¸ä¼šæ˜¾ç¤ºä¸LLMç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè¾“å‡ºä¼šæ›´ç®€æ´ã€‚

åº”ç”¨åœºæ™¯

- ç®¡ç†ä¼šè¯: ä½ å¯ä»¥ä½¿ç”¨ /load å’Œ /save å‘½ä»¤æ¥ä¿å­˜å’ŒåŠ è½½ç‰¹å®šçš„ä¼šè¯çŠ¶æ€ï¼Œä»è€Œåœ¨ä¸åŒæ—¶é—´ç‚¹ç»§ç»­å…ˆå‰çš„å·¥ä½œã€‚
- è‡ªå®šä¹‰æ¶ˆæ¯æ ¼å¼: ä½¿ç”¨ /set template å’Œ /set format json å¯ä»¥è‡ªå®šä¹‰å’Œæ§åˆ¶æ¨¡å‹è¾“å‡ºçš„æ ¼å¼ï¼Œé€‚ç”¨äºä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚
- è°ƒè¯•å’Œæ€§èƒ½ç›‘æ§: é€šè¿‡ /set verbose å’Œ /set quietï¼Œä½ å¯ä»¥æ§åˆ¶æ˜¯å¦æŸ¥çœ‹æ¨¡å‹çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè¿™åœ¨è°ƒè¯•æˆ–æ€§èƒ½ç›‘æ§æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚

è¿™äº›å‘½ä»¤å’Œè®¾ç½®å¯ä»¥å¸®åŠ©ä½ æ›´çµæ´»åœ°æ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºå’Œä¼šè¯çš„ç®¡ç†ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ä½ çš„ä½¿ç”¨éœ€æ±‚ã€‚

### Where are models stored?

macOS: ~/.ollama/models
Linux: /usr/share/ollama/.ollama/models
Windows: %userprofile%\.ollama\models

é»˜è®¤çš„æ¨¡å‹ä¿å­˜è·¯å¾„ä½äºCç›˜ï¼Œï¼ˆ%userprofile%\.ollama\modelsï¼‰ï¼Œå¯ä»¥é€šè¿‡è®¾ç½® OLLAMA_MODELS è¿›è¡Œä¿®æ”¹ï¼Œç„¶åé‡å¯ç»ˆç«¯ï¼Œé‡å¯ollamaæœåŠ¡ï¼ˆéœ€è¦å»çŠ¶æ€æ é‡Œå…³æ‰ç¨‹åºï¼‰

```bat
setx OLLAMA_MODELS "D:\ollama_model"
OLLAMA_MODELS=D:\workspace\ollama\models
```

### ollama Setting environment variables on Windows

[ollama/docs/faq](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-windows)

On Windows, Ollama inherits your user and system environment variables.

1. First Quit Ollama by clicking on it in the task bar.
2. Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for environment variables.
3. Click on Edit environment variables for your account.
4. Edit or create a new variable for your user account for OLLAMA_HOST, OLLAMA_MODELS, etc.
   1. OLLAMA_HOST=0.0.0.0:11434
5. Click OK/Apply to save.
6. Start the Ollama application from the Windows Start menu.

### keep a model loaded in memory or make it unload immediately?

The `keep_alive` parameter can be set to:

- a duration string (such as "10m" or "24h")
- a number in seconds (such as 3600)
- any negative number which will keep the model loaded in memory (e.g. -1 or "-1m")
- '0' which will unload the model immediately after generating a response

```sh
# to preload a model and leave it in memory use:
curl http://localhost:11434/api/generate -d '{"model": "deepseek-r1:7b", "keep_alive": -1}'

# To unload the model and free up memory use:
curl http://localhost:11434/api/generate -d '{"model": "MFDoom/deepseek-r1-tool-calling:8b", "keep_alive": 0}'
```

### config Ollama

#### context window size

[How to Increase Ollama Context Size: A Complete Step-by-Step Guide - Deep AI â€” Leading Generative AI-powered Solutions for Business](https://deepai.tn/glossary/ollama/how-increase-ollama-context-size/)
[GGUF specification](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)
huggingface ä¸Šæˆ–æ¨¡å‹çš„ config æ–‡ä»¶ä¸­è®°å½•çš„å¤§å°ï¼šsequence_len: 4096

By default, Ollama uses a context window size of 2048 tokens.

show the context size *really is* in the current model being run `ollama show (model name)`. In the ollama CLI, `/show info`

```sh
ollama show deepseek-r1:7b
# Model
#   architecture        qwen2
#   parameters          7.6B
#   context length      131072
#   embedding length    3584
#   quantization        Q4_K_M

# Parameters
#   stop    "<ï½œbeginâ–ofâ–sentenceï½œ>"
#   stop    "<ï½œendâ–ofâ–sentenceï½œ>"
#   stop    "<ï½œUserï½œ>"
#   stop    "<ï½œAssistantï½œ>"

# When using the API, specify the num_ctx parameter:
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "options": {
    "num_ctx": 4096
  }
}'

/set parameter num_ctx 131072
/save deepseek-r1:7b-ctx-128k

/set parameter num_ctx 65536
/save deepseek-r1:7b-ctx-64k
/set parameter num_ctx 32768
/save deepseek-r1:7b-ctx-32k
/set parameter num_ctx 16384
/save deepseek-r1:7b-ctx-16k
/set parameter num_ctx 8192
/save deepseek-r1:7b-ctx-8k
/set parameter num_ctx 6144
/save deepseek-r1:7b-ctx-6k
/set parameter num_ctx 4096
/save deepseek-r1:7b-ctx-4k
/set parameter num_ctx 3072
/save deepseek-r1:7b-ctx-3k
/set parameter num_ctx 2048
/save deepseek-r1:7b-ctx-2k


ollama ps
# NAME              ID              SIZE      PROCESSOR    UNTIL
# deepseek-r1:7b-ctx-128k    946b57ccb619    12 GB    100% CPU     4 minutes from now
# deepseek-r1:7b-ctx-64k    af0516d55326    13 GB    58%/42% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-64k    af0516d55326    8.4 GB    100% CPU     4 minutes from now  2025å¹´3æœˆ3æ—¥
# deepseek-r1:7b-ctx-32k    1b1debea5066    9.5 GB    39%/61% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-16k    3fc83f5dea49    7.2 GB    19%/81% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-8k    fdb679a1bd50    6.3 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-6k    edd7d65f4ea1    6.1 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-4k    96cae7f73d2b    6.0 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-3k    25f4b7c66be3    5.5 GB    100% GPU     4 minutes from now
# deepseek-r1:7b-ctx-2k    9be990020b49    5.4 GB    100% GPU     4 minutes from now
# deepseek-r1:7b    0a8c26691023    5.4 GB    100% GPU     4 minutes from now
```

### cline with local deepseek

æœ¬åœ°å…è®¸ deepseek 32b æ¨¡å‹ï¼Œcline è°ƒç”¨æ—¶æŠ¥é”™
[Cline is having trouble... Â· Issue #1094 Â· cline/cline](https://github.com/cline/cline/issues/1094)

```log
Cline uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.5 Sonnet for its advanced agentic coding capabilities.

```

## vLLM

A tool designed to run LLMs very efficiently, especially when serving many users at once.
[Ollama vs VLLM: Which Tool Handles AI Models Better? | by Naman Tripathi | Medium](https://medium.com/@naman1011/ollama-vs-vllm-which-tool-handles-ai-models-better-a93345b911e6)

[Welcome to vLLM â€” vLLM](https://docs.vllm.ai/en/stable/)
[aneeshjoy/vllm-windows: Docker compose to run vLLM on Windows](https://github.com/aneeshjoy/vllm-windows)
[vllm/vllm-openai Tags | Docker Hub](https://hub.docker.com/r/vllm/vllm-openai/tags)

[AutoAWQ â€” vLLM](https://docs.vllm.ai/en/latest/features/quantization/auto_awq.html) To create a new 4-bit quantized model, you can leverage AutoAWQ. Quantization reduces the modelâ€™s precision from BF16/FP16 to INT4 which effectively reduces the total model memory footprint. The main benefits are lower latency and memory usage.

To determine whether a given model is supported, you can check the config.json file inside the HF repository. If the "architectures" field contains a model architecture listed below, then it should be supported in theory. [Supported Models â€” vLLM](https://docs.vllm.ai/en/v0.6.5/models/supported_models.html)

### vllm api

```sh
# Test by accessing the /models endpoints
curl http://127.0.0.1:8003/v1/models
```

llm chat completion
[API Reference - OpenAI API](https://platform.openai.com/docs/api-reference/chat/create)

```sh
curl -X POST http://127.0.0.1:9997/v1/chat/completions \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "qwen2.5-instruct",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "What is the largest animal?"
        }
    ]
  }'
```

embeddings [API Reference - OpenAI API](https://platform.openai.com/docs/api-reference/embeddings)

```sh
curl -X 'POST' 'http://localhost:9997/v1/embeddings' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "bge-m3",
    "input": "Hello, world!"
  }'
```

rank

```sh
curl http://localhost:9997/v1/rerank \
  -H "Content-Type: application/json" \
  -d '{
  "model": "bge-reranker-v2-m3",
  "query": "Organic skincare products for sensitive skin",
  "documents": [
    "Eco-friendly kitchenware for modern homes",
    "Biodegradable cleaning supplies for eco-conscious consumers",
    "Organic cotton baby clothes for sensitive skin",
    "Natural organic skincare range for sensitive skin",
    "Tech gadgets for smart homes: 2024 edition",
    "Sustainable gardening tools and compost solutions",
    "Sensitive skin-friendly facial cleansers and toners",
    "Organic food wraps and storage solutions",
    "All-natural pet food for dogs with allergies",
    "Yoga mats made from recycled materials"
  ],
  "top_n": 3
}'

INFO 04-10 11:11:21 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000
INFO 04-10 11:11:21 [launcher.py:26] Available routes are:
INFO 04-10 11:11:21 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD
INFO 04-10 11:11:21 [launcher.py:34] Route: /docs, Methods: GET, HEAD
INFO 04-10 11:11:21 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 04-10 11:11:21 [launcher.py:34] Route: /redoc, Methods: GET, HEAD
INFO 04-10 11:11:21 [launcher.py:34] Route: /health, Methods: GET
INFO 04-10 11:11:21 [launcher.py:34] Route: /load, Methods: GET
INFO 04-10 11:11:21 [launcher.py:34] Route: /ping, Methods: POST, GET
INFO 04-10 11:11:21 [launcher.py:34] Route: /tokenize, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /detokenize, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/models, Methods: GET
INFO 04-10 11:11:21 [launcher.py:34] Route: /version, Methods: GET
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/chat/completions, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/completions, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/embeddings, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /pooling, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /score, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/score, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /rerank, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/rerank, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v2/rerank, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /invocations, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /metrics, Methods: GET
```

### vllm run model

```sh
docker pull vllm/vllm-openai:v0.8.3

# æŒ‡å®šç¨‹åºåªèƒ½ä½¿ç”¨ç¼–å·ä¸º 0 å’Œ 1 çš„ GPUã€‚è¿™å¯¹äºå¤š GPU ç³»ç»Ÿéå¸¸æœ‰ç”¨ï¼Œå¯ä»¥æ§åˆ¶ç¨‹åºä½¿ç”¨å“ªäº› GPUã€‚
export CUDA_VISIBLE_DEVICES=0,1

# Name or path of the huggingface model to use. Default: â€œfacebook/opt-125mâ€
--model

# deepseek_r1 think enable
--enable-reasoning --reasoning-parser deepseek_r1

--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]
# The model name(s) used in the API. If multiple names are provided, the server will respond to any of the provided names. The model name in the model field of a response will be the first name in this list. If not specified, the model name will be the same as the --model argument. Noted that this name(s) will also be used in model_name tag content of prometheus metrics, if multiple names provided, metrics tag will take the first one.

--gpu-memory-utilization <value>
# gpu-memory-utilization æ˜¯ç”¨äºè®¾ç½® GPU å†…å­˜åˆ©ç”¨ç‡çš„å‚æ•°ï¼Œ<value> æ˜¯ä¸€ä¸ªä»‹äº 0 åˆ° 1 ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼Œè¡¨ç¤º GPU å†…å­˜çš„ä½¿ç”¨æ¯”ä¾‹
```

```sh
# By default, vLLM downloads models from HuggingFace. If you would like to use models from ModelScope, set the environment variable VLLM_USE_MODELSCOPE before initializing the engine.
# [DeepSeek-R1 Â· modelscope](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1)
# [DeepSeek-R1-Distill-Qwen-1.5B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)
# default max-model-len: max_seq_len=32768
    # --env "HUGGING_FACE_HUB_TOKEN=hf_oo" \
    # --model mistralai/Mistral-7B-v0.1
docker run --runtime nvidia --gpus all \
    --detach \
    --env TZ=Asia/Shanghai \
    --name vllm \
    --restart always \
    --env VLLM_USE_MODELSCOPE=True \
    --volume //d/workspace/vllm/.cache/:/root/.cache/ \
    --volume //d/workspace/models/:/models/ \
    --publish 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:v0.7.2 \
    --model /models/DeepSeek-R1-Distill-Qwen-32B \
    --served-model-name deepseek-r1:1.5b \
    --max-model-len 15520 \
    --gpu-memory-utilization 0.9

# Deploy with docker on Linux:
docker run --runtime nvidia --gpus all \
    --name my_vllm_container \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
     --env "HUGGING_FACE_HUB_TOKEN=<secret>" \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model deepseek-ai/DeepSeek-R1

# Load and run the model:
vllm serve "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
# Load and run the model:
docker exec -it my_vllm_container bash -c "vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

# Test by accessing the /models endpoints
http://127.0.0.1:8000/v1/models

# Check throughput ( I am running on a RTX 3090 )
http://127.0.0.1:8000/metrics

# Call the server using curl:
curl -X POST "http://localhost:8000/v1/chat/completions" \
    -H "Content-Type: application/json" \
    --data '{
        "model": "deepseek-r1:1.5b",
        "messages": [
            {
                "role": "user",
                "content": "What is the capital of France?"
            }
        ]
    }'

# OpenAI Completions API with vLLM
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
    }'

# OpenAI Chat Completions API with vLLM
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"}
        ]
    }'
```

### vllm å‚æ•°

[å¼•æ“å‚æ•° â€” vLLM æ–‡æ¡£](https://docs.vllm.com.cn/en/latest/serving/engine_args.html)

```sh
# è¦ä½¿ç”¨çš„ huggingface æ¨¡å‹çš„åç§°æˆ–è·¯å¾„ã€‚ é»˜è®¤å€¼ï¼šâ€œfacebook/opt-125mâ€
--model

--api-key API_KEY

# å¯é€‰å€¼ï¼šauto, generate, embedding, embed, classify, score, reward, transcription
# æ¨¡å‹è¦ä½¿ç”¨çš„ä»»åŠ¡ã€‚å³ä½¿åŒä¸€ä¸ªæ¨¡å‹å¯ä»¥ç”¨äºå¤šä¸ªä»»åŠ¡ï¼Œæ¯ä¸ª vLLM å®ä¾‹ä¹Ÿåªæ”¯æŒä¸€ä¸ªä»»åŠ¡ã€‚å½“æ¨¡å‹åªæ”¯æŒä¸€ä¸ªä»»åŠ¡æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ "auto" æ¥é€‰æ‹©å®ƒï¼›å¦åˆ™ï¼Œæ‚¨å¿…é¡»æ˜ç¡®æŒ‡å®šè¦ä½¿ç”¨çš„ä»»åŠ¡ã€‚
# é»˜è®¤å€¼ï¼šâ€œautoâ€
--task

# é™åˆ¶ PyTorch å¯è§çš„ GPU è®¾å¤‡
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
# å°†å¼ é‡å¹¶è¡ŒåŒ–åˆ° 8 ä¸ª GPU ä¸Šã€‚è¿™ä¸ªè®¾ç½®å’Œä½ çš„ CUDA_VISIBLE_DEVICES å‚æ•°ç›¸ç¬¦ï¼Œè¦ç¡®è®¤æ¨¡å‹å¯ä»¥æ”¯æŒ 8-way çš„å¼ é‡å¹¶è¡Œ
--tensor-parallel-size 8

# ç”¨äºæ¨¡å‹æ‰§è¡Œå™¨çš„ GPU å†…å­˜ fractionï¼ŒèŒƒå›´å¯ä»¥ä» 0 åˆ° 1ã€‚ä¾‹å¦‚ï¼Œå€¼ä¸º 0.5 æ„å‘³ç€ 50% çš„ GPU å†…å­˜åˆ©ç”¨ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ 0.9ã€‚è¿™æ˜¯ä¸€ä¸ªæ¯ä¸ªå®ä¾‹çš„é™åˆ¶ï¼Œå¹¶ä¸”ä»…é€‚ç”¨äºå½“å‰çš„ vLLM å®ä¾‹ã€‚å¦‚æœæ‚¨åœ¨åŒä¸€ GPU ä¸Šè¿è¡Œå¦ä¸€ä¸ª vLLM å®ä¾‹ï¼Œåˆ™æ— å…³ç´§è¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨åœ¨åŒä¸€ GPU ä¸Šè¿è¡Œä¸¤ä¸ª vLLM å®ä¾‹ï¼Œæ‚¨å¯ä»¥å°†æ¯ä¸ªå®ä¾‹çš„ GPU å†…å­˜åˆ©ç”¨ç‡è®¾ç½®ä¸º 0.5ã€‚
# é»˜è®¤å€¼ï¼š0.9
# è®¾ç½®æ¯ä¸ª GPU æœ€å¤§çš„æ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹ä¸º 90%ã€‚å¦‚æœ GPU ä¸Šçš„æ˜¾å­˜å®¹é‡è¾ƒå¤§ï¼ˆä¾‹å¦‚ 24GB æˆ– 40GBï¼‰ï¼Œé€šå¸¸è®¾ç½®ä¸º 0.9 æ˜¯å®‰å…¨çš„ï¼Œä½†å¦‚æœæ˜¾å­˜è¾ƒå°ï¼Œæˆ–è€…ä½ æœ‰å¤šä¸ªè¿›ç¨‹åœ¨åŒæ—¶ä½¿ç”¨ GPUï¼Œå¯èƒ½ä¼šå¯¼è‡´ Out of Memory é”™è¯¯
# ç¡®ä¿æ¯ä¸ª GPU ä¸Šçš„æ˜¾å­˜è¶³å¤Ÿï¼Œå¹¶ä¸”æ²¡æœ‰å…¶ä»–è¿›ç¨‹å ç”¨æ˜¾å­˜ã€‚å¦‚æœé‡åˆ°å†…å­˜æº¢å‡ºï¼Œå¯ä»¥å°è¯•è°ƒæ•´è¯¥å€¼ï¼Œæˆ–è€…é€æ­¥å‡å°‘æ¯ä¸ª GPU çš„æ˜¾å­˜ä½¿ç”¨ã€‚
--gpu-memory-utilization 0.9

# åœ¨æ˜¾å­˜ä¸è¶³æ—¶ä¼šä½¿ç”¨ CPU æ‰©å±•å†…å­˜ï¼Œè®¾ç½®æ¯ GPU çš„ CPU offloading ç©ºé—´ï¼ˆGiBï¼‰ã€‚æ ¹æ®å¯ç”¨ç³»ç»Ÿå†…å­˜è®¾ç½®ï¼Œä¾‹å¦‚ 45GBã€‚
# è¦å¸è½½åˆ° CPU çš„ç©ºé—´ï¼ˆGiBï¼‰ï¼Œæ¯ä¸ª GPUã€‚é»˜è®¤ä¸º 0ï¼Œè¡¨ç¤ºä¸å¸è½½ã€‚ç›´è§‚åœ°çœ‹ï¼Œæ­¤å‚æ•°å¯ä»¥è¢«è§†ä¸ºå¢åŠ  GPU å†…å­˜å¤§å°çš„è™šæ‹Ÿæ–¹å¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ª 24 GB çš„ GPU å¹¶å°†å…¶è®¾ç½®ä¸º 10ï¼Œå®é™…ä¸Šæ‚¨å¯ä»¥å°†å…¶è§†ä¸º 34 GB çš„ GPUã€‚ç„¶åæ‚¨å¯ä»¥åŠ è½½ä¸€ä¸ª 13B æ¨¡å‹ä¸ BF16 æƒé‡ï¼Œè¿™è‡³å°‘éœ€è¦ 26GB çš„ GPU å†…å­˜ã€‚è¯·æ³¨æ„ï¼Œè¿™éœ€è¦å¿«é€Ÿçš„ CPU-GPU äº’è¿ï¼Œå› ä¸ºæ¨¡å‹çš„ä¸€éƒ¨åˆ†åœ¨æ¯ä¸ªæ¨¡å‹å‰å‘ä¼ é€’ä¸­ä» CPU å†…å­˜åŠ¨æ€åŠ è½½åˆ° GPU å†…å­˜ã€‚
# é»˜è®¤å€¼ï¼š0
--cpu-offload-gb 0

# æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†è‡ªåŠ¨ä»æ¨¡å‹é…ç½®ä¸­æ´¾ç”Ÿã€‚  "model_max_length": 16384
# æŒ‡å®šæ¨¡å‹æ”¯æŒçš„æœ€å¤§è¾“å…¥é•¿åº¦ä¸º 8192 tokensã€‚è¿™ä¸ªå€¼éœ€è¦ä¸ä½ çš„æ¨¡å‹å¤§å°ã€æ˜¾å­˜å’Œå¹¶è¡Œåº¦ç›¸åŒ¹é…ã€‚
--max-model-len 8192

# è¦ä½¿ç”¨çš„æ¨¡å‹å®ç°ã€‚å¯é€‰å€¼ï¼šauto, vllm, transformers é»˜è®¤å€¼ï¼šâ€œautoâ€
# â€œautoâ€ å°†å°è¯•ä½¿ç”¨ vLLM å®ç°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦‚æœ vLLM å®ç°ä¸å¯ç”¨ï¼Œåˆ™å›é€€åˆ° Transformers å®ç°ã€‚
# â€œvllmâ€ å°†ä½¿ç”¨ vLLM æ¨¡å‹å®ç°ã€‚
# â€œtransformersâ€ å°†ä½¿ç”¨ Transformers æ¨¡å‹å®ç°ã€‚
--model-impl

# ç”¨äºé‡åŒ–æƒé‡çš„æ–¹æ³•ã€‚å¦‚æœä¸º Noneï¼Œæˆ‘ä»¬é¦–å…ˆæ£€æŸ¥æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­çš„ quantization_config å±æ€§ã€‚å¦‚æœä¸º Noneï¼Œæˆ‘ä»¬å‡è®¾æ¨¡å‹æƒé‡æœªé‡åŒ–ï¼Œå¹¶ä½¿ç”¨ dtype æ¥ç¡®å®šæƒé‡çš„æ•°æ®ç±»å‹ã€‚
# å¯é€‰å€¼ï¼šaqlm, awq, deepspeedfp, tpu_int8, fp8, ptpc_fp8, fbgemm_fp8, modelopt, marlin, gguf, gptq_marlin_24, gptq_marlin, awq_marlin, gptq, compressed-tensors, bitsandbytes, qqq, hqq, experts_int8, neuron_quant, ipex, quark, moe_wna16, None
--quantization, -q


# ä½¿ç”¨è´Ÿè½½å‡è¡¡ï¼Œç¡®ä¿å®ƒèƒ½å¤Ÿåœ¨å¤šä¸ª GPU ä¹‹é—´åˆ†é…å·¥ä½œã€‚
--max-requests
--max-requests-per-gpu

# æ£€æŸ¥å†…å­˜ç®¡ç† å¦‚æœæ¨¡å‹å› ä¸ºå†…å­˜ä¸è¶³åªåœ¨ GPU 0 ä¸Šè¿è¡Œï¼Œå¯ä»¥å°è¯•è°ƒæ•´å†…å­˜åˆ†é…è®¾ç½®ï¼Œå¦‚ç¯å¢ƒå˜é‡ PYTORCH_CUDA_ALLOC_CONFã€‚æœ‰åŠ©äºå‡å°‘ CUDA å†…å­˜çš„ç¢ç‰‡åŒ–ï¼Œå…è®¸æ¨¡å‹æ›´æœ‰æ•ˆåœ°ä½¿ç”¨å¤šä¸ª GPUã€‚
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256

# å¯ç”¨å‰ç¼€ç¼“å­˜ã€‚
--enable-prefix-caching
# ä¿¡ä»»è¿œç¨‹ä»£ç 
--trust-remote-code

# ä½¿ç”¨ç‰ˆæœ¬1
VLLM_USE_V1=1
```

### vllm GGUF

[GGUF â€” vLLM](https://docs.vllm.ai/en/stable/features/quantization/gguf.html)

[gguf-split: split and merge gguf per batch of tensors by phymbert Â· Pull Request #6135 Â· ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp/pull/6135)
[llama.cpp/docs/docker.md at master Â· ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md)

```sh
# [llama.cpp gguf-split](https://github.com/ggml-org/llama.cpp/tree/b3785/examples/gguf-split)
# --merge
gguf-split --merge /tmp/ggml-out-q4_0-2-00001-of-00003.gguf /tmp/ggml-out-q4_0-2-merge.gguf

gguf_merge: /tmp/ggml-out-q4_0-2-00001-of-00003.gguf -> /tmp/ggml-out-q4_0-2-merge.gguf
gguf_merge: reading metadata /tmp/ggml-out-q4_0-2-00001-of-00003.gguf ...done
gguf_merge: reading metadata /tmp/ggml-out-q4_0-2-00002-of-00003.gguf ...done
gguf_merge: reading metadata /tmp/ggml-out-q4_0-2-00003-of-00003.gguf ...done
gguf_merge: writing tensors /tmp/ggml-out-q4_0-2-00001-of-00003.gguf ...done
gguf_merge: writing tensors /tmp/ggml-out-q4_0-2-00002-of-00003.gguf ...done
gguf_merge: writing tensors /tmp/ggml-out-q4_0-2-00003-of-00003.gguf ...done
gguf_merge: /tmp/ggml-out-q4_0-2-merge.gguf merged from 3 split with 325 tensors.
```

## llama.cpp

[ggml-org/llama.cpp: LLM inference in C/C++](https://github.com/ggml-org/llama.cpp)
[LLaMA.cpp HTTP Server README Â· ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md)

[Local AI performance variables table](https://martech.org/how-to-run-deepseek-locally-on-your-computer/)

llama-cli

```sh
# llama-cli
#
# A CLI tool for accessing and experimenting with most of llama.cpp's functionality.
# Run in conversation mode
# Models with a built-in chat template will automatically activate conversation mode. If this doesn't occur, you can manually enable it by adding -cnv and specifying a suitable chat template with --chat-template NAME
llama-cli -m model.gguf
#
# > hi, who are you?
# Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?
#
# > what is 1+1?
# Easy peasy! The answer to 1+1 is... 2!
```

llama-server

```sh
# llama-server
#
# A lightweight, OpenAI API compatible, HTTP server for serving LLMs.
# Start a local HTTP server with default configuration on port 8080
llama-server -m model.gguf --port 8080
#
# Basic web UI can be accessed via browser: http://localhost:8080
# Chat completion endpoint: http://localhost:8080/v1/chat/completions
Support multiple-users and parallel decoding
# up to 4 concurrent requests, each with 4096 max context
llama-server -m model.gguf -c 16384 -np 4

# Serve an embedding model
# use the /embedding endpoint
llama-server -m model.gguf --embedding --pooling cls -ub 8192
# Serve a reranking model
# use the /reranking endpoint
llama-server -m model.gguf --reranking
```

### llama docker

```sh
# ghcr.io/ggml-org/llama.cpp:full-cuda: Same as full but compiled with CUDA support. (platforms: linux/amd64)
$ docker run --gpus all -v /dsdata/modelscope/:/models ghcr.io/ggml-org/llama.cpp:full-cuda bash
Unknown command: bash
Available commands:
  --run (-r): Run a model previously converted into ggml
              ex: -m /models/7B/ggml-model-q4_0.bin -p "Building a website can be done in 10 simple steps:" -n 512
  --bench (-b): Benchmark the performance of the inference for various parameters.
              ex: -m model.gguf
  --perplexity (-p): Measure the perplexity of a model over a given text.
              ex: -m model.gguf -f file.txt
  --convert (-c): Convert a llama model into ggml
              ex: --outtype f16 "/models/7B/"
  --quantize (-q): Optimize with quantization process ggml
              ex: "/models/7B/ggml-model-f16.bin" "/models/7B/ggml-model-q4_0.bin" 2
  --all-in-one (-a): Execute --convert & --quantize
              ex: "/models/" 7B
  --server (-s): Run a model on the server
              ex: -m /models/7B/ggml-model-q4_0.bin -c 2048 -ngl 43 -mg 1 --port 8080
```

### Unsloth

[Run DeepSeek-R1 Dynamic 1.58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)
[Tutorial: How to Run DeepSeek-R1 Locally | Unsloth Documentation](https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/tutorial-how-to-run-deepseek-r1-locally)

```sh
# Example with Q4_0 K quantized cache Notice -no-cnv disables auto conversation mode
./llama.cpp/llama-cli \
    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --cache-type-k q4_0 \
    --threads 12 -no-cnv --prio 2 \
    --temp 0.6 \
    --ctx-size 8192 \
    --seed 3407 \
    --prompt "<ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>"

docker run -v /path/to/models:/models -p 8000:8000 ghcr.io/ggml-org/llama.cpp:server -m /models/7B/ggml-model-q4_0.gguf --port 8000 --host 0.0.0.0 -n 512
docker run --gpus all -v /path/to/models:/models local/llama.cpp:server-cuda -m /models/7B/ggml-model-q4_0.gguf --port 8000 --host 0.0.0.0 -n 512 --n-gpu-layers 1

# Deploy with docker on Linux:
docker run --runtime nvidia --gpus all \
    --detach \
    --restart always \
    --name llama \
    --env TZ=Asia/Shanghai \
    --env CUDA_VISIBLE_DEVICES=3,4 \
    --volume /dsdata/modelscope/:/models/ \
    --publish 8000:8000 \
    ghcr.io/ggml-org/llama.cpp:server \
    --port 8000 --host 0.0.0.0 -n 512 \
    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf
```

#### Chat Template Issues?

[Tutorial: How to Run DeepSeek-R1 Locally DeepSeek Chat Template | Unsloth Documentation](https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/tutorial-how-to-run-deepseek-r1-locally#deepseek-chat-template)

```sh
print_info: BOS token        = 0 '<ï½œbeginâ–ofâ–sentenceï½œ>'
print_info: EOS token        = 1 '<ï½œendâ–ofâ–sentenceï½œ>'
print_info: EOT token        = 1 '<ï½œendâ–ofâ–sentenceï½œ>'
print_info: PAD token        = 128815 '<ï½œPADâ–TOKENï½œ>'
print_info: LF token         = 201 'ÄŠ'
print_info: FIM PRE token    = 128801 '<ï½œfimâ–beginï½œ>'
print_info: FIM SUF token    = 128800 '<ï½œfimâ–holeï½œ>'
print_info: FIM MID token    = 128802 '<ï½œfimâ–endï½œ>'
print_info: EOG token        = 1 '<ï½œendâ–ofâ–sentenceï½œ>'
```

All distilled versions and the main 671B R1 model use the same chat template:

```sh
<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>It's 2.<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>Explain more!<ï½œAssistantï½œ>
```

A BOS is forcibly added, and an EOS separates each interaction. To counteract double BOS tokens during inference, you should only call tokenizer.encode(..., add_special_tokens = False) since the chat template auto adds a BOS token as well.
For llama.cpp / GGUF inference, you should skip the BOS since itâ€™ll auto add it.

`<ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>`

The `<think>` and `</think>` tokens get their own designated tokens. For the distilled versions for Qwen and Llama, some tokens are re-mapped, whilst Qwen for example did not have a BOS token, so <|object_ref_start|> had to be used instead.

### llama server å‚æ•°

[LLaMA.cpp HTTP Server README](https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md)

```sh
-t, --threads N     number of threads to use during generation (default: -1) (env: LLAMA_ARG_THREADS)
--prio N            set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)
-c, --ctx-size N    size of the prompt context (default: 4096, 0 = loaded from model) (env: LLAMA_ARG_CTX_SIZE)
-ctk, --cache-type-k TYPE   KV cache data type for K allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_K)
-ctv, --cache-type-v TYPE   KV cache data type for V allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_V)
-dev, --device <dev1,dev2,..>   comma-separated list of devices to use for offloading (none = don't offload) use --list-devices to see a list of available devices (env: LLAMA_ARG_DEVICE)
--list-devices    print list of available devices and exit
-ngl, --gpu-layers, --n-gpu-layers N  number of layers to store in VRAM (env: LLAMA_ARG_N_GPU_LAYERS)
-sm, --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:
                                      - none: use one GPU only
                                      - layer (default): split layers and KV across GPUs
                                      - row: split rows across GPUs (env: LLAMA_ARG_SPLIT_MODE)
-m, --model FNAME   model path (default: models/$filename with filename from --hf-file or --model-url if set, otherwise models/7B/ggml-model-f16.gguf)
(env: LLAMA_ARG_MODEL)
```

Sampling params

```sh
--jinja    Enable experimental Jinja templating engine (required for tool use)
--reasoning-format FORMAT    Controls extraction of model thinking traces and the format / field in which they are returned (default: deepseek; allowed values: deepseek, none; requires --jinja). none will leave thinking traces inline in message.content in a model-specific format, while deepseek will return them separately under message.reasoning_content
```

Example-specific params

```sh
--host HOST   ip address to listen (default: 127.0.0.1) (env: LLAMA_ARG_HOST)
--port PORT   port to listen (default: 8080) (env: LLAMA_ARG_PORT)

--embedding, --embeddings    restrict to only support embedding use case; use only with dedicated embedding models (default: disabled) (env: LLAMA_ARG_EMBEDDINGS)
--reranking, --rerank    enable reranking endpoint on server (default: disabled) (env: LLAMA_ARG_RERANKING)
--api-key KEY    API key to use for authentication (default: none) (env: LLAMA_API_KEY)  usage: --header "Authorization: Bearer KEY"
```

## æ¨¡å‹éƒ¨ç½²å·¥å…·

Ollamaï¼šé€‚åˆä¸ªäºº + æœ¬åœ°éƒ¨ç½² + è½»é‡ä½“éªŒ
vLLMï¼šé€‚åˆä¼ä¸šçº§ + æœåŠ¡å™¨éƒ¨ç½² + é«˜æ€§èƒ½æ‰©å±•

[å¯¹æ¥æœ¬åœ°å¤§æ¨¡å‹æ—¶ï¼Œé€‰æ‹© Ollma è¿˜æ˜¯ vLLMï¼Ÿ - OSCHINA - ä¸­æ–‡å¼€æºæŠ€æœ¯äº¤æµç¤¾åŒº](https://www.oschina.net/news/321572)
[å¤§æ¨¡å‹å·¥å…·å¯¹æ¯”ï¼šSGLang, Ollama, VLLM, LLaMA.cppå¦‚ä½•é€‰æ‹©ï¼Ÿ](https://stable-learn.com/zh/ai-model-tools-comparison/)

[Integrate Local Models Deployed by Xinference | Dify](https://docs.dify.ai/development/models-integration/xinference)
[Integrate Local Models Deployed by OpenLLM | Dify](https://docs.dify.ai/development/models-integration/openllm)
[Integrate Local Models Deployed by LocalAI | Dify](https://docs.dify.ai/development/models-integration/localai)

| å·¥å…·åç§°        | æ€§èƒ½è¡¨ç°                                                     | æ˜“ç”¨æ€§                                 | é€‚ç”¨åœºæ™¯                                 | ç¡¬ä»¶éœ€æ±‚                       | æ¨¡å‹æ”¯æŒ                      | éƒ¨ç½²æ–¹å¼                       |
|-------------|----------------------------------------------------------|-------------------------------------|--------------------------------------|----------------------------|---------------------------|----------------------------|
| SGLang v0.4 | é›¶å¼€é”€æ‰¹å¤„ç†æå‡1.1å€ååé‡ï¼Œç¼“å­˜æ„ŸçŸ¥è´Ÿè½½å‡è¡¡æå‡1.9å€ï¼Œç»“æ„åŒ–è¾“å‡ºæé€Ÿ10å€                | éœ€ä¸€å®šæŠ€æœ¯åŸºç¡€ï¼Œä½†æä¾›å®Œæ•´APIå’Œç¤ºä¾‹                 | ä¼ä¸šçº§æ¨ç†æœåŠ¡ã€é«˜å¹¶å‘åœºæ™¯ã€éœ€è¦ç»“æ„åŒ–è¾“å‡ºçš„åº”ç”¨             | æ¨èA100/H100ï¼Œæ”¯æŒå¤šGPUéƒ¨ç½²       | å…¨é¢æ”¯æŒä¸»æµå¤§æ¨¡å‹ï¼Œç‰¹åˆ«ä¼˜åŒ–DeepSeekç­‰æ¨¡å‹ | Dockerã€PythonåŒ…             |
| Ollama      | ç»§æ‰¿ llama.cpp çš„é«˜æ•ˆæ¨ç†èƒ½åŠ›ï¼Œæä¾›ä¾¿æ·çš„æ¨¡å‹ç®¡ç†å’Œè¿è¡Œæœºåˆ¶                      | å°ç™½å‹å¥½ï¼Œæä¾›å›¾å½¢ç•Œé¢å®‰è£…ç¨‹åºä¸€é”®è¿è¡Œå’Œå‘½ä»¤è¡Œï¼Œæ”¯æŒ REST API | ä¸ªäººå¼€å‘è€…åˆ›æ„éªŒè¯ã€å­¦ç”Ÿè¾…åŠ©å­¦ä¹ ã€æ—¥å¸¸é—®ç­”ã€åˆ›æ„å†™ä½œç­‰ä¸ªäººè½»é‡çº§åº”ç”¨åœºæ™¯ | ä¸ llama.cpp ç›¸åŒï¼Œä½†æä¾›æ›´ç®€ä¾¿çš„èµ„æºç®¡ç† | æ¨¡å‹åº“ä¸°å¯Œï¼Œæ¶µç›– 1700 å¤šæ¬¾ï¼Œæ”¯æŒä¸€é”®ä¸‹è½½å®‰è£… | ç‹¬ç«‹åº”ç”¨ç¨‹åºã€Dockerã€REST API     |
| VLLM        | å€ŸåŠ© PagedAttention å’Œ Continuous Batching æŠ€æœ¯ï¼Œå¤š GPU ç¯å¢ƒä¸‹æ€§èƒ½ä¼˜å¼‚ | éœ€è¦ä¸€å®šæŠ€æœ¯åŸºç¡€ï¼Œé…ç½®ç›¸å¯¹å¤æ‚                     | å¤§è§„æ¨¡åœ¨çº¿æ¨ç†æœåŠ¡ã€é«˜å¹¶å‘åœºæ™¯                      | è¦æ±‚ NVIDIA GPUï¼Œæ¨è A100/H100 | æ”¯æŒä¸»æµ Hugging Face æ¨¡å‹      | PythonåŒ…ã€OpenAIå…¼å®¹APIã€Docker |
| LLaMA.cpp   | å¤šçº§é‡åŒ–æ”¯æŒï¼Œè·¨å¹³å°ä¼˜åŒ–ï¼Œé«˜æ•ˆæ¨ç†                                        | å‘½ä»¤è¡Œç•Œé¢ç›´è§‚ï¼Œæä¾›å¤šè¯­è¨€ç»‘å®š                     | è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€ç§»åŠ¨ç«¯åº”ç”¨ã€æœ¬åœ°æœåŠ¡                    | CPU/GPU å‡å¯ï¼Œé’ˆå¯¹å„ç±»ç¡¬ä»¶ä¼˜åŒ–        | GGUFæ ¼å¼æ¨¡å‹ï¼Œå¹¿æ³›å…¼å®¹æ€§            | å‘½ä»¤è¡Œå·¥å…·ã€APIæœåŠ¡å™¨ã€å¤šè¯­è¨€ç»‘å®š         |

### æ¨¡å‹æ˜¾å­˜ä½¿ç”¨é‡è®¡ç®—

[Can Your Computer Run This LLM?](https://www.canirunthisllm.net/)
[Ollama GPU Compatibility Calculator - React App](https://aleibovici.github.io/ollama-gpu-calculator/)

[æ¨¡å‹æ˜¾å­˜ä½¿ç”¨é‡è®¡ç®— â€” Xinference](https://inference.readthedocs.io/zh-cn/stable/models/model_memory.html)
[LLM Model VRAM Calculator - a Hugging Face Space by NyxKrage](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)

[STOP asking for "the best model for my pc" : r/ollama](https://www.reddit.com/r/ollama/comments/1j8qp3g/stop_asking_for_the_best_model_for_my_pc/?share_id=xGRLbTeGKPHcJcmxcm5Je&utm_content=1&utm_medium=ios_app&utm_name=iossmf&utm_source=share&utm_term=22)

```sh
xinference cal-model-mem -s 7 -f gptq -c 8192 -n GOT-OCR2_0
xinference cal-model-mem -s 7 -q Int4 -f gptq -c 16384 -n qwen1.5-chat
# model_name: qwen1.5-chat
# kv_cache_dtype: 16
# model size: 7.0 B
# quant: Int4
# context: 16384
# gpu mem usage:
#   model mem: 4139 MB
#   kv_cache: 8192 MB
#   overhead: 650 MB
#   active: 17024 MB
#   total: 30005 MB (30 GB)

## GPU ä½¿ç”¨æƒ…å†µç›‘æ§
watch -n 1 nvidia-smi
```

### ollama æ¨¡å‹éƒ¨ç½²å·¥å…·

Ollama, a popular local LLM deployment tool, supports a broad range of open-source LLMs and offers an intuitive experience, making it ideal for single-user, local environments.

### OpenLLM

[bentoml/OpenLLM: Run any open-source LLMs, such as Llama, Mistral, as OpenAI compatible API endpoint in the cloud.](https://github.com/bentoml/OpenLLM)

From Ollama to OpenLLM: Running LLMs in the Cloud
[From Ollama to OpenLLM: Running LLMs in the Cloud](https://www.bentoml.com/blog/from-ollama-to-openllm-running-llms-in-the-cloud)

```powershell
# [ã€è§£å†³ã€‘æ— æ³•å°†â€œXXXâ€é¡¹è¯†åˆ«ä¸º cmdletã€å‡½æ•°ã€è„šæœ¬æ–‡ä»¶æˆ–å¯è¿è¡Œç¨‹åºçš„åç§°ã€‚è¯·æ£€æŸ¥åç§°çš„æ‹¼å†™ï¼Œå¦‚æœåŒ…æ‹¬è·¯å¾„ï¼Œè¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ï¼Œç„¶åå†è¯•ä¸€æ¬¡_æ— æ³•å°†â€œlabelmeâ€é¡¹è¯†åˆ«ä¸º cmdletã€å‡½æ•°ã€è„šæœ¬æ–‡ä»¶æˆ–å¯è¿è¡Œç¨‹åºçš„åç§°ã€‚è¯·æ£€æŸ¥å-CSDNåšå®¢](https://blog.csdn.net/weixin_41362657/article/details/110649744)
PS D:\>
Get-ExecutionPolicy -List

        Scope ExecutionPolicy
        ----- ---------------
MachinePolicy       Undefined
   UserPolicy       Undefined
      Process       Undefined
  CurrentUser       Undefined
 LocalMachine       Undefined

# Scope: Process, CurrentUser, LocalMachine, UserPolicy, MachinePolicy
# ExecutionPolicy: Unrestricted, RemoteSigned, AllSigned, Restricted, Default, Bypass, Undefinedâ€
Set-ExecutionPolicy RemoteSigned -Scope CurrentUser
Set-ExecutionPolicy Unrestricted -Scope CurrentUser
Set-ExecutionPolicy RemoteSigned -Scope LocalMachine
```

### LocalAI

[LocalAI-examples/langchain-chroma at main Â· mudler/LocalAI-examples](https://github.com/mudler/LocalAI-examples/tree/main/langchain-chroma)

```sh
# é…ç½®åˆ°dify æ—¶ 404
# 2025-02-11 11:51:10 3:51AM WRN Client error ip=172.20.0.1 latency="24.32Âµs" method=POST status=404 url=/rerank

curl http://localhost:8080/console/api/workspaces/current/models/model-types/rerank

curl http://localhost:8080/v1/rerank \
  -H "Content-Type: application/json" \
  -d '{
  "model": "cross-encoder",
  "query": "Organic skincare products for sensitive skin",
  "documents": [
    "Eco-friendly kitchenware for modern homes",
    "Biodegradable cleaning supplies for eco-conscious consumers",
    "Organic cotton baby clothes for sensitive skin",
    "Natural organic skincare range for sensitive skin",
    "Tech gadgets for smart homes: 2024 edition",
    "Sustainable gardening tools and compost solutions",
    "Sensitive skin-friendly facial cleansers and toners",
    "Organic food wraps and storage solutions",
    "All-natural pet food for dogs with allergies",
    "Yoga mats made from recycled materials"
  ],
  "top_n": 3
}'

```

### xinference

[åœ¨Xinferenceä¸Šéƒ¨ç½²è‡ªå®šä¹‰å¤§æ¨¡å‹â€”â€”FreedomIntelligence/HuatuoGPT2-13Bä¸ºä¾‹ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/685747169)

```sh
# å°†æ¨¡å‹ä¸‹è½½æºè®¾ç½®ä¸º ModelScopeã€‚è®¾ç½®ç¯å¢ƒå˜é‡ XINFERENCE_MODEL_SRC=modelscope
docker pull registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference
docker image tag registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xprobe_xinference/xinference:latest
docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9997:9997 --gpus all xprobe/xinference:v<your_version> xinference-local -H 0.0.0.0 --log-level debug

  --restart always \
docker run --detach \
  --name xinference \
  --env TZ=Asia/Shanghai \
  --publish 9997:9997 \
  --env XINFERENCE_MODEL_SRC=modelscope \
  --volume //c/workspace/xinference/.xinference:/root/.xinference \
  --volume //c/workspace/xinference/.cache/huggingface:/root/.cache/huggingface \
  --volume //c/workspace/xinference/.cache/modelscope:/root/.cache/modelscope \
  --gpus all \
  xprobe_xinference/xinference:v0.15.4 \
  sh /root/.xinference/startup.sh

# Windowsä¸‹æ”¹æˆä¸€è¡Œæ‰§è¡Œ
docker run --detach --env TZ=Asia/Shanghai --publish 9997:9997 --name xinference --restart always -e XINFERENCE_MODEL_SRC=modelscope -v //c/workspace/xinference/.xinference:/root/.xinference -v //c/workspace/xinference/.cache/huggingface:/root/.cache/huggingface -v //c/workspace/xinference/.cache/modelscope:/root/.cache/modelscope --gpus all xprobe_xinference/xinference sh /root/.xinference/startup.sh

alias xinference='docker exec -it xinference xinference'
```

```sh
#!/bin/bash
# startup.sh æ”¾åœ¨å¤–éƒ¨ç£ç›˜æŒ‚è½½è¿›å»ï¼Œå¯åŠ¨æ—¶æ‰§è¡Œ
# å¯åŠ¨ä¸”åå°è¿è¡Œ
xinference-local -H 0.0.0.0 &
# xinference-local -H 0.0.0.0 --log-level debug &
# æ£€æµ‹æ˜¯å¦å¯åŠ¨
while true; do
  if curl -s "http://localhost:9997" > /dev/null; then
    break
  else
    sleep 1
  fi
done

#è‡ªåŠ¨åŠ è½½ embedding
xinference launch --model-name bge-m3 --model-type embedding &
# xinference launch --model-name jina-embeddings-v3 --model-type embedding &
#è‡ªåŠ¨åŠ è½½ rerank
xinference launch --model-name jina-reranker-v2 --model-type rerank &

#ç­‰å¾…åå°è¿è¡Œç»“æŸï¼Œå®é™…ä¸Šxinference-localæ˜¯ä¸ä¼šç»“æŸçš„ï¼Œæ‰€ä»¥èƒ½ä¿è¯æ­¤è„šæœ¬è¿›ç¨‹ä¸ç»“æŸï¼Œä»è€Œä¸ä¼šè‡ªåŠ¨é‡å¯
wait
```

è°ƒç”¨æ¨¡å‹æ¥å£

```sh
# llm chat
curl -X 'POST' \
  'http://127.0.0.1:9997/v1/chat/completions' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "qwen2.5-instruct",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "What is the largest animal?"
        }
    ]
  }'

# embeddings
curl -X 'POST' 'http://localhost:9997/v1/embeddings' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "bge-m3",
    "input": "Hello, world!"
  }'

# rank
curl http://localhost:9997/v1/rerank \
  -H "Content-Type: application/json" \
  -d '{
  "model": "bge-reranker-v2-m3",
  "query": "Organic skincare products for sensitive skin",
  "documents": [
    "Eco-friendly kitchenware for modern homes",
    "Biodegradable cleaning supplies for eco-conscious consumers",
    "Organic cotton baby clothes for sensitive skin",
    "Natural organic skincare range for sensitive skin",
    "Tech gadgets for smart homes: 2024 edition",
    "Sustainable gardening tools and compost solutions",
    "Sensitive skin-friendly facial cleansers and toners",
    "Organic food wraps and storage solutions",
    "All-natural pet food for dogs with allergies",
    "Yoga mats made from recycled materials"
  ],
  "top_n": 3
}'
```

æœ¬åœ°å®‰è£…

```sh
# æœ¬åœ°å®‰è£…
conda create --name py312 python=3.12
conda activate py312

# å®‰è£…xinferenceçš„ä¾èµ–
pip install "xinference[all]"

# å¯åŠ¨xinference
xinference-local # æˆ‘ä½¿ç”¨è¿™ä¸ªå‘½ä»¤å¯åŠ¨ä¸äº†
# è®¾ç½®ä½¿ç”¨modelscopeä¸‹è½½æ¨¡å‹
# å¦‚æœä½ å°±ä¸€å—gpuè¿˜æ˜¯0çš„è¯å°±è¦æŒ‡å®šå¯åŠ¨
CUDA_VISIBLE_DEVICES=0
# ä½¿ç”¨è¿™ä¸ªå‘½ä»¤å¯ä»¥å¯åŠ¨
XINFERENCE_HOME=d:/xinference/ XINFERENCE_MODEL_SRC=modelscope xinference-local --host 0.0.0.0 --port 9997

# åˆ—ä¸¾æœ¬åœ°æ¨¡å‹
xinference list --endpoint "http://127.0.0.1:9997"

# [jina-embeddings-v2-base-zh â€” Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/embedding/jina-embeddings-v2-base-zh.html)
# Model ID: jinaai/jina-embeddings-v2-base-zh
# xinference launch --model-name jina-embeddings-v2-base-zh --model-type embedding
xinference launch --model-name jina-embeddings-v3 --model-type embedding &
xinference launch --model-name bge-m3 --model-type embedding &

# [jina-reranker-v2 â€” Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/rerank/jina-reranker-v2.html)
# Model ID: jinaai/jina-reranker-v2-base-multilingual
xinference launch --model-name jina-reranker-v2 --model-type rerank &
xinference launch --model-name bge-reranker-large --model-type rerank &
xinference launch --model-name bge-reranker-v2-minicpm-layerwise --model-type rerank &

# OCR æ¨¡å‹ [GOT-OCR2_0 â€” Xinference](https://inference.readthedocs.io/zh-cn/v0.16.3/models/builtin/image/got-ocr2_0.html)
xinference launch --model-name GOT-OCR2_0 --model-type image
# Launch model name: GOT-OCR2_0 with kwargs: {}
# Model uid: GOT-OCR2_0

# åˆ—å‡ºæ‰€æœ‰å†…ç½®æ”¯æŒçš„æ¨¡å‹
xinference registrations -t embedding
xinference registrations -t rerank
```

### SGLang

[sgl-project/sglang: SGLang is a fast serving framework for large language models and vision language models.](https://github.com/sgl-project/sglang)

### Embedding model

[Embedding models Â· Ollama Blog](https://ollama.com/blog/embedding-models)

[MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)
MTEBï¼ˆMassive Text Embedding Benchmarkï¼‰æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ–‡æœ¬åµŒå…¥ï¼ˆEmbeddingï¼‰æ¨¡å‹çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•å¹³å°ã€‚é€šè¿‡å¤šä»»åŠ¡å’Œå¤šæ•°æ®é›†çš„ç»„åˆï¼ŒMTEBå¯ä»¥å…¨é¢è¡¡é‡ä¸åŒEmbeddingæ¨¡å‹åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€è¯­ä¹‰æ£€ç´¢ã€æ–‡æœ¬èšç±»ç­‰ã€‚

```sh
ollama pull mxbai-embed-large
```

### reranking models

Ollama rerank model [linux6200/bge-reranker-v2-m3](https://ollama.com/linux6200/bge-reranker-v2-m3)

deploy local embedding/reranking models using xinference/LocalAI/OpenLLM.

[Using Xinference â€” Xinference](https://inference.readthedocs.io/en/latest/getting_started/using_xinference.html#using-xinference-with-docker)

```sh
docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9998:9997 --gpus all xprobe/xinference:<your_version> xinference-local -H 0.0.0.0 --log-level debug

# 1024 ç»´åº¦  æœ€å¤§ token æ•° 8192
jina-embeddings-v3
```

## æœ¬åœ°çŸ¥è¯†åº“æ­å»º

[chatchat-space/Langchain-Chatchat: Langchain-Chatchatï¼ˆåŸLangchain-ChatGLMï¼‰åŸºäº Langchain ä¸ ChatGLM ç­‰è¯­è¨€æ¨¡å‹çš„æœ¬åœ°çŸ¥è¯†åº“é—®ç­” | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM) QA app with langchain](https://github.com/chatchat-space/Langchain-Chatchat)
[Langchain-ChatGLMï¼šåŸºäºæœ¬åœ°çŸ¥è¯†åº“é—®ç­”_langchain chatglm-CSDNåšå®¢](https://blog.csdn.net/dzysunshine/article/details/131003488)

[AnythingLLMã€Dify ä¸ Open-WebUIï¼šå¦‚ä½•æ¥å…¥ Ollamaï¼Œå®ƒä»¬æœ‰ä½•ä¸åŒï¼Ÿä¸€ã€å‰è¨€ éšç€å¤§è¯­è¨€æ¨¡ - æ˜é‡‘](https://juejin.cn/post/7455148200627781684)
[Difyã€Anything LLMã€Ollamaã€ç¡…åŸºæµåŠ¨Â åˆ†åˆ«ä¸Â DeepSeekÂ è”åˆæ­å»ºæœ¬åœ°çŸ¥è¯†åº“çš„å¯¹æ¯”è¡¨æ ¼ï¼Œæ¶µç›–åŠŸèƒ½ã€æ€§èƒ½ã€æˆæœ¬å’Œé€‚ç”¨åœºæ™¯ç­‰æ ¸å¿ƒç»´åº¦ã€‚ #æ™®é€šäººå¦‚ä½•ç©è½¬DeepSeek# #deepseek#](https://www.toutiao.com/w/1823311450553344/?app=news_article&category_new=search_thread_aggr&chn_id=94349607399&is_new_connect=0&is_new_user=0&req_id_new=202502090734553364518F875E49146204&share_did=MS4wLjACAAAAZtHq-FtDCLT2HiypHOsw85chAV9wX6K1Jv5UfAUyX0mVQ7HN00yXQhdxXLFA_4OY&share_token=D19DE10F-BE2F-4A6D-A704-AA68397479DC&share_uid=MS4wLjABAAAADwEoMfih4HUgYxYOjTmk9NvGlX1gGSG-ctDaxpGLfOkP-tRdhwDh6SLBAt5iihFb&source=m_redirect&timestamp=1739057696&tt_from=weixin_moments&use_new_style=1&utm_campaign=client_share&utm_medium=toutiao_ios&utm_source=weixin_moments&wxshare_count=1)

LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ï¼Œä¸»è¦æ‹¥æœ‰ 3ä¸ªèƒ½åŠ›ï¼š

1. å¯ä»¥è°ƒç”¨LLMæ¨¡å‹
2. å¯ä»¥å°† LLM æ¨¡å‹ä¸å¤–éƒ¨æ•°æ®æºè¿›è¡Œè¿æ¥
3. å…è®¸ä¸ LLM æ¨¡å‹è¿›è¡Œäº¤äº’

çŸ¥è¯†åº“é—®ç­”å®ç°æ­¥éª¤
åŸºäºLangchainæ€æƒ³å®ç°åŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„é—®ç­”åº”ç”¨ã€‚å®ç°è¿‡ç¨‹å¦‚ä¸‹ï¼š
1ã€åŠ è½½æ–‡ä»¶
2ã€è¯»å–æ–‡æœ¬
3ã€æ–‡æœ¬åˆ†å‰²
4ã€æ–‡æœ¬å‘é‡åŒ–
5ã€é—®å¥å‘é‡åŒ–
6ã€åœ¨æ–‡æœ¬å‘é‡ä¸­åŒ¹é…å‡ºä¸é—®å¥å‘é‡æœ€ç›¸ä¼¼çš„top kä¸ª
7ã€åŒ¹é…å‡ºçš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡å’Œé—®é¢˜ä¸€èµ·æ·»åŠ åˆ°promptä¸­
8ã€æäº¤ç»™LLMç”Ÿæˆå›ç­”ã€‚

### open-webui

[open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)](https://github.com/open-webui/open-webui)

### MaxKB æ–¹æ¡ˆ

[MaxKB/README_CN.md at main Â· 1Panel-dev/MaxKB](https://github.com/1Panel-dev/MaxKB/blob/main/README_CN.md)

[MaxKB ç¦»çº¿å®‰è£…åŒ…ä¸‹è½½ - FIT2CLOUD é£è‡´äº‘](https://community.fit2cloud.com/#/download/maxkb/v1-9-1)

```sh
# Linux æœºå™¨
docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data -v ~/.python-packages:/opt/maxkb/app/sandbox/python-packages cr2.fit2cloud.com/1panel/maxkb

# Windows æœºå™¨
docker run -d --name=maxkb --restart=always -p 8080:8080 -v C:/maxkb:/var/lib/postgresql/data -v C:/python-packages:/opt/maxkb/app/sandbox/python-packages cr2.fit2cloud.com/1panel/maxkb
docker run -d --name=maxkb --restart=always -p 8080:8080 -v d:/docker/maxkb:/var/lib/postgresql/data -v d:/docker/maxkb/python-packages:/opt/maxkb/app/sandbox/python-packages 1panel/maxkb:v1.10.0-lts

# ç”¨æˆ·å: admin
# å¯†ç : MaxKB@123..
# Max1+1
```

### anything-llm

[Mintplex-Labs/anything-llm: The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, and more.](https://github.com/Mintplex-Labs/anything-llm)

[anything-llm HOW_TO_USE_DOCKER](https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/HOW_TO_USE_DOCKER.md)

```powershell
# Run this in powershell terminal
$env:STORAGE_LOCATION="$HOME\Documents\anythingllm"; `
If(!(Test-Path $env:STORAGE_LOCATION)) {New-Item $env:STORAGE_LOCATION -ItemType Directory}; `
If(!(Test-Path "$env:STORAGE_LOCATION\.env")) {New-Item "$env:STORAGE_LOCATION\.env" -ItemType File}; `
docker run -d -p 3001:3001 `
--cap-add SYS_ADMIN `
-v "$env:STORAGE_LOCATION`:/app/server/storage" `
-v "$env:STORAGE_LOCATION\.env:/app/server/.env" `
-e STORAGE_DIR="/app/server/storage" `
mintplexlabs/anythingllm;

# anyadmin / Anyadmin1+1
```

### FastGPT

[labring/FastGPT: FastGPT is a knowledge-based platform built on the LLMs, offers a comprehensive suite of out-of-the-box capabilities such as data processing, RAG retrieval, and visual AI workflow orchestration, letting you easily develop and deploy complex question-answering systems without the need for extensive setup or configuration.](https://github.com/labring/FastGPT)

[å¿«é€Ÿäº†è§£ FastGPT | FastGPT](https://doc.tryfastgpt.ai/docs/intro/)

## Dify

[Dify github](https://github.com/langgenius/dify)
[åœ¨çº¿ç‰ˆ Studio - Dify](https://cloud.dify.ai/apps)

[Dify æ–‡æ¡£](https://docs.dify.ai/zh-hans)

[ç‰¹æ€§ä¸æŠ€æœ¯è§„æ ¼ | Dify](https://docs.dify.ai/zh-hans/getting-started/readme/features-and-specifications)

[DifyShare - Share your flows. View the magic.](https://difyshare.com/)
[BannyLon/DifyAIA: åŸºäºDifyè‡ªä¸»åˆ›å»ºçš„AIåº”ç”¨DSLå·¥ä½œæµ](https://github.com/BannyLon/DifyAIA)
[DeepSeek+dify æœ¬åœ°çŸ¥è¯†åº“ï¼šé«˜çº§åº”ç”¨Agent+å·¥ä½œæµ](https://mp.weixin.qq.com/s/1p5KRDflsIISdOvm4QkWYw)
[Dify æ¶æ„ç¯‡| å¤šç§Ÿæˆ·ä¸‹çš„SSOåŠŸèƒ½ - 53AI-AIçŸ¥è¯†åº“|å¤§æ¨¡å‹çŸ¥è¯†åº“|å¤§æ¨¡å‹è®­ç»ƒ|æ™ºèƒ½ä½“å¼€å‘](https://www.53ai.com/news/dify/2025032992518.html)

Dify is an open-source LLM app development platform. Dify's intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.

æ¡ˆä¾‹ï¼š

- è®­ç»ƒå‡ºä¸“å±äºâ€œä½ â€çš„é—®ç­”æœºå™¨äºº
- å®˜ç½‘ AI æ™ºèƒ½å®¢æœ
- æ¥å…¥å¾®ä¿¡
- æ¥å…¥é’‰é’‰

[jaguarliuu/rookie\_text2data: Difyæ’ä»¶ - è‡ªç„¶è¯­è¨€è·å–æ•°æ®åº“æ•°æ®](https://github.com/jaguarliuu/rookie_text2data)
[Markdownè½¬mdæ–‡ä»¶ Markdown Exporter - Dify Marketplace](https://marketplace.dify.ai/plugins/bowenliang123/md_exporter)

### dify deploy

[Deploy Dify on Kubernetes](https://github.com/Winson-030/dify-kubernetes)

.env æ–‡ä»¶ä¿®æ”¹

```sh
LOG_LEVEL=DEBUG

LOG_TZ=Asia/Shanghai
LOG_FILE=/app/logs/server.log

DEBUG=true
FLASK_DEBUG=true

## customize
# Add environment variables below at the end of .env file, then docker compose down && docker compose up -d
PLUGIN_PYTHON_ENV_INIT_TIMEOUT=720
PIP_MIRROR_URL=https://mirrors.aliyun.com/pypi/simple
```

### Dify 1.0.1 deploy

[Release v1.0.1 Â· langgenius/dify](https://github.com/langgenius/dify/releases/tag/1.0.1)

PowerShell ä¸‹å¯åŠ¨

### Dify 1.0.0 deploy

[dify-docs/zh_CN/development/migration/migrate-to-v1.md at main Â· langgenius/dify-docs](https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/migration/migrate-to-v1.md)

```sh
root@e81a1e9bfdc9:/app/api# poetry run flask install-plugins --workers=2
2025-03-09 06:08:52.421 INFO [MainThread] [utils.py:149] - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-03-09 06:08:52.422 INFO [MainThread] [utils.py:162] - NumExpr defaulting to 16 threads.
```

### Dify 0.15.3 deploy

[Docker Compose éƒ¨ç½² | Dify](https://docs.dify.ai/zh-hans/getting-started/install-self-hosted/docker-compose)

.env ä¿®æ”¹éƒ¨åˆ†

```sh
# .env

# Used to change the OpenAI base address, default is https://api.openai.com/v1.
# When OpenAI cannot be accessed in China, replace it with a domestic mirror address,
# or when a local model provides OpenAI compatible API, it can be replaced.
OPENAI_API_BASE=https://api.openai.com/v1

# Defaults to gevent. If using windows, it can be switched to sync or solo.
SERVER_WORKER_CLASS=gevent

# Upload file size limit, default 15M.
UPLOAD_FILE_SIZE_LIMIT=15
# Upload image file size limit, default 10M.
UPLOAD_IMAGE_FILE_SIZE_LIMIT=10
# Upload video file size limit, default 100M.
UPLOAD_VIDEO_FILE_SIZE_LIMIT=100
# Upload audio file size limit, default 50M.
UPLOAD_AUDIO_FILE_SIZE_LIMIT=50

MAIL_DEFAULT_SEND_FROM=è‡ªå·±çš„é‚®ç®±
#Â SMTP server configuration, used when MAIL_TYPE is `smtp`
SMTP_SERVER= å¯¹åº”é‚®ç®±çš„smtpï¼Œä¸€èˆ¬éƒ½åœ¨è®¾ç½®é‡Œ
SMTP_PORT=465
SMTP_USERNAME= è‡ªå·±çš„é‚®ç®±
SMTP_PASSWORD= Â è‡ªå·±çš„å¯†ç 
SMTP_USE_TLS=true
SMTP_OPPORTUNISTIC_TLS=false
```

Docker Compose éƒ¨ç½²

```sh
# å…‹éš† Dify æºä»£ç è‡³æœ¬åœ°ç¯å¢ƒã€‚
# å‡è®¾å½“å‰æœ€æ–°ç‰ˆæœ¬ä¸º 0.15.3
git clone https://github.com/langgenius/dify.git --branch 0.15.3

# å¯åŠ¨ Dify
# 1.  è¿›å…¥ Dify æºä»£ç çš„ Docker ç›®å½•
cd dify/docker
# 2.  å¤åˆ¶ç¯å¢ƒé…ç½®æ–‡ä»¶
cp .env.example .env
# 3.  å¯åŠ¨ Docker å®¹å™¨
docker-compose up -d
```

æ›´æ–° Dify è¿›å…¥ dify æºä»£ç çš„ docker ç›®å½•ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```sh
cd dify/docker
docker compose down
git pull origin main
docker compose pull
docker compose up -d
```

è®¿é—® Dify
ä½ å¯ä»¥å…ˆå‰å¾€ç®¡ç†å‘˜åˆå§‹åŒ–é¡µé¢è®¾ç½®è®¾ç½®ç®¡ç†å‘˜è´¦æˆ·ï¼š

```sh
# æœ¬åœ°ç¯å¢ƒ
http://localhost/install

# æœåŠ¡å™¨ç¯å¢ƒ
http://your_server_ip/install

# Dify ä¸»é¡µé¢ï¼š
# æœ¬åœ°ç¯å¢ƒ
http://localhost

# æœåŠ¡å™¨ç¯å¢ƒ
http://your_server_ip
```

### å¦‚ä½•é‡ç½®difyç®¡ç†å‘˜å¯†ç 

```sh
docker exec -it docker-api-1 flask reset-password
# ç„¶åæŒ‰ç…§æç¤ºè¾“å…¥ç®¡ç†å‘˜emailä»¥åŠä¸¤æ¬¡æ–°å¯†ç å³å¯ã€‚
```

### dify é’‰é’‰

[å°† Dify åº”ç”¨ä¸é’‰é’‰æœºå™¨äººé›†æˆ | Dify](https://docs.dify.ai/zh-hans/learn-more/use-cases/dify-on-dingtalk)

### dify database

select name,email,interface_language,last_login_at,last_login_ip,status,created_at,last_active_at from accounts;

### dify issue

```sh
# agent è°ƒç”¨é›…è™è´¢ç»
# prompt ä»Šå¤©æœ‰å“ªäº›æ–°é—»
[ollama] Error: API request failed with status code 400: {"error":"registry.ollama.ai/library/deepseek-r1:7b does not support tools"}
```

## firecrawl

[firecrawl/CONTRIBUTING](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md)

[localhost:3002](http://localhost:3002/)
[admin/queues](http://localhost:3002/admin/queues)

```sh
docker pull node:18-slim
docker pull node:22-slim
docker pull rust:1-slim
docker pull golang:1.24


curl -X POST http://localhost:3002/v1/crawl \
    -H 'Content-Type: application/json' \
    -d '{
      "url": "https://www.baidu.com"
    }'

curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "limit": 10,
      "scrapeOptions": {
        "formats": ["markdown", "html"]
      }
    }'


```

## GPU ä»‹ç»

GPU å‘½åè§„åˆ™è§£è¯»

[ä¸€æ–‡è¯»æ‡‚ NVIDIA GPU äº§å“çº¿-51CTO.COM](https://www.51cto.com/article/805028.html)
[è‹±ä¼Ÿè¾¾GPUå„å‹å·å¯¹æ¯” - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/718450083)
[æ¯”è¾ƒ GeForce ç³»åˆ—æœ€æ–°ä¸€ä»£æ˜¾å¡å’Œå‰ä»£æ˜¾å¡ | NVIDIA](https://www.nvidia.cn/geforce/graphics-cards/compare/)

### å­—æ¯ï¼š æ¶æ„ä»£å·ï¼ˆArchitectureï¼‰

æ–°çš„æ¶æ„é€šå¸¸ä»£è¡¨ç€æ€§èƒ½ã€èƒ½æ•ˆæ¯”å’Œæ–°æŠ€æœ¯çš„æ˜¾è‘—æå‡ã€‚

ä»£è¡¨ GPU çš„æ ¸å¿ƒæ¶æ„ï¼Œé€šå¸¸ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªå­—æ¯è¡¨ç¤ºï¼Œä»£è¡¨ GPU çš„å¾®æ¶æ„ã€‚ä¾‹å¦‚ï¼š

Kï¼šKepler æ¶æ„ 2012
Vï¼šVolta æ¶æ„ 2017
Tï¼šTuring æ¶æ„ 2018, RTX 20 ç³»åˆ—, GTX 16 ç³»åˆ—
Aï¼šAmpere æ¶æ„ 2019, RTX 30 ç³»åˆ—
Hï¼šHopper æ¶æ„ 2022, RTX 4000 ç³»åˆ—
L: Ada Lovelace æ¶æ„ 2022, RTX 40 ç³»åˆ—
B: Blackwell æ¶æ„, RTX 50 ç³»åˆ—

### æ•°å­—ï¼šæ€§èƒ½å±‚çº§ï¼ˆTierï¼‰

é€šå¸¸ç”¨æ•°å­—è¡¨ç¤ºï¼Œæ•°å­—è¶Šå¤§é€šå¸¸ä»£è¡¨æ€§èƒ½è¶Šå¼ºã€‚

ä»£è¡¨ GPU çš„å…·ä½“å‹å·ï¼Œé€šå¸¸ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªæ•°å­—è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼š

â€œ4â€ ç³»åˆ—ï¼šå…¥é—¨çº§æˆ–ä½åŠŸè€—çº§
â€œ10â€ ç³»åˆ—ï¼šä¸­ç«¯æ¨ç†ä¼˜åŒ–çº§
â€œ40â€ ç³»åˆ—ï¼šé«˜ç«¯å›¾å½¢å’Œè™šæ‹Ÿå·¥ä½œç«™çº§
â€œ100â€ ç³»åˆ—ï¼šæ——èˆ°çº§é«˜æ€§èƒ½è®¡ç®—å’Œäººå·¥æ™ºèƒ½çº§

### å¸¸è§çš„GPU å‹å·å¯¹æ¯”è§£æï¼šåŸºäº GPU å‘½åæ¨æ–­æ˜¾å¡ç‰¹æ€§

ç¤ºä¾‹ä¸€ï¼šT4 ä¸ L4 çš„æ¯”è¾ƒ

L4 æ˜¯ T4 çš„ç›´æ¥åç»§è€…ï¼Œå±äºåŒä¸€æ€§èƒ½å±‚çº§ï¼Œé’ˆå¯¹ç›¸ä¼¼çš„åº”ç”¨åœºæ™¯è®¾è®¡ã€‚ç„¶è€Œï¼Œä¸¤è€…åœ¨å¾®æ¶æ„å’ŒæŠ€æœ¯è§„æ ¼ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼š

å¾®æ¶æ„ï¼š L4 é‡‡ç”¨æ›´æ–°çš„ Ada Lovelace æ¶æ„ï¼ˆ2023 å¹´å‘å¸ƒï¼‰ï¼Œè€Œ T4 åˆ™é‡‡ç”¨è¾ƒæ—©çš„ Turing æ¶æ„ï¼ˆ2018 å¹´å‘å¸ƒï¼‰ã€‚
æ˜¾å­˜å®¹é‡ï¼š L4 é…å¤‡äº†æ›´å¤§çš„æ˜¾å­˜å®¹é‡ï¼Œè¾¾åˆ° 24 GBï¼Œè€Œ T4 ä»…æœ‰ 16 GBã€‚
æ ¸å¿ƒæ•°é‡å’Œæ€§èƒ½ï¼š L4 æ‹¥æœ‰æ›´å¤šä¸”æ›´å¼ºå¤§çš„è®¡ç®—æ ¸å¿ƒï¼Œå› æ­¤åœ¨æ€§èƒ½ä¸Šä¼˜äº T4ã€‚
è™½ç„¶ä¸¤è€…çš„ç›®æ ‡åŠŸè€—ç›¸ä¼¼ï¼Œä½† L4 å‡­å€Ÿæ›´å…ˆè¿›çš„æ¶æ„å’Œæ›´é«˜çš„æ˜¾å­˜å®¹é‡ï¼Œåœ¨ç›¸åŒçš„åŠŸè€—ä¸‹èƒ½å¤Ÿæä¾›æ›´å¼ºçš„è®¡ç®—æ€§èƒ½ï¼Œæ›´é€‚åˆå¤„ç†å¯¹æ˜¾å­˜å®¹é‡æœ‰è¾ƒé«˜è¦æ±‚çš„ä»»åŠ¡ã€‚

ç¤ºä¾‹äºŒï¼šA10 ä¸ A100 çš„æ¯”è¾ƒ

A100 æ˜¯åŸºäº Ampere æ¶æ„çš„æ——èˆ°çº§äº§å“ï¼Œè€Œ A10 åˆ™æ˜¯è¯¥æ¶æ„ä¸‹çš„ä¸€ä¸ªè¾ƒä½å±‚çº§çš„å‹å·ã€‚ä¸¤è€…éƒ½åŸºäºç›¸åŒçš„ Ampere å¾®æ¶æ„ï¼Œä½†åœ¨è§„æ¨¡å’Œæ€§èƒ½ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼š

æ ¸å¿ƒæ•°é‡å’Œæ€§èƒ½ï¼š A100 æ‹¥æœ‰è¿œå¤šäº A10 çš„è®¡ç®—æ ¸å¿ƒï¼Œå› æ­¤åœ¨è®¡ç®—æ€§èƒ½ä¸Šè¿œè¶… A10ã€‚
æ˜¾å­˜å®¹é‡ï¼š A100 é…å¤‡äº†æ›´å¤§çš„æ˜¾å­˜å®¹é‡ï¼Œä»¥æ”¯æŒæ›´å¤§è§„æ¨¡çš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ã€‚
åŠŸè€—ï¼š ç”±äºè§„æ¨¡æ›´å¤§ã€æ€§èƒ½æ›´å¼ºï¼ŒA100 çš„åŠŸè€—ä¹Ÿé«˜äº A10ã€‚
å› æ­¤ï¼ŒA100 æ›´é€‚åˆéœ€è¦å¤„ç†å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒã€å¾®è°ƒå’Œé«˜ååé‡æ¨ç†ç­‰ demanding è®¡ç®—ä»»åŠ¡çš„åœºæ™¯ï¼Œè€Œ A10 åˆ™æ›´é€‚åˆå¯¹æˆæœ¬å’ŒåŠŸè€—æ•æ„Ÿã€å¯¹æ€§èƒ½è¦æ±‚ç›¸å¯¹è¾ƒä½çš„åº”ç”¨åœºæ™¯ã€‚

### é¢å‘ AI å’Œæœºå™¨å­¦ä¹ çš„æ˜¾å¡

| æ’å | GPU å‹å· | æ¶æ„ | CUDA æ ¸å¿ƒæ•° | æ˜¾å­˜ | ä¸»è¦ç”¨é€” |
| --- | --- | --- | --- | --- | --- |
| 1 | NVIDIA H100 | Hopper | 16896 | 80GB HBM3 | å¤§è§„æ¨¡ AI è®­ç»ƒ |
| 2 | NVIDIA A100 | Ampere | 6912 | 40GB HBM2 | AI æ¨ç†ã€æœºå™¨å­¦ä¹  |
| 3 | NVIDIA V100 | Volta | 5120 | 32GB HBM2 | ç¥ç»ç½‘ç»œè®­ç»ƒã€ç§‘å­¦è®¡ç®— |
| 4 | Tesla T4 | Turing | 2560 | 16GB GDDR6 | è½»é‡ AI æ¨ç†ã€äº‘æ¨ç† |
| 5 | Tesla P100 | Pascal | 3584 | 16GB HBM2 | æ•°æ®ä¸­å¿ƒåŠ é€Ÿã€æœºå™¨å­¦ä¹ æ¨ç† |
| 6 | NVIDIA A40 | Ampere | ? | 48GB | AI æ¨ç†ã€æœºå™¨å­¦ä¹  |

H20 96GB

### ç¡¬ä»¶èµ„æºé…ç½®

æ˜¾å­˜éœ€æ±‚ â‰ˆ æ¨¡å‹å‚æ•° Ã— å‚æ•°å­—èŠ‚æ•° Ã— å®‰å…¨ç³»æ•°ï¼ˆ1.3-1.5ï¼‰

CPU GPU é…ç½®æ¯”ä¾‹ï¼šå»ºè®®å†…å­˜æ˜¯æ˜¾å­˜çš„1.5å€ä»¥ä¸Š

### gpu ç›¸å…³å‘½ä»¤

```sh
nvcc --version
# nvcc: NVIDIA (R) Cuda compiler driver
# Copyright (c) 2005-2021 NVIDIA Corporation
# Built on Thu_Nov_18_09:45:30_PST_2021
# Cuda compilation tools, release 11.5, V11.5.119
# Build cuda_11.5.r11.5/compiler.30672275_0

nvidia-smi
# Mon Mar 31 14:24:25 2025
# +-----------------------------------------------------------------------------------------+
# | NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
# |-----------------------------------------+------------------------+----------------------+
# | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
# |                                         |                        |               MIG M. |
# |=========================================+========================+======================|
# |   0  NVIDIA A40                     Off |   00000000:67:00.0 Off |                    0 |
# |  0%   75C    P0            303W /  300W |   43985MiB /  46068MiB |     96%      Default |
# |                                         |                        |                  N/A |
# +-----------------------------------------+------------------------+----------------------+

# +-----------------------------------------------------------------------------------------+
# | Processes:                                                                              |
# |  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
# |        ID   ID                                                               Usage      |
# |=========================================================================================|
# |    0   N/A  N/A           25809      C   /usr/local/bin/ollama                 43976MiB |
# +-----------------------------------------------------------------------------------------+

pip install nvitop
# æŸ¥çœ‹
nvitop
```
