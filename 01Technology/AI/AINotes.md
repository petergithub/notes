# AI News & Information

AIå¹²ä¸äº†ä½ å¹²ä¸äº†çš„äº‹ï¼Œå› ä¸ºä½ å¹²ä¸äº†ä½ å°±çœ‹ä¸å‡ºæ¥å®ƒæ˜¯èƒ¡å¹²è¿˜æ˜¯çœŸå¹²ã€‚AIçš„æœ‰æ•ˆèƒ½åŠ›èŒƒå›´ç­‰äºè·Ÿå®ƒé…å¯¹çš„äººçš„èƒ½åŠ›èŒƒå›´ã€‚

- [Humata - ChatGPT for all your files](https://www.humata.ai/)
- [Code Llama: Inference code for CodeLlama models](https://github.com/facebookresearch/codellama)

## GPT åˆ†ç±»

[é«˜ç››å‘å¸ƒä¸­å›½AIæŠ¥å‘Šï¼šå…¨é¢å‰–æBATä¸‰å·¨å¤´ä»¥åŠ26ä¸ªæ ¸å¿ƒç©å®¶](https://36kr.com/p/5091725.html)

- ç”¨ [Perplexity](http://Perplexity.ai) ä»£æ›¿ Googleï¼›
- ç”¨ [Poe](https://poe.com/sage) ä»£æ›¿ ChatGPT.
- ç”¨ Promptable ä»£æ›¿ OpenAI Playground.
- ç”¨ [Elicit](https://elicit.org) ä»£æ›¿ Google Scholar

- [æå®¢æ—¶é—´ AI æŒ‡å—](https://zhinan.geekbang.org/)
- å†™æ–‡æ¡ˆç”¨ ChatGPTã€Notion AIæˆ– Jarvisï¼ŒClaudeï¼Œ
  - [Poe](https://poe.com/sage)
  - Slack Claude
  - [æ–‡å¿ƒä¸€è¨€ ç™¾åº¦](https://yiyan.baidu.com/)
  - [é€šä¹‰åƒé—® é˜¿é‡Œ](https://qianwen.aliyun.com/)
  - [Google AI Studio](https://aistudio.google.com/app/prompts/new_chat)
  - [Poe](https://poe.com/sage)
  - [Kimi.ai - å¸®ä½ çœ‹æ›´å¤§çš„ä¸–ç•Œ](https://kimi.moonshot.cn/)
  - [DeepSeek](https://platform.deepseek.com)
  - [è®¯é£æ˜Ÿç«å¤§æ¨¡å‹ æ–‡æ¡ˆ å›¾ç‰‡ PPT ä»£ç ](https://xinghuo.xfyun.cn/desk)
  - [å•†æ±¤å•†é‡è¯­è¨€å¤§æ¨¡å‹ æ³¨å†Œéœ€è¦ä¼ä¸šé‚€è¯·ç ](https://chat.sensetime.com/wb/register)
  - è…¾è®¯æ··å…ƒåŠ©æ‰‹ å¾®ä¿¡å°ç¨‹åº
  - bardï¼Œ
  - Copy aiï¼Œ
  - [Writesonic](https://app.writesonic.comï¼Œ
  - Character AIï¼Œ
  - [Jasper](https://www.jasper.ai/)
  - [Forefront Chat](https://chat.forefront.ai/)
- å›¾åƒå¤„ç†
  - [æ–‡å¿ƒä¸€æ ¼ ç™¾åº¦](https://yige.baidu.com/)
  - [é€šä¹‰ä¸‡ç›¸ é˜¿é‡Œ](https://wanxiang.aliyun.com/creation)
  - Midjourney Discord æˆ– Getimg.aiï¼Œstable diffusionï¼Œdall-e
  - [OmniParser å›¾ç‰‡è§£æ](https://microsoft.github.io/OmniParser/)ï¼šOmniParser is a comprehensive method for parsing user interface screenshots into structured and easy-to-understand elements
  - [ç”Ÿæˆæ’ç”»ã€æ¼«ç”» Infinite AI Artboard - Recraft](https://www.recraft.ai/)
- éŸ³è§†é¢‘è½¬æ–‡æœ¬
  - [é€šä¹‰å¬æ‚Ÿ - é˜¿é‡Œ](https://tingwu.aliyun.com)
  - [Aiko â€” Sindre Sorhus è¯­éŸ³è½¬æ–‡å­—](https://sindresorhus.com/aiko)
  - [MacWhisper](https://goodsnooze.gumroad.com/l/macwhisper) [MacWhisper](https://app.gumroad.com/d/29e33b796f6ce9bb186f87cdf2fadb16)
  - Buzz transcribes and translates audio offline on your personal computer. [Buzz](https://github.com/chidiwilliams/buzz)
  - [AudioPen](https://audiopen.ai/demo)
- æ–‡æœ¬è½¬éŸ³è§†é¢‘
  - éŸ³é¢‘ç”Ÿæˆç”¨ voice.ai æˆ– Eleven Labsï¼Œè¿å­—å¹•éƒ½å¯ä»¥ç”¨è‡ªå·±é€šè¿‡AIâ€œè®­ç»ƒâ€çš„ä¸ªæ€§ä¸“å±å­—ä½“ã€‚
- è§†é¢‘å¤„ç†
  - æŠŠç”»å‡ºæ¥çš„äººç‰©ï¼Œå–‚æ–‡æ¡ˆè®©ä»–å˜æˆé¢‘è§†çš„æ˜¯D-IDï¼›
  - è§†é¢‘åˆ¶ä½œç”¨ Descript æˆ– Runway
  - ä¸€ä¸ªé¢‘è§†ä¸­çš„äººç‰©æ›¿æ¢æˆè™šæ‹Ÿäººæˆ–CGçš„æ˜¯Wonderï¼›
  - ç»™é¢‘è§†é…éŸ³åˆ›å»ºéŸ³ä¹å°±ç”¨Soundrawï¼›
  - [Open-Sora è§†é¢‘ç”Ÿæˆ](https://github.com/hpcaitech/Open-Sora)
  - [Open Sora demo - a Hugging Face Space by hpcai-tech](https://huggingface.co/spaces/hpcai-tech/open-sora)
  - [å¯çµ AI - æ–°ä¸€ä»£ AI åˆ›æ„ç”Ÿäº§åŠ›å¹³å°](https://klingai.kuaishou.com/)
- æ™ºèƒ½åŒ–ç ”å‘
  - [Codeium Â· Free AI Code Completion & Chat](https://codeium.com/faq)
  - [é€šä¹‰çµç _æ™ºèƒ½ç¼–ç åŠ©æ‰‹ é˜¿é‡Œäº‘](https://tongyi.aliyun.com/lingma/download)
  - ç™¾åº¦ Comate 2.0ä»£ç åŠ©æ‰‹ [Baidu Comate Â· Coding mate](https://comate.baidu.com/)
  - [OpenDevin - OpenHands](https://github.com/All-Hands-AI/OpenHands)
  - [yetone/avante.nvim: Use your Neovim like using Cursor AI IDE!](https://github.com/yetone/avante.nvim)
  - [é˜¿é‡Œæ™ºèƒ½åŒ–ç ”å‘ä¸€å¹´å¤ç›˜](https://mp.weixin.qq.com/s/JTpLy8Z0klokHVcaDZm2RQ)
  - [CodeRabbit is an AI-powered code reviewer](https://docs.coderabbit.ai/)
  - [rikvermeulen/co-op-gitlab: Automate code reviews and feedback for GitLab Merge Requests using OpenAI GPT-3/4](https://github.com/rikvermeulen/co-op-gitlab)
  - [Diffblue Cover write Java unit tests](https://docs.diffblue.com/get-started/get-started/get-started-cover-plugin)
- æŠŠçœŸäººå˜æˆè™šæ‹Ÿäººçš„æ˜¯ Meta humanï¼Œæƒ³æ›´ç®€å•å˜è™šæ‹Ÿäººçš„æ˜¯Ready player meï¼›
- PPT:
  - chatgptç”Ÿæˆçš„markdownè¯­æ³•å†…å®¹å¯¼å…¥[MindShow](https://www.mindshow.fun/#/home)
  - [AI PPT Maker - Best Online Free (No Sign up)](https://aipptmaker.ai/en)
  - [å¿«é€ŸåšPPT Gamma - Presentations and Slide Decks with AI | Gamma](https://gamma.app/)
- ç¬¬ä¸‰æ–¹ç¤¾ç¾¤ï¼Œcivitai
- æ•°å­¦ [MathGPT å­¦è€Œæ€](https://www.mathgpt.com/)
- åŒ»ç–—é—®é¢˜å›ç­”[MediSearch](https://medisearch.io/zh)
- æŸ¥æ‰¾æ³•å¾‹æ¡ˆä¾‹ã€æ¡æ–‡ [MetaLaw ç±»æ¡ˆæ£€ç´¢ï¼Œä¸€é”®ç›´è¾¾ï¼Œè®©ä½ çš„æ³•å¾‹ç ”ç©¶æ•ˆç‡å¿«äºº10å€](https://meta.law/)
- åŠå…¬å°æµ£ç†Š:åˆ¶ä½œexcelè¡¨æ ¼ã€‚
- Scribe AI chrome æ’ä»¶ï¼šè‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–æ“ä½œæŒ‡å—, åˆ›å»ºé€æ­¥æŒ‡å—çš„åœºæ™¯ï¼ŒåŒ…æ‹¬æ“ä½œè¯´æ˜ã€æ ‡å‡†æ“ä½œè§„ç¨‹ã€åŸ¹è®­æ‰‹å†Œç­‰ã€‚
- ç¼–æ’
  - [Dify github](https://github.com/langgenius/dify)
  - [TEN Agent is a conversational AI](https://github.com/TEN-framework/TEN-Agent)
  - [RAGä¸ƒåäºŒå¼ï¼š2024å¹´åº¦RAGæ¸…å•](https://mp.weixin.qq.com/s/_pnezCv-sKmzhho7Xw3D2g)

[AI å·¥å…·åˆ—è¡¨ é£ä¹¦æ–‡æ¡£](https://zl49so8lbq.feishu.cn/wiki/wikcn6YTN3CrZTS8RhrEca8c8Eg)
[æå®¢æ—¶é—´ AIGC çŸ¥è¯†åº“](https://gp477l8icq.feishu.cn/wiki/JUXnwzSuviL5E9kh6jUc8FRinHe)
[ModelScopeé­”æ­ç¤¾åŒº æ—¨åœ¨æ‰“é€ ä¸‹ä¸€ä»£å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ï¼Œè®©æ¨¡å‹åº”ç”¨æ›´ç®€å•ã€‚](https://community.modelscope.cn/)
[ima.copilot-è…¾è®¯æ™ºèƒ½å·¥ä½œå°](https://ima.qq.com/) ima.copilotï¼ˆç®€ç§°imaï¼‰æ˜¯ä¸€æ¬¾ç”±è…¾è®¯æ··å…ƒå¤§æ¨¡å‹æä¾›æŠ€æœ¯æ”¯æŒçš„ï¼Œé¢å‘å­¦ä¹ ã€åŠå…¬åœºæ™¯ï¼Œä»¥çŸ¥è¯†åº“ä¸ºæ ¸å¿ƒçš„AIæ™ºèƒ½å·¥ä½œå°ï¼Œæ˜¯è¯»ã€æœã€å†™ä¸€ä½“çš„æ•ˆç‡å·¥å…·

[xtekky/gpt4free: decentralising the Ai Industry, just some language model api's...](https://github.com/xtekky/gpt4free)

| Website s | Model(s) |
| --- | --- |
| [forefront.ai](https://chat.forefront.ai) | GPT-4/3.5 |
| [poe.com](https://poe.com) | GPT-4/3.5 |
| [writesonic.com](https://writesonic.com) | GPT-3.5 / Internet |
| [t3nsor.com](https://t3nsor.com) | GPT-3.5 |
| [you.com](https://you.com) | GPT-3.5 / Internet / good search |
| [sqlchat.ai](https://sqlchat.ai) | GPT-3.5 |
| [bard.google.com](https://bard.google.com) | custom / search |
| [bing.com/chat](https://bing.com/chat) | GPT-4/3.5 |
| [italygpt.it](https://italygpt.it) | GPT-3.5 |

## æ¨¡å‹

### deepseek

deepseek janus pro å¤šæ¨¡æ€å¤§æ¨¡å‹ç‚¸è£‚å‡ºåœºï¼Œtransformeræ¶æ„ï¼Œæ²¡æœ‰èµ°diffusionè·¯çº¿

[DeepSeek Token ç”¨é‡è®¡ç®— | DeepSeek API Docs](https://api-docs.deepseek.com/zh-cn/quick_start/token_usage)

[KTransformers 4090å•å¡è·‘671B DeepSeek-R1 - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/23212558318)
[å•å¡RTX4090éƒ¨ç½²R1æ»¡è¡€ç‰ˆä¹‹KTransformersç¯‡-é˜¿æœ±](https://mp.weixin.qq.com/s/g3JsrLUuMXDX-8lSSzb06A)

[4090å•å¡éƒ¨ç½²QWen2.5-VLè§†è§‰æ¨¡å‹](https://mp.weixin.qq.com/s/Ha-J5uUKk7XUqMfW_VEqHg)

## GPT é¡¹ç›®

GPT (Generative Pre-trained Transformer)

[ChatGPT](https://chat.openai.com/chat)
[What Is ChatGPT Doing â€¦ and Why Does It Work?â€”Stephen Wolfram Writings](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
[ChatGPTä¸ºå•¥è¿™ä¹ˆå¼ºï¼šä¸‡å­—é•¿æ–‡è¯¦è§£ by WolframAlphaä¹‹çˆ¶-ä»Šæ—¥å¤´æ¡](https://www.toutiao.com/article/7200604582392087095)
[ChatGPTå­¦ä¹ èµ„æ–™åˆé›† Original å•å»ºä¼Ÿ é˜¿æœ±è¯´](https://mp.weixin.qq.com/s/ZApy_d873Y1DEmGc6NjplQ)

[å¦‚ä½•ä½¿ç”¨ChatGPT APIè®­ç»ƒè‡ªå®šä¹‰çŸ¥è¯†åº“AIèŠå¤©æœºå™¨äºº - é—ªç”µåš](https://www.wbolt.com/how-to-train-ai-chatbot.html)
[How to Train an AI Chatbot With Custom Knowledge Base Using ChatGPT API | Beebom](https://beebom.com/how-train-ai-chatbot-custom-knowledge-base-chatgpt-api/)
[Fine-tuning - OpenAI API](https://platform.openai.com/docs/guides/fine-tuning)

[ç¬”è®°æœ¬å°±èƒ½è¿è¡Œçš„ChatGPTå¹³æ›¿æ¥äº†ï¼Œé™„å®Œæ•´ç‰ˆæŠ€æœ¯æŠ¥å‘Š-ä»Šæ—¥å¤´æ¡](https://www.toutiao.com/article/7216255969465303613)
[nomic-ai/gpt4all: gpt4all: a chatbot trained on a massive collection of clean assistant data including code, stories and dialogue](https://github.com/nomic-ai/gpt4all)

[ç”¨ç¬”è®°æœ¬è¿è¡Œ650äº¿å‚æ•°å¤§æ¨¡å‹ InfoQç²¾é€‰æ–‡ç« ](https://www.infoq.cn/article/qucNy1wcUq87HCSTjddQ)
[Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp | Simon Willisonâ€™s TILs](https://til.simonwillison.net/llms/llama-7b-m2)
[ggerganov/llama.cpp: Port of Facebook's LLaMA model in C/C++](https://github.com/ggerganov/llama.cpp)

[äººäººéƒ½èƒ½æ‡‚çš„ChatGPTè§£è¯»_AI_å¼ æ°_InfoQç²¾é€‰æ–‡ç« ](https://www.infoq.cn/article/VWrPIRvRg6E3O74q7PtL)

[go-zoox/chatgpt-for-chatbot-feishu: å¿«é€Ÿå°† ChatGPT æ¥å…¥é£ä¹¦ï¼ŒåŸºäº OpenAI å®˜æ–¹æ¥å£ï¼Œä½œä¸ºç§äººå·¥ä½œåŠ©ç†æˆ–è€…ä¼ä¸šå‘˜å·¥åŠ©ç†](https://github.com/go-zoox/chatgpt-for-chatbot-feishu)

[zhayujie/chatgpt-on-wechat: ä½¿ç”¨ChatGPTæ­å»ºå¾®ä¿¡èŠå¤©æœºå™¨äººï¼ŒåŸºäºChatGPT3.5 APIå’Œitchatå®ç°ã€‚Wechat robot based on ChatGPT, which using OpenAI api and itchat library.](https://github.com/zhayujie/chatgpt-on-wechat)

[ConnectAI-E/Dingtalk-OpenAI: ğŸ”” é’‰é’‰ & ğŸ¤– GPT-3.5 è®©ä½ çš„å·¥ä½œæ•ˆç‡ç›´æ¥èµ·é£ ğŸš€ ç§èŠç¾¤èŠæ–¹å¼ã€å•èŠä¸²èŠæ¨¡å¼ã€è§’è‰²æ‰®æ¼”ã€å›¾ç‰‡åˆ›ä½œ ğŸš€](https://github.com/ConnectAI-E/Dingtalk-OpenAI)

[chatgpt-web: åŸºäºChatGPT3.5 APIå®ç°çš„ç§æœ‰åŒ–webç¨‹åº](https://github.com/869413421/chatgpt-web)

[pengzhile/pandora: æ½˜å¤šæ‹‰ï¼Œä¸€ä¸ªè®©ä½ å‘¼å¸é¡ºç•…çš„ChatGPTã€‚Pandora, a ChatGPT that helps you breathe smoothly.](https://github.com/pengzhile/pandora)

[ChatALL](https://github.com/sunner/ChatALL/releases)

### AI åº”ç”¨å¼€å‘

[å¤§æ¨¡å‹ AI åº”ç”¨å…¨æ ˆå¼€å‘çŸ¥è¯†ä½“ç³» v1.3.1 - é£ä¹¦äº‘æ–‡æ¡£](https://agiclass.feishu.cn/docx/Z3Aed6qXboiF8gxGuaccNHxanOc)
[GitHub - deepseek-ai/awesome-deepseek-integration: Integrate the DeepSeek API into popular softwares](https://github.com/deepseek-ai/awesome-deepseek-integration)
[ç«å±±å¼•æ“ é«˜ä»£ç  Python SDK Arkitect volcengine/ai-app-lab](https://github.com/volcengine/ai-app-lab/)

[AI å…¨æ ˆå­¦å‘˜éƒ¨åˆ†ä½œå“é›† - é£ä¹¦äº‘æ–‡æ¡£](https://agiclass.feishu.cn/docx/M5xydPVjWovB9exHBjDc7IMYnub)

[çŸ¥ä¹ã€ŠAI å¤§æ¨¡å‹å…¨æ ˆå·¥ç¨‹å¸ˆã€‹è¯¾ç¨‹å¤§çº²ï¼ˆç¬¬ 05 æœŸï¼‰ - é£ä¹¦äº‘æ–‡æ¡£](https://agiclass.feishu.cn/docx/KjFSdqxTZoDDfcxzikHcjjx0nDg)

[AIå¤§æ¨¡å‹é¢ è¦†ç¨‹åºå‘˜çš„ä»·å€¼ - ç¨‹åºå‘˜çš„AIå¤§æ¨¡å‹è¿›é˜¶ä¹‹æ—…0122æœŸ](https://www.zhihu.com/xen/market/training/training-video/1730906752995045376/1730906966715797506?education_channel_code=ZHZN-d62bb90dfad9e02) è®²è§£äº†è‡ªè®­ç»ƒ å¾®è°ƒç­‰æ–¹æ³•
[å¤§æ¨¡å‹åº”ç”¨å¼€å‘æŠ€æœ¯ä½“ç³»ä¸²è®² - ç¨‹åºå‘˜çš„AIå¤§æ¨¡å‹è¿›é˜¶ä¹‹æ—…0125æœŸ](https://www.zhihu.com/xen/market/training/training-video/1731335160308744192/1731335415037206528?education_channel_code=ZHZN-cd8085beea05e6d) å­™å¿—åˆšè€å¸ˆå›ç­”é—®é¢˜
[ä½¿â½¤ Assistants APIå¿«é€Ÿæ­å»ºé¢†åŸŸä¸“å±AI - ç¨‹åºå‘˜çš„AIå¤§æ¨¡å‹è¿›é˜¶ä¹‹æ—…0122æœŸ](https://www.zhihu.com/xen/market/training/training-video/1730906752995045376/1730907032264232960?education_channel_code=ZHZN-d62bb90dfad9e02)

å¤§æ¨¡å‹é¢†åŸŸå²—ä½æ¨è
 Â· å¤§æ¨¡å‹è®­ç»ƒå¸ˆ
 Â· å¤§æ¨¡å‹ç®—æ³•å·¥ç¨‹å¸ˆ
 Â· æç¤ºè¯å·¥ç¨‹å¸ˆ
 Â· å¤§æ¨¡å‹å…¨æ ˆå¼€å‘å·¥ç¨‹å¸ˆ
 Â· å¤§æ¨¡å‹æ–¹å‘äº§å“ç»ç†
 Â· å¤§æ¨¡å‹æ–¹å‘é¡¹ç›®ç»ç†

ç»§ç»­æœ¬å²—ä½ Â· å¤§å¤§æå‡æ•ˆç‡ï¼Œæ¨ªå‘å·åŒè¡Œï¼Œçºµå‘å·ä¸Šä¸‹æ¸¸
æˆä¸ºè¶…çº§ä¸ªä½“ ç‹¬ç«‹å¼€å‘è€…ï¼Œåšè‡ªå·±çš„å°è€æ¿
æˆä¸ºå¤§æ¨¡å‹è®­ç»ƒå¸ˆ åšå…¬å¸çš„æŠ€æœ¯æ ¸å¿ƒ
ç‹¬ç«‹åˆ›ä¸š å‡­å¤§æ¨¡å‹å‚ç›´è½åœ°èƒ½åŠ›è§£å†³ç‹¬æœ‰åœº

ä½¿ç”¨ Assistants API å¿«é€Ÿæ­å»ºé¢†åŸŸä¸“å±AIåŠ©æ‰‹
å®˜æ–¹Webç•Œé¢å¯ä½“éªŒ/è°ƒè¯• https://platform.openapi/playground
Demoæ¡†æ¶ åŠå…·ä½“å®ç° Streamlit ç®€ä»‹â€” A faster way to build and share data apps

[ChatTTS-ui: ä¸€ä¸ªç®€å•çš„æœ¬åœ°ç½‘é¡µç•Œé¢ï¼Œç›´æ¥ä½¿ç”¨ChatTTSå°†æ–‡å­—åˆæˆä¸ºè¯­éŸ³ï¼ŒåŒæ—¶æ”¯æŒå¯¹å¤–æä¾›APIæ¥å£ã€‚](https://github.com/jianchang512/ChatTTS-ui)

### pengzhile/pandora

[pengzhile/pandora: æ½˜å¤šæ‹‰ï¼Œä¸€ä¸ªè®©ä½ å‘¼å¸é¡ºç•…çš„ChatGPTã€‚Pandora, a ChatGPT that helps you breathe smoothly.](https://github.com/pengzhile/pandora)

[ChatGPT Auth å¸®åŠ© ChatGPT è¢«æ‹’ç”¨æˆ·è·å– Access Tokenã€‚](https://ai.fakeopen.com/auth)

#### docker script

```sh
#!/bin/bash
# docker run -rm -e PANDORA_CLOUD=cloud -e PANDORA_SERVER=0.0.0.0:8899 -p 8899:8899 -d pengzhile/pandora --name=pandora

docker run --detach \
    --hostname 127.0.0.1 \
    --publish 8899:8899 \
    --name pandora \
    --restart always \
    -e TZ=Asia/Shanghai \
    -e PANDORA_CLOUD=cloud \
    -e PANDORA_SERVER=0.0.0.0:8899 \
    --volume /data/pandora:/data \
    pengzhile/pandora
```

### Text-to-SQL

[Canner/WrenAI: Open-source GenBI AI Agent that empowers data-driven teams to chat with their data to generate Text-to-SQL, charts, spreadsheets, reports, and BI](https://github.com/Canner/WrenAI)

[CodePhiliaX/Chat2DB: AI-driven database tool and SQL client, The hottest GUI client, supporting MySQL, Oracle, PostgreSQL, DB2, SQL Server, DB2, SQLite, H2, ClickHouse, and more.](https://github.com/codePhiliaX/Chat2DB)

[vanna-ai/vanna: Chat with your SQL database ğŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG.](https://github.com/vanna-ai/vanna)

## prompt

[Maximizing the Potential of LLMs: A Guide to Prompt Engineering](https://www.ruxu.dev/articles/ai/maximizing-the-potential-of-llms/)

[f/awesome-chatgpt-prompts: This repo includes ChatGPT prompt curation to use ChatGPT better.](https://github.com/f/awesome-chatgpt-prompts)

[æç¤ºè¯æŠ€å·§](https://mp.weixin.qq.com/s/eqIqbbyqlgkCU78SKuZCMw)

### [Learn Prompting](https://learnprompting.org/zh-Hans/docs/intro)

#### Learn Prompting Sample

[ğŸŸ¡ Coding Assistance | Learn Prompting](https://learnprompting.org/docs/basic_applications/coding_assistance)

1. act like a senior developer
2. as a very junior developer
3. You can also dictate that it have a certain area of expertise (e.g., sorting algorithms) or number of years of experience
4. Act as Microsoft SQL Server.

##### English translator and Improver

Prompt: I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is "lovin istanbul and the city"

##### Interviewer

Prompt: I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the position position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is "Hi"

##### English Pronunciation Helper

Prompt: I want you to act as an English pronunciation assistant for Turkish speaking people. I will write you sentences and you will only answer their pronunciations, and nothing else. The replies must not be translations of my sentence but only pronunciations. Pronunciations should use Turkish Latin letters for phonetics. Do not write explanations on replies. My first sentence is "how the weather is in Istanbul?"

##### Travel Guide

Prompt: I want you to act as a travel guide. I will write you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion request is ""I am in Istanbul/BeyoÄŸlu and I want to visit only museums."

## Text to Image

[10æ¬¾AIç»˜ç”»ç”Ÿæˆå™¨ï¼Œäººäººéƒ½æ˜¯æ’ç”»å¸ˆï¼](https://pixso.cn/designskills/10-ai-paint-builders/)

[DALLÂ·E 2](https://openai.com/product/dall-e-2)

[Text To Image - AI Image Generator API | DeepAI](https://deepai.org/machine-learning-model/text2img)

[æ–‡å¿ƒä¸€æ ¼ - AIè‰ºæœ¯å’Œåˆ›æ„è¾…åŠ©å¹³å°](https://yige.baidu.com/creation)

[22ä¸ªå›½å†…AIç»˜ç”»ç½‘ç«™æ±‡æ€»](https://zl49so8lbq.feishu.cn/wiki/MdzYw0mtki3OPGkPk03cnd9hnfg#Uaaidf4Mro82FFx8ucic8MUOnBf)

### image prompt

[Guide for prompt writing | BoostPixels](https://boostpixels.com/guide)
[midjourneyå²ä¸Šæœ€å…¨æ•™ç¨‹-æŒç»­æ›´æ–° - Feishu Docs](https://nw3t0riwqkt.feishu.cn/docx/NCVdd118toPLkRxKBexcQZiunJZ)

[å¢¨æœ¬å…³é”®è¯åŠ©æ‰‹](https://www.mbprompt.com/#/)
[MidJourney Prompt Tool](https://prompt.noonshot.com/)

### Stable Diffusion

[Stable Diffusion Models: a beginner's guide - Stable Diffusion Art](https://stable-diffusion-art.com/models/)
[ControlNet: A Complete Guide - Stable Diffusion Art](https://stable-diffusion-art.com/controlnet/)
[å¸¸ç”¨çš„ControlNetä»¥åŠå¦‚ä½•åœ¨Stable Diffusion WebUIä¸­ä½¿ç”¨ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/620074109)

[Install Stable Diffusion on Mac](https://uxplanet.org/install-stable-diffusion-ui-on-mac-beginners-guide-351e40a9e8e2)
[webui Online Services Â· AUTOMATIC1111/stable-diffusion-webui Wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)
[Stability-AI/generative-models: Generative Models by Stability AI](https://github.com/Stability-AI/generative-models)
[Mikubill/sd-webui-controlnet: WebUI extension for ControlNet](https://github.com/Mikubill/sd-webui-controlnet)

### MidJourney æ¡ˆä¾‹

[ã€Midjourneyæ•™ç¨‹ã€‘è®¾è®¡éº»ç“œä¹Ÿèƒ½10åˆ†é’Ÿä¸Šæ¶ä¸€å¥—è¡¨æƒ…åŒ…](https://mp.weixin.qq.com/s/FagQ3HdAnx-HLfJK4NRMBQ)

### gpt4all

```sh
#  get started with the CPU quantized gpt4all model
./gpt4all-lora-quantized-OSX-m1

```

### llama

```sh
./main -m ./models/7B/ggml-model-q4_0.bin \
  -t 8 \
  -n 128 \
  -p 'The first man on the moon was '

./talk.sh "The first man on the moon was "
```

### Mistral-7B

æ¥è‡ªæ³•å›½çš„å¼€æºå¤§æ¨¡å‹

[æœ€å¥½çš„7Bæ¨¡å‹æ˜“ä¸»ï¼Œå…è´¹å¼€æºå¯å•†ç”¨ï¼Œæ¥è‡ªâ€œæ¬§æ´²çš„OpenAIâ€-ä»Šæ—¥å¤´æ¡](https://www.toutiao.com/article/7287811935905546763/)
[mistralai (Mistral AI_)](https://huggingface.co/mistralai)

## ChatGPT doc

[Playground - OpenAI API](https://platform.openai.com/playground)

[Assistants overview - OpenAI API](https://platform.openai.com/docs/assistants/overview)

## ç†è®ºçŸ¥è¯†

[Deepseekå¤§æ¨¡å‹æ¨ç†ç®—æ³•å…¶å®å¾ˆç®€å• | é™ˆç»](https://mp.weixin.qq.com/s/SaK9mlj6NCKxEFig6KFGVQ)

## æœºå™¨å­¦ä¹ æ•™ç¨‹

[Getting Started With MachineLearning (all in one)](https://pan.baidu.com/s/1tNXYQNadAsDGfPvuuj7_Tw)

[microsoft/ML-For-Beginners: 12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all](https://github.com/microsoft/ML-For-Beginners)

[microsoft/AI-For-Beginners: 12 Weeks, 24 Lessons, AI for All!](https://github.com/microsoft/AI-For-Beginners)

1. [ç¬¬ä¸€ç« ï¼šå…¨ä¸–ç•Œæœ€ç®€å•çš„æœºå™¨å­¦ä¹ å…¥é—¨æŒ‡å—](https://zhuanlan.zhihu.com/p/24339995)
2. [ç¬¬äºŒç« ï¼šç”¨æœºå™¨å­¦ä¹ åˆ¶ä½œè¶…çº§é©¬é‡Œå¥¥çš„å…³å¡](https://zhuanlan.zhihu.com/p/24344720)
3. [ç¬¬ä¸‰ç« :å›¾åƒè¯†åˆ«ã€é¸Ÿoré£æœºã€‘ï¼Ÿæ·±åº¦å­¦ä¹ ä¸å·ç§¯ç¥ç»ç½‘ç»œ] https://zhuanlan.zhihu.com/p/24524583
4. [ç¬¬å››ç« ï¼šç”¨æ·±åº¦å­¦ä¹ è¯†åˆ«äººè„¸](https://zhuanlan.zhihu.com/p/24567586)
5. [ç¬¬äº”ç« ï¼šGoogle ç¿»è¯‘èƒŒåçš„é»‘ç§‘æŠ€ï¼šç¥ç»ç½‘ç»œå’Œåºåˆ—åˆ°åºåˆ—å­¦ä¹ ](https://zhuanlan.zhihu.com/p/24590838)
6. [ç¬¬å…­ç« ï¼šå¦‚ä½•ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Ÿ](https://zhuanlan.zhihu.com/p/24703268)

### å¬å›ç‡å’Œç²¾ç¡®ç‡

[ç®—æ³•å·¥ç¨‹å¸ˆè¯´çš„å¬å›æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ - æ‹’æµ·ç©ºé—´](https://refusea.com/?p=1546)

åœ¨ç®—æ³•å·¥ç¨‹ä¸­ï¼Œ"å¬å›"æ˜¯ä¸€ä¸ªé‡è¦çš„æ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯æ£€ç´¢å’Œæœºå™¨å­¦ä¹ é¢†åŸŸã€‚

å¬å›ç‡ï¼ˆRecallï¼‰æ˜¯ä¸€ç§è¡¡é‡æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„æŒ‡æ ‡ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹è¯†åˆ«å‡ºç›¸å…³å®ä¾‹çš„èƒ½åŠ›ã€‚

å…·ä½“æ¥è¯´ï¼Œ**å¬å›ç‡æ˜¯æŒ‡æ¨¡å‹æ­£ç¡®è¯†åˆ«å‡ºçš„æ­£ä¾‹ï¼ˆçœŸæ­£ä¾‹ï¼‰å æ‰€æœ‰å®é™…æ­£ä¾‹ï¼ˆçœŸæ­£ä¾‹+å‡åä¾‹ï¼‰çš„æ¯”ä¾‹**ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯æ¨¡å‹æ‰¾åˆ°çš„ç›¸å…³å®ä¾‹å æ‰€æœ‰ç›¸å…³å®ä¾‹çš„æ¯”ä¾‹ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªç”¨äºæ£€æµ‹åƒåœ¾é‚®ä»¶çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå¬å›ç‡å°±æ˜¯æ¨¡å‹æ­£ç¡®æ ‡è®°ä¸ºåƒåœ¾é‚®ä»¶çš„é‚®ä»¶æ•°é‡å æ‰€æœ‰å®é™…åƒåœ¾é‚®ä»¶æ•°é‡çš„æ¯”ä¾‹ã€‚

å¬å›ç‡å’Œç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰é€šå¸¸ä¸€èµ·ä½¿ç”¨ï¼Œä»¥è·å¾—æ¨¡å‹æ€§èƒ½çš„å…¨é¢è§†å›¾ã€‚

**ç²¾ç¡®ç‡æ˜¯æ¨¡å‹é¢„æµ‹ä¸ºæ­£ä¾‹çš„å®ä¾‹ä¸­å®é™…ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹**ï¼Œè€Œå¬å›ç‡åˆ™å…³æ³¨æ¨¡å‹èƒ½æ‰¾åˆ°å¤šå°‘å®é™…çš„æ­£ä¾‹ã€‚

ä¾‹å­
å¦‚æœæœ‰ 1000 é‚®ä»¶éœ€è¦æ£€æµ‹ï¼Œç®—æ³•æ£€æµ‹å‡ºæœ‰ 800 åƒåœ¾é‚®ä»¶ï¼Œå®é™…è¿™ 800 é‡ŒçœŸæ­£çš„åƒåœ¾é‚®ä»¶æ˜¯ 600ï¼ŒåŒæ—¶ç®—æ³•è¿˜é—æ¼äº† 50 åƒåœ¾é‚®ä»¶ã€‚é‚£ä¹ˆå¬å›ç‡å’Œç²¾ç¡®ç‡æ˜¯å¤šå°‘ï¼Ÿæ€ä¹ˆè®¡ç®—çš„ï¼Ÿ

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆå®šä¹‰ä»¥ä¸‹å‡ ä¸ªæ¦‚å¿µï¼š

çœŸæ­£ä¾‹ï¼ˆTrue Positiveï¼ŒTPï¼‰ï¼šç®—æ³•æ­£ç¡®åœ°é¢„æµ‹ä¸ºåƒåœ¾é‚®ä»¶çš„é‚®ä»¶æ•°é‡ï¼Œå³600å°ã€‚
å‡æ­£ä¾‹ï¼ˆFalse Positiveï¼ŒFPï¼‰ï¼šç®—æ³•é”™è¯¯åœ°é¢„æµ‹ä¸ºåƒåœ¾é‚®ä»¶çš„é‚®ä»¶æ•°é‡ï¼Œå³800ï¼ˆç®—æ³•é¢„æµ‹ä¸ºåƒåœ¾é‚®ä»¶çš„æ•°é‡ï¼‰- 600ï¼ˆçœŸæ­£çš„åƒåœ¾é‚®ä»¶æ•°é‡ï¼‰= 200å°ã€‚
å‡åä¾‹ï¼ˆFalse Negativeï¼ŒFNï¼‰ï¼šç®—æ³•é”™è¯¯åœ°é¢„æµ‹ä¸ºéåƒåœ¾é‚®ä»¶çš„é‚®ä»¶æ•°é‡ï¼Œå³é—æ¼çš„åƒåœ¾é‚®ä»¶æ•°é‡ï¼Œå³50å°ã€‚
æ ¹æ®è¿™äº›å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å¬å›ç‡å’Œç²¾ç¡®ç‡ï¼š

å¬å›ç‡ï¼ˆRecallï¼‰= çœŸæ­£ä¾‹ / (çœŸæ­£ä¾‹ + å‡åä¾‹) = 600 / (600 + 50) = 0.923ï¼Œæˆ–è€…è¯´92.3%ã€‚
ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰= çœŸæ­£ä¾‹ / (çœŸæ­£ä¾‹ + å‡æ­£ä¾‹) = 600 / (600 + 200) = 0.75ï¼Œæˆ–è€…è¯´75%ã€‚
æ‰€ä»¥ï¼Œè¿™ä¸ªåƒåœ¾é‚®ä»¶æ£€æµ‹ç®—æ³•çš„å¬å›ç‡æ˜¯92.3%ï¼Œç²¾ç¡®ç‡æ˜¯75%ã€‚

### ç®€ä»‹

çº¿æ€§å›å½’ä¸»è¦ç”¨æ¥è§£å†³è¿ç»­å€¼é¢„æµ‹çš„é—®é¢˜ï¼Œé€»è¾‘å›å½’ç”¨æ¥è§£å†³åˆ†ç±»çš„é—®é¢˜ï¼Œè¾“å‡ºçš„å±äºæŸä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œå·¥ä¸šç•Œç»å¸¸ä¼šç”¨é€»è¾‘å›å½’æ¥åšæ’åº

## LLM ä»‹ç»

[Getting Started With Large Language Models - DZone Refcardz](https://dzone.com/refcardz/getting-started-with-large-language-models)

æ–‡å¿ƒå¤§æ¨¡å‹ERNIEæ˜¯ç™¾åº¦å‘å¸ƒçš„äº§ä¸šçº§çŸ¥è¯†å¢å¼ºå¤§æ¨¡å‹ï¼Œæ¶µç›–äº†NLPå¤§æ¨¡å‹å’Œè·¨æ¨¡æ€å¤§æ¨¡å‹ã€‚

https://github.com/PaddlePaddle/ERNIE

## AI åœ¨ç ”å‘åœºæ™¯è½åœ°çš„ç°çŠ¶

- **æ™ºèƒ½ç ”å‘æ’ä»¶**ï¼šä»¥ Github Copilot/ é€šä¹‰çµç  /Comate ä¸ºä»£è¡¨ï¼Œä¸»è¦ä»¥ JetBrainsã€VSCode ä¸ºæ’ä»¶å½¢å¼ä¸ºç”¨æˆ·æä¾›ä»£ç è¡¥å…¨ä¸ºä¸»çš„æ™ºèƒ½ç¼–ç æœåŠ¡
- **AI Native çš„ IDE**ï¼šä»¥ Cursorã€Windsurfã€MarsCode ä¸ºä»£è¡¨ï¼Œä»¥ç‹¬ç«‹ IDE çš„æ–¹å¼ä¸ºå¼€å‘è€…æä¾›æœåŠ¡ï¼Œè€Œæœ‰ä¸€äº›å…¬å¸å¦‚ PearAI å·²ç»å¼€å§‹èµ°å¼€æºè·¯å¾„ï¼Œä»–ä»¬çš„å…±åŒç‰¹ç‚¹æ˜¯ä»¥ VSCode ä¸ºæŠ€æœ¯åº•åº§è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œå¥½å¤„æ˜¯èƒ½æå¤§ç¨‹åº¦ä¸Šåˆ©ç”¨ VSCode çš„æ’ä»¶å’Œå¼€æºç”Ÿæ€
- **CodeReview æ™ºèƒ½åŒ–**ï¼šè¿™ä¸ªé¢†åŸŸèµ·æ­¥æ¯”è¾ƒæ—©ï¼Œä½†æ•ˆæœå§‹ç»ˆä¸€èˆ¬ï¼Œè¿˜éœ€è¦å¾ˆé•¿æ—¶é—´çš„æ‘¸ç´¢ï¼Œé˜¿é‡Œå†…éƒ¨å¾ˆæ—©å°±å¯åŠ¨äº†è¿™ä¸ªé¡¹ç›®ï¼Œä½†æ•ˆæœå¹¶ä¸æ˜¾è‘—ï¼Œè¿™é‡Œæ—¢å­˜åœ¨æ¨¡å‹çš„èƒ½åŠ›çš„é—®é¢˜ï¼Œä¹Ÿå­˜åœ¨å·¥ç¨‹åŒ–ä¸è¶³çš„é—®é¢˜
- **RAG æœç´¢åœºæ™¯**ï¼šRAG å…¶å®è§£å†³çš„æ˜¯æœç´¢å’Œ Summary çš„é—®é¢˜ï¼Œä¾‹å¦‚çŸ¥è¯†æœç´¢ï¼Œæ™ºèƒ½ç­”ç–‘ï¼Œä½†ä¹Ÿå­˜åœ¨éå¸¸å¤§çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç”¨æˆ·é—®é¢˜çš„ä¸Šä¸‹æ–‡ä¸è¶³ï¼ŒçŸ¥è¯†ä¸ä¿é²œï¼Œä¿¡æ¯ä¸å®Œæ•´ï¼Œå¾ˆéš¾è¯„æµ‹ï¼Œç­‰ç­‰ï¼Œä½†ç”±äºå…¶é—¨æ§›æ¯”è¾ƒä½ï¼Œåè€Œæ˜¯å¤§å¤šæ•°å›¢é˜Ÿä¼šé¦–å…ˆæ¶‰è¶³çš„é¢†åŸŸ
- **å…¶ä»–çš„åœºæ™¯**ï¼šä¾‹å¦‚æ™ºèƒ½è§£å†³ä»£ç å†²çªï¼Œè‡ªåŠ¨è§£å†³ç¼–è¯‘é—®é¢˜ç­‰ä¹Ÿéƒ½åœ¨é˜¿é‡Œå†…éƒ¨å¹³å°æ—©å·²ä¸Šçº¿ï¼Œæ™ºèƒ½è¯Šæ–­ï¼Œæ™ºèƒ½ç›‘æ§ç­‰å‡æœ‰äººåœ¨è°ƒç ”ä¸­
- **å±€éƒ¨æ™ºèƒ½åŒ–çš„ Agent**ï¼šä»¥ Gru.ai ç­‰äº§å“ä¸ºä»£è¡¨å¸®åŠ©ç”¨æˆ·ç”Ÿæˆå•å…ƒæµ‹è¯•ï¼Œä»¥ readme-ai ä¸ºä»£è¡¨å¸®åŠ©å¼€å‘è€…ç”Ÿæˆ Readmeï¼Œä»¥ RepoAgent ä¸ºä»£è¡¨å¸®ç”¨æˆ·è¡¥å……æ³¨é‡Šç­‰ç­‰ï¼Œè€Œé˜¿é‡Œåœ¨å†…éƒ¨ä¹Ÿè¿˜å®ç°äº†å¸®åŠ©ç”¨æˆ·æŒ‰æ•´ä¸ªä»“åº“ç”Ÿæˆæ³¨é‡Šï¼Œç”Ÿæˆå•å…ƒæµ‹è¯•çš„ Agent ï¼Œè¿™ç±» Agent çš„ç‰¹ç‚¹æ˜¯åœºæ™¯æ¯”è¾ƒæ¯”è¾ƒå‚ç›´ç®€å•ï¼Œé—®é¢˜ä¸å‘æ•£ï¼ŒæˆåŠŸç‡æ¯”è¾ƒé«˜
- **å¹¿æ³›è‡ªåŠ¨åŒ–çš„ Agent**ï¼šä»¥ Devinã€OpenDevin ä¸ºä»£è¡¨ï¼Œä»¥ SWE-bench ä¸ºä¸»è¦è¯„æµ‹é›†çš„æ–¹å¼ï¼Œåˆ©ç”¨å¤§æ¨¡å‹ç”Ÿæˆå®ç°ä¸€ä¸ªä»»åŠ¡çš„ planï¼Œå¹¶è°ƒç”¨å·¥å…·ï¼Œåœ¨ä¸€ä¸ªç‹¬ç«‹çš„å®¹å™¨å†…æ‰§è¡Œï¼Œå¹¶ä¸”èƒ½å’Œç”¨æˆ·äº¤äº’çš„æ–¹å¼æ¥å®ç°ä¸€äº›ç®€å•çš„ issue æˆ–è€…éœ€æ±‚

## AI ç¼–ç åŠ©æ‰‹

[What else should determine my model use in Cline? : r/ChatGPTCoding](https://www.reddit.com/r/ChatGPTCoding/comments/1hsx76e/what_else_should_determine_my_model_use_in_cline/)

### Continue

[Context providers | Continue](https://docs.continue.dev/customize/context-providers#gitlab-merge-request)

add context: Ctrl+I

### Cline

[cline: Autonomous coding agent](https://github.com/cline/cline)

[VSCode + Cline + VLLM + Qwen2.5 = Fast : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1gbb2de/vscode_cline_vllm_qwen25_fast/)

[set up cline and LLM](https://www.reddit.com/r/LocalLLaMA/comments/1gbb2de/comment/ltkf1z3)

```sh
docker run --runtime nvidia --gpus all \
-v ~/.cache/huggingface:/root/.cache/huggingface \
--ipc=host -p 8000:8000 \
vllm/vllm-openai \
--model Qwen/Qwen2.5-32B-Instruct-AWQ  --tensor-parallel-size 2 \
--quantization awq_marlin --enable-auto-tool-choice --tool-call-parser hermes \
--kv-cache-dtype fp8_e5m2 \
--rope-scaling '{ "factor": 4.0, "original_max_position_embeddings": 32768, "type": "yarn" }'
```

Install the Cline extension onto VSCode: https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev

Cline ä½¿ç”¨ GitHub Copilotï¼šAPI Provider > vscode lm api > Language Model > github copilot

### Gemini Code Assist

[Gemini Code Assist for business | Google Cloud](https://codeassist.google/products/business)

## ollama

[ollama/ollama: Get up and running with large language models.](https://github.com/ollama/ollama)
[ollama/ollama - Docker Image | Docker Hub](https://hub.docker.com/r/ollama/ollama)
[ollama Importing a model](https://github.com/ollama/ollama/blob/main/docs/import.md)
[ollama api](https://github.com/ollama/ollama/blob/main/docs/api.md)

```sh
docker run -d --env OLLAMA_HOST=0.0.0.0:11434 -v /data/docker/ollama/ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
alias ollama='docker exec -it ollama ollama'

# Show model information
ollama show llama3.2
# List models on your computer
ollama list
# List which models are currently loaded
ollama ps
# Start Ollama
ollama serve

# Run a model
ollama run modelName
# Show more info: duration and eval rate
ollama run modelName --verbose
# Stop a running model
ollama stop modelName
# Remove a model
ollama rm modelName

#
# Run with parameter: num_ctx Context Window
ollama run llama3 â€“set parameter num_ctx 4096 --temperature 0.7 --top-p 0.9 --memory-limit 8GB --batch-size 8
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
ollama run llama2 --debug
# ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·
ollama run llama2 --profile

# ä½¿ç”¨curlæµ‹è¯•API
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello, how are you?"
}'

# æ‰¹å¤„ç†
ollama run llama2 < batch_prompts.txt > responses.txt

# [ollama Model library](https://ollama.com/library)

# deepseek
# deepseek 1.5b ~ 32b ä¸Šä¸‹æ–‡é•¿åº¦ 32,768  æœ€å¤§è¾“å…¥ 32,768
# [DeepSeek R1å’ŒDeepSeek V3 API_å¤§æ¨¡å‹æœåŠ¡å¹³å°ç™¾ç‚¼(Model Studio)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/model-studio/developer-reference/deepseek#94082c580cot9)
# [deepseek-r1:1.5b](https://ollama.com/library/deepseek-r1:1.5b)
# DeepSeek-R1-Distill-Qwen-1.5B
ollama run deepseek-r1:1.5b
# DeepSeek-R1-Distill-Qwen-7B
ollama run deepseek-r1:7b
# DeepSeek-R1-Distill-Qwen-32B
ollama run deepseek-r1:32b

# DeepSeek-R1-Distill-Llama-70B
# Error: model requires more system memory (37.3 GiB) than is available (24.7 GiB)
ollama run deepseek-r1:70b

# 34b æ‰§è¡Œå¤ªæ…¢
ollama run codellama:34b

# 70b Error: model requires more system memory (31.2 GiB) than is available (27.3 GiB)
ollama run codellama:70b

# Cline uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.5 Sonnet for its advanced agentic coding capabilities.
ollama run qwen2.5:32b

# [DeepSeek Coder](https://deepseekcoder.github.io/)
# [DeepSeek-Coder/Evaluation/HumanEval - deepseek-ai/DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder/tree/main/Evaluation/HumanEval)
#
# DeepSeek-Coder-V2 comes in two primary types: Instruct and Base.
# Base model
# A base model is a general-purpose language model trained on a large corpus of text (e.g., code, documentation, and natural language). It has no specific fine-tuning for instruction-following or task-oriented behaviour.
#
# Instruct model
# An instruct model is a fine-tuned version of a base model, optimized to follow instructions and perform specific tasks. It is trained on datasets containing instruction-response pairs (e.g., â€œWrite a SQL query to find duplicatesâ€ â†’ â€œSELECT â€¦â€). Excels at task-oriented interactions (e.g., debugging, refactoring, answering questions).

# [second-state-DeepSeek-Coder-V2-Lite-Instruct-GGUF: Mirror of https://huggingface.co/second-state/DeepSeek-Coder-V2-Lite-Instruct-GGUF](https://gitee.com/hf-models/second-state-DeepSeek-Coder-V2-Lite-Instruct-GGUF)
# DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf  Quant method: Q4_K_M
# medium, balanced quality - recommended
# DeepSeek-Coder-V2-Instruct 236B
# DeepSeek-Coder-V2-Lite-Instruct 16B
ollama run deepseek-coder-v2:16b-lite-instruct-q4_K_M

# [MFDoom/deepseek-r1-tool-calling:8b](https://ollama.com/MFDoom/deepseek-r1-tool-calling:8b)
# DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen. With Tool Calling support.
ollama run MFDoom/deepseek-r1-tool-calling:8b
```

### ollama å‘½ä»¤

[ollamaçš„å‘½ä»¤æ³¨è§£_ollamaå‘½ä»¤è¡Œ-CSDNåšå®¢](https://blog.csdn.net/sunyuhua_keyboard/article/details/141174683)

```sh
>>> /?
Available Commands:
  /set            Set session variables
  /show           Show model information
  /load <model>   Load a session or model
  /save <model>   Save your current session
  /clear          Clear session context
  /bye            Exit
  /?, /help       Help for a command
  /? shortcuts    Help for keyboard shortcuts

Use """ to begin a multi-line message.

>>> /set
Available Commands:
  /set parameter ...     Set a parameter
  /set system <string>   Set system message
  /set template <string> Set prompt template
  /set history           Enable history
  /set nohistory         Disable history
  /set wordwrap          Enable wordwrap
  /set nowordwrap        Disable wordwrap
  /set format json       Enable JSON mode
  /set noformat          Disable formatting
  /set verbose           Show LLM stats
  /set quiet             Disable LLM stats
```

ä¸»å‘½ä»¤
/set: ç”¨äºè®¾ç½®ä¼šè¯å‚æ•°å’Œé…ç½®ã€‚ä¾‹å¦‚ï¼Œè®¾ç½®æ¶ˆæ¯æ ¼å¼ã€å¯ç”¨æˆ–ç¦ç”¨å†å²è®°å½•ç­‰ã€‚
/show: æ˜¾ç¤ºæ¨¡å‹çš„ç›¸å…³ä¿¡æ¯ï¼Œå¦‚å½“å‰åŠ è½½çš„æ¨¡å‹çš„åç§°ã€ç‰ˆæœ¬ç­‰ã€‚
/load : åŠ è½½ä¸€ä¸ªç‰¹å®šçš„æ¨¡å‹æˆ–ä¼šè¯ã€‚ä½ å¯ä»¥æŒ‡å®šä¸€ä¸ªæ¨¡å‹çš„åç§°æˆ–è·¯å¾„æ¥åŠ è½½å®ƒã€‚
/save : ä¿å­˜å½“å‰çš„ä¼šè¯çŠ¶æ€æˆ–æ¨¡å‹ã€‚ä½ å¯ä»¥å°†å½“å‰ä¼šè¯æˆ–æ¨¡å‹çš„é…ç½®ä¿å­˜ä¸ºä¸€ä¸ªæ–‡ä»¶ï¼Œä»¥ä¾¿ä»¥åä½¿ç”¨ã€‚
/clear: æ¸…é™¤ä¼šè¯ä¸Šä¸‹æ–‡ã€‚è¿™å°†åˆ é™¤å½“å‰ä¼šè¯ä¸­çš„æ‰€æœ‰å†å²è®°å½•æˆ–å¯¹è¯å†…å®¹ã€‚
/bye: é€€å‡ºä¼šè¯ã€‚è¿™ä¸ªå‘½ä»¤å°†ç»“æŸå½“å‰ä¸æ¨¡å‹çš„å¯¹è¯ï¼Œå¹¶é€€å‡ºç¨‹åºã€‚
/? æˆ– /help: æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯ã€‚å¦‚æœä½ éœ€è¦å…³äºæŸä¸ªå‘½ä»¤çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥ä½¿ç”¨è¿™äº›å‘½ä»¤ã€‚
/? shortcuts: æ˜¾ç¤ºé”®ç›˜å¿«æ·é”®çš„å¸®åŠ©ä¿¡æ¯ã€‚è¿™å¯ä»¥å¸®åŠ©ä½ æ›´å¿«é€Ÿåœ°è¿›è¡Œæ“ä½œã€‚

/set å­å‘½ä»¤
/set parameter â€¦: è®¾ç½®æŸä¸ªå‚æ•°ã€‚è¿™å¯èƒ½åŒ…æ‹¬ä¸€äº›ç‰¹å®šçš„é…ç½®é¡¹ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºæˆ–è¾“å‡ºæ–¹å¼ã€‚
/set system : è®¾ç½®ç³»ç»Ÿæ¶ˆæ¯ã€‚ä½ å¯ä»¥æä¾›ä¸€ä¸ªå­—ç¬¦ä¸²ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯ï¼Œè¿™é€šå¸¸ç”¨äºåœ¨å¯¹è¯å¼€å§‹æ—¶å‘æ¨¡å‹ä¼ è¾¾èƒŒæ™¯ä¿¡æ¯æˆ–ç‰¹å®šæŒ‡ä»¤ã€‚
/set template : è®¾ç½®æç¤ºæ¨¡æ¿ã€‚è¿™å…è®¸ä½ å®šä¹‰ä¸€ä¸ªæ¨¡æ¿ï¼Œç”¨äºæ ¼å¼åŒ–ä½ ä¸æ¨¡å‹çš„å¯¹è¯ã€‚
/set history: å¯ç”¨å†å²è®°å½•ã€‚è¿™æ„å‘³ç€æ¨¡å‹ä¼šä¿å­˜ä½ å½“å‰ä¼šè¯ä¸­çš„å¯¹è¯å†å²ï¼Œä»¥ä¾¿ç¨åå‚è€ƒæˆ–ä½¿ç”¨ã€‚
/set nohistory: ç¦ç”¨å†å²è®°å½•ã€‚ä½¿ç”¨è¿™ä¸ªå‘½ä»¤åï¼Œæ¨¡å‹å°†ä¸ä¼šä¿å­˜ä¼šè¯å†å²ã€‚
/set wordwrap: å¯ç”¨è‡ªåŠ¨æ¢è¡Œã€‚è¿™åœ¨é•¿æ–‡æœ¬æ¶ˆæ¯çš„æƒ…å†µä¸‹éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥è®©æ–‡æœ¬è‡ªåŠ¨æ¢è¡Œä»¥ä¾¿äºé˜…è¯»ã€‚
/set nowordwrap: ç¦ç”¨è‡ªåŠ¨æ¢è¡Œã€‚å¦‚æœä¸éœ€è¦è‡ªåŠ¨æ¢è¡Œï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ã€‚
/set format json: å¯ç”¨JSONæ¨¡å¼æ ¼å¼åŒ–è¾“å‡ºã€‚è¿™ä¼šå°†æ¨¡å‹çš„å“åº”æ ¼å¼åŒ–ä¸ºJSONæ ¼å¼ï¼Œæ–¹ä¾¿ç»“æ„åŒ–æ•°æ®çš„å¤„ç†ã€‚
/set noformat: ç¦ç”¨æ ¼å¼åŒ–è¾“å‡ºã€‚å¦‚æœä¸éœ€è¦ä»»ä½•ç‰¹å®šæ ¼å¼çš„è¾“å‡ºï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ã€‚
/set verbose: å¯ç”¨è¯¦ç»†æ¨¡å¼ï¼Œè¿™ä¼šæ˜¾ç¤ºä¸LLMç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚å“åº”æ—¶é—´ã€æ¶ˆè€—èµ„æºç­‰ã€‚
/set quiet: ç¦ç”¨è¯¦ç»†æ¨¡å¼ã€‚å¯ç”¨åï¼Œå°†ä¸ä¼šæ˜¾ç¤ºä¸LLMç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè¾“å‡ºä¼šæ›´ç®€æ´ã€‚

åº”ç”¨åœºæ™¯

- ç®¡ç†ä¼šè¯: ä½ å¯ä»¥ä½¿ç”¨ /load å’Œ /save å‘½ä»¤æ¥ä¿å­˜å’ŒåŠ è½½ç‰¹å®šçš„ä¼šè¯çŠ¶æ€ï¼Œä»è€Œåœ¨ä¸åŒæ—¶é—´ç‚¹ç»§ç»­å…ˆå‰çš„å·¥ä½œã€‚
- è‡ªå®šä¹‰æ¶ˆæ¯æ ¼å¼: ä½¿ç”¨ /set template å’Œ /set format json å¯ä»¥è‡ªå®šä¹‰å’Œæ§åˆ¶æ¨¡å‹è¾“å‡ºçš„æ ¼å¼ï¼Œé€‚ç”¨äºä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚
- è°ƒè¯•å’Œæ€§èƒ½ç›‘æ§: é€šè¿‡ /set verbose å’Œ /set quietï¼Œä½ å¯ä»¥æ§åˆ¶æ˜¯å¦æŸ¥çœ‹æ¨¡å‹çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè¿™åœ¨è°ƒè¯•æˆ–æ€§èƒ½ç›‘æ§æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚

è¿™äº›å‘½ä»¤å’Œè®¾ç½®å¯ä»¥å¸®åŠ©ä½ æ›´çµæ´»åœ°æ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºå’Œä¼šè¯çš„ç®¡ç†ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ä½ çš„ä½¿ç”¨éœ€æ±‚ã€‚

### Where are models stored?

macOS: ~/.ollama/models
Linux: /usr/share/ollama/.ollama/models
Windows: %userprofile%\.ollama\models

é»˜è®¤çš„æ¨¡å‹ä¿å­˜è·¯å¾„ä½äºCç›˜ï¼Œï¼ˆ%userprofile%\.ollama\modelsï¼‰ï¼Œå¯ä»¥é€šè¿‡è®¾ç½® OLLAMA_MODELS è¿›è¡Œä¿®æ”¹ï¼Œç„¶åé‡å¯ç»ˆç«¯ï¼Œé‡å¯ollamaæœåŠ¡ï¼ˆéœ€è¦å»çŠ¶æ€æ é‡Œå…³æ‰ç¨‹åºï¼‰

```bat
setx OLLAMA_MODELS "D:\ollama_model"
```

### ollama Setting environment variables on Windows

[ollama/docs/faq](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-windows)

On Windows, Ollama inherits your user and system environment variables.

1. First Quit Ollama by clicking on it in the task bar.
2. Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for environment variables.
3. Click on Edit environment variables for your account.
4. Edit or create a new variable for your user account for OLLAMA_HOST, OLLAMA_MODELS, etc.
   1. OLLAMA_HOST=0.0.0.0:11434
5. Click OK/Apply to save.
6. Start the Ollama application from the Windows Start menu.

### keep a model loaded in memory or make it unload immediately?

The `keep_alive` parameter can be set to:

- a duration string (such as "10m" or "24h")
- a number in seconds (such as 3600)
- any negative number which will keep the model loaded in memory (e.g. -1 or "-1m")
- '0' which will unload the model immediately after generating a response

```sh
# to preload a model and leave it in memory use:
curl http://10.10.65.77:11434/api/generate -d '{"model": "deepseek-r1:7b", "keep_alive": -1}'

# To unload the model and free up memory use:
curl http://10.10.65.77:11434/api/generate -d '{"model": "MFDoom/deepseek-r1-tool-calling:8b", "keep_alive": 0}'
```

### config Ollama

#### context window size

[How to Increase Ollama Context Size: A Complete Step-by-Step Guide - Deep AI â€” Leading Generative AI-powered Solutions for Business](https://deepai.tn/glossary/ollama/how-increase-ollama-context-size/)
[GGUF specification](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)
huggingface ä¸Šæˆ–æ¨¡å‹çš„ config æ–‡ä»¶ä¸­è®°å½•çš„å¤§å°ï¼šsequence_len: 4096

By default, Ollama uses a context window size of 2048 tokens.

show the context size *really is* in the current model being run `ollama show (model name)`. In the ollama CLI, `/show info`

```sh
ollama show deepseek-r1:7b
# Model
#   architecture        qwen2
#   parameters          7.6B
#   context length      131072
#   embedding length    3584
#   quantization        Q4_K_M

# Parameters
#   stop    "<ï½œbeginâ–ofâ–sentenceï½œ>"
#   stop    "<ï½œendâ–ofâ–sentenceï½œ>"
#   stop    "<ï½œUserï½œ>"
#   stop    "<ï½œAssistantï½œ>"

# When using the API, specify the num_ctx parameter:
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "options": {
    "num_ctx": 4096
  }
}'

/set parameter num_ctx 131072
/save deepseek-r1:7b-ctx-128k

/set parameter num_ctx 65536
/save deepseek-r1:7b-ctx-64k
/set parameter num_ctx 32768
/save deepseek-r1:7b-ctx-32k
/set parameter num_ctx 16384
/save deepseek-r1:7b-ctx-16k
/set parameter num_ctx 8192
/save deepseek-r1:7b-ctx-8k
/set parameter num_ctx 6144
/save deepseek-r1:7b-ctx-6k
/set parameter num_ctx 4096
/save deepseek-r1:7b-ctx-4k
/set parameter num_ctx 3072
/save deepseek-r1:7b-ctx-3k
/set parameter num_ctx 2048
/save deepseek-r1:7b-ctx-2k


ollama ps
# NAME              ID              SIZE      PROCESSOR    UNTIL
# deepseek-r1:7b-ctx-128k    946b57ccb619    12 GB    100% CPU     4 minutes from now
# deepseek-r1:7b-ctx-64k    af0516d55326    13 GB    58%/42% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-64k    af0516d55326    8.4 GB    100% CPU     4 minutes from now  2025å¹´3æœˆ3æ—¥
# deepseek-r1:7b-ctx-32k    1b1debea5066    9.5 GB    39%/61% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-16k    3fc83f5dea49    7.2 GB    19%/81% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-8k    fdb679a1bd50    6.3 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-6k    edd7d65f4ea1    6.1 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-4k    96cae7f73d2b    6.0 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-3k    25f4b7c66be3    5.5 GB    100% GPU     4 minutes from now
# deepseek-r1:7b-ctx-2k    9be990020b49    5.4 GB    100% GPU     4 minutes from now
# deepseek-r1:7b    0a8c26691023    5.4 GB    100% GPU     4 minutes from now
```

### cline with local deepseek

æœ¬åœ°å…è®¸ deepseek 32b æ¨¡å‹ï¼Œcline è°ƒç”¨æ—¶æŠ¥é”™
[Cline is having trouble... Â· Issue #1094 Â· cline/cline](https://github.com/cline/cline/issues/1094)

```log
Cline uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.5 Sonnet for its advanced agentic coding capabilities.

```

## æ¨¡å‹éƒ¨ç½²å·¥å…·

Ollamaï¼šé€‚åˆä¸ªäºº + æœ¬åœ°éƒ¨ç½² + è½»é‡ä½“éªŒ
vLLMï¼šé€‚åˆä¼ä¸šçº§ + æœåŠ¡å™¨éƒ¨ç½² + é«˜æ€§èƒ½æ‰©å±•

[å¯¹æ¥æœ¬åœ°å¤§æ¨¡å‹æ—¶ï¼Œé€‰æ‹© Ollma è¿˜æ˜¯ vLLMï¼Ÿ - OSCHINA - ä¸­æ–‡å¼€æºæŠ€æœ¯äº¤æµç¤¾åŒº](https://www.oschina.net/news/321572)

### æ¨¡å‹æ˜¾å­˜ä½¿ç”¨é‡è®¡ç®—

[æ¨¡å‹æ˜¾å­˜ä½¿ç”¨é‡è®¡ç®— â€” Xinference](https://inference.readthedocs.io/zh-cn/stable/models/model_memory.html)
[LLM Model VRAM Calculator - a Hugging Face Space by NyxKrage](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)

```sh
xinference cal-model-mem -s 7 -f gptq -c 8192 -n GOT-OCR2_0
xinference cal-model-mem -s 7 -q Int4 -f gptq -c 16384 -n qwen1.5-chat
# model_name: qwen1.5-chat
# kv_cache_dtype: 16
# model size: 7.0 B
# quant: Int4
# context: 16384
# gpu mem usage:
#   model mem: 4139 MB
#   kv_cache: 8192 MB
#   overhead: 650 MB
#   active: 17024 MB
#   total: 30005 MB (30 GB)
```

### ollama æ¨¡å‹éƒ¨ç½²å·¥å…·

Ollama, a popular local LLM deployment tool, supports a broad range of open-source LLMs and offers an intuitive experience, making it ideal for single-user, local environments.

### OpenLLM

[bentoml/OpenLLM: Run any open-source LLMs, such as Llama, Mistral, as OpenAI compatible API endpoint in the cloud.](https://github.com/bentoml/OpenLLM)

From Ollama to OpenLLM: Running LLMs in the Cloud
[From Ollama to OpenLLM: Running LLMs in the Cloud](https://www.bentoml.com/blog/from-ollama-to-openllm-running-llms-in-the-cloud)

```powershell
# [ã€è§£å†³ã€‘æ— æ³•å°†â€œXXXâ€é¡¹è¯†åˆ«ä¸º cmdletã€å‡½æ•°ã€è„šæœ¬æ–‡ä»¶æˆ–å¯è¿è¡Œç¨‹åºçš„åç§°ã€‚è¯·æ£€æŸ¥åç§°çš„æ‹¼å†™ï¼Œå¦‚æœåŒ…æ‹¬è·¯å¾„ï¼Œè¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ï¼Œç„¶åå†è¯•ä¸€æ¬¡_æ— æ³•å°†â€œlabelmeâ€é¡¹è¯†åˆ«ä¸º cmdletã€å‡½æ•°ã€è„šæœ¬æ–‡ä»¶æˆ–å¯è¿è¡Œç¨‹åºçš„åç§°ã€‚è¯·æ£€æŸ¥å-CSDNåšå®¢](https://blog.csdn.net/weixin_41362657/article/details/110649744)
PS D:\>
Get-ExecutionPolicy -List

        Scope ExecutionPolicy
        ----- ---------------
MachinePolicy       Undefined
   UserPolicy       Undefined
      Process       Undefined
  CurrentUser       Undefined
 LocalMachine       Undefined

# Scope: Process, CurrentUser, LocalMachine, UserPolicy, MachinePolicy
# ExecutionPolicy: Unrestricted, RemoteSigned, AllSigned, Restricted, Default, Bypass, Undefinedâ€
Set-ExecutionPolicy RemoteSigned -Scope CurrentUser
Set-ExecutionPolicy Unrestricted -Scope CurrentUser
Set-ExecutionPolicy RemoteSigned -Scope LocalMachine
```

### LocalAI

[LocalAI-examples/langchain-chroma at main Â· mudler/LocalAI-examples](https://github.com/mudler/LocalAI-examples/tree/main/langchain-chroma)

```sh
# é…ç½®åˆ°dify æ—¶ 404
# 2025-02-11 11:51:10 3:51AM WRN Client error ip=172.20.0.1 latency="24.32Âµs" method=POST status=404 url=/rerank

curl http://10.10.65.77:8080/console/api/workspaces/current/models/model-types/rerank

curl http://10.10.65.77:8080/v1/rerank \
  -H "Content-Type: application/json" \
  -d '{
  "model": "cross-encoder",
  "query": "Organic skincare products for sensitive skin",
  "documents": [
    "Eco-friendly kitchenware for modern homes",
    "Biodegradable cleaning supplies for eco-conscious consumers",
    "Organic cotton baby clothes for sensitive skin",
    "Natural organic skincare range for sensitive skin",
    "Tech gadgets for smart homes: 2024 edition",
    "Sustainable gardening tools and compost solutions",
    "Sensitive skin-friendly facial cleansers and toners",
    "Organic food wraps and storage solutions",
    "All-natural pet food for dogs with allergies",
    "Yoga mats made from recycled materials"
  ],
  "top_n": 3
}'

```

### xinference

[åœ¨Xinferenceä¸Šéƒ¨ç½²è‡ªå®šä¹‰å¤§æ¨¡å‹â€”â€”FreedomIntelligence/HuatuoGPT2-13Bä¸ºä¾‹ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/685747169)

```sh
# å°†æ¨¡å‹ä¸‹è½½æºè®¾ç½®ä¸º ModelScopeã€‚è®¾ç½®ç¯å¢ƒå˜é‡ XINFERENCE_MODEL_SRC=modelscope
docker pull registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference
docker image tag registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xprobe_xinference/xinference:latest
docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9997:9997 --gpus all xprobe/xinference:v<your_version> xinference-local -H 0.0.0.0 --log-level debug

docker run --detach \
  --env TZ=Asia/Shanghai \
  --publish 9997:9997 \
  --name xinference \
  --restart always \
  -e XINFERENCE_MODEL_SRC=modelscope \
  -v //d/xinference/.xinference:/root/.xinference \
  -v //d/xinference/.cache/huggingface:/root/.cache/huggingface \
  -v //d/xinference/.cache/modelscope:/root/.cache/modelscope \
  --gpus all \
  xprobe_xinference/xinference \
  sh /root/.xinference/startup.sh

# Windowsä¸‹æ”¹æˆä¸€è¡Œæ‰§è¡Œ
docker run --detach --env TZ=Asia/Shanghai --publish 9997:9997 --name xinference --restart always -e XINFERENCE_MODEL_SRC=modelscope -v //d/xinference/.xinference:/root/.xinference -v //d/xinference/.cache/huggingface:/root/.cache/huggingface -v //d/xinference/.cache/modelscope:/root/.cache/modelscope --gpus all xprobe_xinference/xinference sh /root/.xinference/startup.sh

alias xinference='docker exec -it xinference xinference'
```

```sh
#!/bin/bash
# startup.sh æ”¾åœ¨å¤–éƒ¨ç£ç›˜æŒ‚è½½è¿›å»ï¼Œå¯åŠ¨æ—¶æ‰§è¡Œ
# å¯åŠ¨ä¸”åå°è¿è¡Œ
xinference-local -H 0.0.0.0 &
# xinference-local -H 0.0.0.0 --log-level debug &
# æ£€æµ‹æ˜¯å¦å¯åŠ¨
while true; do
  if curl -s "http://localhost:9997" > /dev/null; then
    break
  else
    sleep 1
  fi
done

xinference launch --model-name jina-embeddings-v3 --model-type embedding &
xinference launch --model-name jina-reranker-v2 --model-type rerank &

#è‡ªåŠ¨åŠ è½½ embedding
xinference launch --model-name jina-embeddings-v3 --model-type embedding &
xinference launch --model-name bge-m3 --model-type embedding &

#è‡ªåŠ¨åŠ è½½ rerank
xinference launch --model-name jina-reranker-v2 --model-type rerank &
xinference launch --model-name bge-reranker-large --model-type rerank &
xinference launch --model-name bge-reranker-v2-minicpm-layerwise --model-type rerank &

#ç­‰å¾…åå°è¿è¡Œç»“æŸï¼Œå®é™…ä¸Šxinference-localæ˜¯ä¸ä¼šç»“æŸçš„ï¼Œæ‰€ä»¥èƒ½ä¿è¯æ­¤è„šæœ¬è¿›ç¨‹ä¸ç»“æŸï¼Œä»è€Œä¸ä¼šè‡ªåŠ¨é‡å¯
wait
```

```sh
# æœ¬åœ°å®‰è£…
conda create --name py312 python=3.12
conda activate py312

# å®‰è£…xinferenceçš„ä¾èµ–
pip install "xinference[all]"

# å¯åŠ¨xinference
xinference-local # æˆ‘ä½¿ç”¨è¿™ä¸ªå‘½ä»¤å¯åŠ¨ä¸äº†
# è®¾ç½®ä½¿ç”¨modelscopeä¸‹è½½æ¨¡å‹
# å¦‚æœä½ å°±ä¸€å—gpuè¿˜æ˜¯0çš„è¯å°±è¦æŒ‡å®šå¯åŠ¨
CUDA_VISIBLE_DEVICES=0
# ä½¿ç”¨è¿™ä¸ªå‘½ä»¤å¯ä»¥å¯åŠ¨
XINFERENCE_HOME=d:/xinference/ XINFERENCE_MODEL_SRC=modelscope xinference-local --host 0.0.0.0 --port 9997

# åˆ—ä¸¾æœ¬åœ°æ¨¡å‹
xinference list --endpoint "http://127.0.0.1:9997"

# [jina-embeddings-v2-base-zh â€” Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/embedding/jina-embeddings-v2-base-zh.html)
# Model ID: jinaai/jina-embeddings-v2-base-zh
# xinference launch --model-name jina-embeddings-v2-base-zh --model-type embedding
xinference launch --model-name jina-embeddings-v3 --model-type embedding &
xinference launch --model-name bge-m3 --model-type embedding &

# [jina-reranker-v2 â€” Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/rerank/jina-reranker-v2.html)
# Model ID: jinaai/jina-reranker-v2-base-multilingual
xinference launch --model-name jina-reranker-v2 --model-type rerank &
xinference launch --model-name bge-reranker-large --model-type rerank &
xinference launch --model-name bge-reranker-v2-minicpm-layerwise --model-type rerank &

# OCR æ¨¡å‹ [GOT-OCR2_0 â€” Xinference](https://inference.readthedocs.io/zh-cn/v0.16.3/models/builtin/image/got-ocr2_0.html)
xinference launch --model-name GOT-OCR2_0 --model-type image
# Launch model name: GOT-OCR2_0 with kwargs: {}
# Model uid: GOT-OCR2_0
```

### VLLM

A tool designed to run LLMs very efficiently, especially when serving many users at once.
[Ollama vs VLLM: Which Tool Handles AI Models Better? | by Naman Tripathi | Medium](https://medium.com/@naman1011/ollama-vs-vllm-which-tool-handles-ai-models-better-a93345b911e6)

[Using Docker â€” vLLM](https://docs.vllm.ai/en/latest/deployment/docker.html)
[aneeshjoy/vllm-windows: Docker compose to run vLLM on Windows](https://github.com/aneeshjoy/vllm-windows)
[vllm/vllm-openai Tags | Docker Hub](https://hub.docker.com/r/vllm/vllm-openai/tags)

```sh

--model
# Name or path of the huggingface model to use. Default: â€œfacebook/opt-125mâ€

--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]
# The model name(s) used in the API. If multiple names are provided, the server will respond to any of the provided names. The model name in the model field of a response will be the first name in this list. If not specified, the model name will be the same as the --model argument. Noted that this name(s) will also be used in model_name tag content of prometheus metrics, if multiple names provided, metrics tag will take the first one.

--gpu-memory-utilization <value>
# gpu-memory-utilization æ˜¯ç”¨äºè®¾ç½® GPU å†…å­˜åˆ©ç”¨ç‡çš„å‚æ•°ï¼Œ<value> æ˜¯ä¸€ä¸ªä»‹äº 0 åˆ° 1 ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼Œè¡¨ç¤º GPU å†…å­˜çš„ä½¿ç”¨æ¯”ä¾‹
```

```sh
# By default, vLLM downloads models from HuggingFace. If you would like to use models from ModelScope, set the environment variable VLLM_USE_MODELSCOPE before initializing the engine.
# default max-model-len: max_seq_len=32768
    # --env "HUGGING_FACE_HUB_TOKEN=hf_oo" \
    # --model mistralai/Mistral-7B-v0.1
docker run --runtime nvidia --gpus all \
    --detach \
    --env TZ=Asia/Shanghai \
    --name vllm \
    --restart always \
    --env VLLM_USE_MODELSCOPE=True \
    -v //d/workspace/vllm/.cache/:/root/.cache/ \
    -v //d/workspace/models/:/models/ \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:v0.7.2 \
    --model /models/DeepSeek-R1-Distill-Qwen-32B \
    --served-model-name deepseek-reasoner \
    --max-model-len 15520 \
    --gpu-memory-utilization 0.9

# Load and run the model:
vllm serve "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
# Load and run the model:
docker exec -it my_vllm_container bash -c "vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

# Test by accessing the /models endpoints
http://127.0.0.1:8000/v1/models

# Check throughput ( I am running on a RTX 3090 )
http://127.0.0.1:8000/metrics

# Call the server using curl:
curl -X POST "http://localhost:8000/v1/chat/completions" \
    -H "Content-Type: application/json" \
    --data '{
        "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "messages": [
            {
                "role": "user",
                "content": "What is the capital of France?"
            }
        ]
    }'

# OpenAI Completions API with vLLM
curl http://localhost:8000/v1/completions \
curl http://192.168.2.4:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
    }'

# OpenAI Chat Completions API with vLLM
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"}
        ]
    }'
```

## æœ¬åœ°çŸ¥è¯†åº“æ­å»º

[chatchat-space/Langchain-Chatchat: Langchain-Chatchatï¼ˆåŸLangchain-ChatGLMï¼‰åŸºäº Langchain ä¸ ChatGLM ç­‰è¯­è¨€æ¨¡å‹çš„æœ¬åœ°çŸ¥è¯†åº“é—®ç­” | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM) QA app with langchain](https://github.com/chatchat-space/Langchain-Chatchat)
[Langchain-ChatGLMï¼šåŸºäºæœ¬åœ°çŸ¥è¯†åº“é—®ç­”_langchain chatglm-CSDNåšå®¢](https://blog.csdn.net/dzysunshine/article/details/131003488)

[AnythingLLMã€Dify ä¸ Open-WebUIï¼šå¦‚ä½•æ¥å…¥ Ollamaï¼Œå®ƒä»¬æœ‰ä½•ä¸åŒï¼Ÿä¸€ã€å‰è¨€ éšç€å¤§è¯­è¨€æ¨¡ - æ˜é‡‘](https://juejin.cn/post/7455148200627781684)
[Difyã€Anything LLMã€Ollamaã€ç¡…åŸºæµåŠ¨Â åˆ†åˆ«ä¸Â DeepSeekÂ è”åˆæ­å»ºæœ¬åœ°çŸ¥è¯†åº“çš„å¯¹æ¯”è¡¨æ ¼ï¼Œæ¶µç›–åŠŸèƒ½ã€æ€§èƒ½ã€æˆæœ¬å’Œé€‚ç”¨åœºæ™¯ç­‰æ ¸å¿ƒç»´åº¦ã€‚ #æ™®é€šäººå¦‚ä½•ç©è½¬DeepSeek# #deepseek#](https://www.toutiao.com/w/1823311450553344/?app=news_article&category_new=search_thread_aggr&chn_id=94349607399&is_new_connect=0&is_new_user=0&req_id_new=202502090734553364518F875E49146204&share_did=MS4wLjACAAAAZtHq-FtDCLT2HiypHOsw85chAV9wX6K1Jv5UfAUyX0mVQ7HN00yXQhdxXLFA_4OY&share_token=D19DE10F-BE2F-4A6D-A704-AA68397479DC&share_uid=MS4wLjABAAAADwEoMfih4HUgYxYOjTmk9NvGlX1gGSG-ctDaxpGLfOkP-tRdhwDh6SLBAt5iihFb&source=m_redirect&timestamp=1739057696&tt_from=weixin_moments&use_new_style=1&utm_campaign=client_share&utm_medium=toutiao_ios&utm_source=weixin_moments&wxshare_count=1)

LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ï¼Œä¸»è¦æ‹¥æœ‰ 3ä¸ªèƒ½åŠ›ï¼š

1. å¯ä»¥è°ƒç”¨LLMæ¨¡å‹
2. å¯ä»¥å°† LLM æ¨¡å‹ä¸å¤–éƒ¨æ•°æ®æºè¿›è¡Œè¿æ¥
3. å…è®¸ä¸ LLM æ¨¡å‹è¿›è¡Œäº¤äº’

çŸ¥è¯†åº“é—®ç­”å®ç°æ­¥éª¤
åŸºäºLangchainæ€æƒ³å®ç°åŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„é—®ç­”åº”ç”¨ã€‚å®ç°è¿‡ç¨‹å¦‚ä¸‹ï¼š
1ã€åŠ è½½æ–‡ä»¶
2ã€è¯»å–æ–‡æœ¬
3ã€æ–‡æœ¬åˆ†å‰²
4ã€æ–‡æœ¬å‘é‡åŒ–
5ã€é—®å¥å‘é‡åŒ–
6ã€åœ¨æ–‡æœ¬å‘é‡ä¸­åŒ¹é…å‡ºä¸é—®å¥å‘é‡æœ€ç›¸ä¼¼çš„top kä¸ª
7ã€åŒ¹é…å‡ºçš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡å’Œé—®é¢˜ä¸€èµ·æ·»åŠ åˆ°promptä¸­
8ã€æäº¤ç»™LLMç”Ÿæˆå›ç­”ã€‚

### open-webui

[open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)](https://github.com/open-webui/open-webui)

### MaxKB æ–¹æ¡ˆ

[MaxKB/README_CN.md at main Â· 1Panel-dev/MaxKB](https://github.com/1Panel-dev/MaxKB/blob/main/README_CN.md)

[MaxKB ç¦»çº¿å®‰è£…åŒ…ä¸‹è½½ - FIT2CLOUD é£è‡´äº‘](https://community.fit2cloud.com/#/download/maxkb/v1-9-1)

```sh
# Linux æœºå™¨
docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data -v ~/.python-packages:/opt/maxkb/app/sandbox/python-packages cr2.fit2cloud.com/1panel/maxkb

# Windows æœºå™¨
docker run -d --name=maxkb --restart=always -p 8080:8080 -v C:/maxkb:/var/lib/postgresql/data -v C:/python-packages:/opt/maxkb/app/sandbox/python-packages cr2.fit2cloud.com/1panel/maxkb
docker run -d --name=maxkb --restart=always -p 8080:8080 -v d:/docker/maxkb:/var/lib/postgresql/data -v d:/docker/maxkb/python-packages:/opt/maxkb/app/sandbox/python-packages 1panel/maxkb:v1.10.0-lts

# ç”¨æˆ·å: admin
# å¯†ç : MaxKB@123..
# Max1+1
```

### anything-llm

[Mintplex-Labs/anything-llm: The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, and more.](https://github.com/Mintplex-Labs/anything-llm)

[anything-llm HOW_TO_USE_DOCKER](https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/HOW_TO_USE_DOCKER.md)

```powershell
# Run this in powershell terminal
$env:STORAGE_LOCATION="$HOME\Documents\anythingllm"; `
If(!(Test-Path $env:STORAGE_LOCATION)) {New-Item $env:STORAGE_LOCATION -ItemType Directory}; `
If(!(Test-Path "$env:STORAGE_LOCATION\.env")) {New-Item "$env:STORAGE_LOCATION\.env" -ItemType File}; `
docker run -d -p 3001:3001 `
--cap-add SYS_ADMIN `
-v "$env:STORAGE_LOCATION`:/app/server/storage" `
-v "$env:STORAGE_LOCATION\.env:/app/server/.env" `
-e STORAGE_DIR="/app/server/storage" `
mintplexlabs/anythingllm;

# anyadmin / Anyadmin1+1
```

### FastGPT

[labring/FastGPT: FastGPT is a knowledge-based platform built on the LLMs, offers a comprehensive suite of out-of-the-box capabilities such as data processing, RAG retrieval, and visual AI workflow orchestration, letting you easily develop and deploy complex question-answering systems without the need for extensive setup or configuration.](https://github.com/labring/FastGPT)

[å¿«é€Ÿäº†è§£ FastGPT | FastGPT](https://doc.tryfastgpt.ai/docs/intro/)

## Dify

[Dify github](https://github.com/langgenius/dify)
[åœ¨çº¿ç‰ˆ Studio - Dify](https://cloud.dify.ai/apps)

[Dify æ–‡æ¡£](https://docs.dify.ai/zh-hans)

[ç‰¹æ€§ä¸æŠ€æœ¯è§„æ ¼ | Dify](https://docs.dify.ai/zh-hans/getting-started/readme/features-and-specifications)

[DifyShare - Share your flows. View the magic.](https://difyshare.com/)
[BannyLon/DifyAIA: åŸºäºDifyè‡ªä¸»åˆ›å»ºçš„AIåº”ç”¨DSLå·¥ä½œæµ](https://github.com/BannyLon/DifyAIA)

Dify is an open-source LLM app development platform. Dify's intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.

æ¡ˆä¾‹ï¼š

- è®­ç»ƒå‡ºä¸“å±äºâ€œä½ â€çš„é—®ç­”æœºå™¨äºº
- å®˜ç½‘ AI æ™ºèƒ½å®¢æœ
- æ¥å…¥å¾®ä¿¡
- æ¥å…¥é’‰é’‰

### Dify 0.15.3 deploy

[Docker Compose éƒ¨ç½² | Dify](https://docs.dify.ai/zh-hans/getting-started/install-self-hosted/docker-compose)

.env ä¿®æ”¹éƒ¨åˆ†

```sh
# .env

# Used to change the OpenAI base address, default is https://api.openai.com/v1.
# When OpenAI cannot be accessed in China, replace it with a domestic mirror address,
# or when a local model provides OpenAI compatible API, it can be replaced.
OPENAI_API_BASE=https://api.openai.com/v1

# Defaults to gevent. If using windows, it can be switched to sync or solo.
SERVER_WORKER_CLASS=gevent

# Upload file size limit, default 15M.
UPLOAD_FILE_SIZE_LIMIT=15
# Upload image file size limit, default 10M.
UPLOAD_IMAGE_FILE_SIZE_LIMIT=10
# Upload video file size limit, default 100M.
UPLOAD_VIDEO_FILE_SIZE_LIMIT=100
# Upload audio file size limit, default 50M.
UPLOAD_AUDIO_FILE_SIZE_LIMIT=50

MAIL_DEFAULT_SEND_FROM=è‡ªå·±çš„é‚®ç®±
#Â SMTP server configuration, used when MAIL_TYPE is `smtp`
SMTP_SERVER= å¯¹åº”é‚®ç®±çš„smtpï¼Œä¸€èˆ¬éƒ½åœ¨è®¾ç½®é‡Œ
SMTP_PORT=465
SMTP_USERNAME= è‡ªå·±çš„é‚®ç®±
SMTP_PASSWORD= Â è‡ªå·±çš„å¯†ç 
SMTP_USE_TLS=true
SMTP_OPPORTUNISTIC_TLS=false
```

Docker Compose éƒ¨ç½²

```sh
# å…‹éš† Dify æºä»£ç è‡³æœ¬åœ°ç¯å¢ƒã€‚
# å‡è®¾å½“å‰æœ€æ–°ç‰ˆæœ¬ä¸º 0.15.3
git clone https://github.com/langgenius/dify.git --branch 0.15.3

# å¯åŠ¨ Dify
# 1.  è¿›å…¥ Dify æºä»£ç çš„ Docker ç›®å½•
cd dify/docker
# 2.  å¤åˆ¶ç¯å¢ƒé…ç½®æ–‡ä»¶
cp .env.example .env
# 3.  å¯åŠ¨ Docker å®¹å™¨
docker-compose up -d
```

æ›´æ–° Dify è¿›å…¥ dify æºä»£ç çš„ docker ç›®å½•ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```sh
cd dify/docker
docker compose down
git pull origin main
docker compose pull
docker compose up -d
```

è®¿é—® Dify
ä½ å¯ä»¥å…ˆå‰å¾€ç®¡ç†å‘˜åˆå§‹åŒ–é¡µé¢è®¾ç½®è®¾ç½®ç®¡ç†å‘˜è´¦æˆ·ï¼š

```sh
# æœ¬åœ°ç¯å¢ƒ
http://localhost/install

# æœåŠ¡å™¨ç¯å¢ƒ
http://your_server_ip/install

# Dify ä¸»é¡µé¢ï¼š
# æœ¬åœ°ç¯å¢ƒ
http://localhost

# æœåŠ¡å™¨ç¯å¢ƒ
http://your_server_ip
```

### å¦‚ä½•é‡ç½®difyç®¡ç†å‘˜å¯†ç 

```sh
docker exec -it docker-api-1 flask reset-password
# ç„¶åæŒ‰ç…§æç¤ºè¾“å…¥ç®¡ç†å‘˜emailä»¥åŠä¸¤æ¬¡æ–°å¯†ç å³å¯ã€‚
```

### dify é’‰é’‰

[å°† Dify åº”ç”¨ä¸é’‰é’‰æœºå™¨äººé›†æˆ | Dify](https://docs.dify.ai/zh-hans/learn-more/use-cases/dify-on-dingtalk)

### éƒ¨ç½²æ¨¡å‹

[Integrate Local Models Deployed by Xinference | Dify](https://docs.dify.ai/development/models-integration/xinference)
[Integrate Local Models Deployed by OpenLLM | Dify](https://docs.dify.ai/development/models-integration/openllm)
[Integrate Local Models Deployed by LocalAI | Dify](https://docs.dify.ai/development/models-integration/localai)

wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O bert
wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j

### Embedding model

[Embedding models Â· Ollama Blog](https://ollama.com/blog/embedding-models)

[MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)
MTEBï¼ˆMassive Text Embedding Benchmarkï¼‰æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ–‡æœ¬åµŒå…¥ï¼ˆEmbeddingï¼‰æ¨¡å‹çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•å¹³å°ã€‚é€šè¿‡å¤šä»»åŠ¡å’Œå¤šæ•°æ®é›†çš„ç»„åˆï¼ŒMTEBå¯ä»¥å…¨é¢è¡¡é‡ä¸åŒEmbeddingæ¨¡å‹åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€è¯­ä¹‰æ£€ç´¢ã€æ–‡æœ¬èšç±»ç­‰ã€‚

```sh
ollama pull mxbai-embed-large
```

### reranking models

Ollama rerank model [linux6200/bge-reranker-v2-m3](https://ollama.com/linux6200/bge-reranker-v2-m3)

deploy local embedding/reranking models using xinference/LocalAI/OpenLLM.

[Using Xinference â€” Xinference](https://inference.readthedocs.io/en/latest/getting_started/using_xinference.html#using-xinference-with-docker)

```sh
docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9998:9997 --gpus all xprobe/xinference:<your_version> xinference-local -H 0.0.0.0 --log-level debug

# 1024 ç»´åº¦  æœ€å¤§ token æ•° 8192
jina-embeddings-v3
```

### dify issue

```sh
# agent è°ƒç”¨é›…è™è´¢ç»
# prompt ä»Šå¤©æœ‰å“ªäº›æ–°é—»
[ollama] Error: API request failed with status code 400: {"error":"registry.ollama.ai/library/deepseek-r1:7b does not support tools"}
```

## firecrawl

[firecrawl/CONTRIBUTING](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md)

[localhost:3002](http://localhost:3002/)
[admin/queues](http://localhost:3002/admin/queues)

```sh
docker pull node:18-slim
docker pull node:22-slim
docker pull rust:1-slim
docker pull golang:1.24


curl -X POST http://localhost:3002/v1/crawl \
    -H 'Content-Type: application/json' \
    -d '{
      "url": "https://www.baidu.com"
    }'

curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "limit": 10,
      "scrapeOptions": {
        "formats": ["markdown", "html"]
      }
    }'


```

## GPU ä»‹ç»

GPU å‘½åè§„åˆ™è§£è¯»

[ä¸€æ–‡è¯»æ‡‚ NVIDIA GPU äº§å“çº¿-51CTO.COM](https://www.51cto.com/article/805028.html)
[è‹±ä¼Ÿè¾¾GPUå„å‹å·å¯¹æ¯” - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/718450083)
[æ¯”è¾ƒ GeForce ç³»åˆ—æœ€æ–°ä¸€ä»£æ˜¾å¡å’Œå‰ä»£æ˜¾å¡ | NVIDIA](https://www.nvidia.cn/geforce/graphics-cards/compare/)

### å­—æ¯ï¼š æ¶æ„ä»£å·ï¼ˆArchitectureï¼‰

æ–°çš„æ¶æ„é€šå¸¸ä»£è¡¨ç€æ€§èƒ½ã€èƒ½æ•ˆæ¯”å’Œæ–°æŠ€æœ¯çš„æ˜¾è‘—æå‡ã€‚

ä»£è¡¨ GPU çš„æ ¸å¿ƒæ¶æ„ï¼Œé€šå¸¸ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªå­—æ¯è¡¨ç¤ºï¼Œä»£è¡¨ GPU çš„å¾®æ¶æ„ã€‚ä¾‹å¦‚ï¼š

Kï¼šKepler æ¶æ„ 2012
Vï¼šVolta æ¶æ„ 2017
Tï¼šTuring æ¶æ„ 2018, RTX 20 ç³»åˆ—, GTX 16 ç³»åˆ—
Aï¼šAmpere æ¶æ„ 2019, RTX 30 ç³»åˆ—
Hï¼šHopper æ¶æ„ 2022, RTX 4000 ç³»åˆ—
L: Ada Lovelace æ¶æ„ 2022, RTX 40 ç³»åˆ—
B: Blackwell æ¶æ„, RTX 50 ç³»åˆ—

### æ•°å­—ï¼šæ€§èƒ½å±‚çº§ï¼ˆTierï¼‰

é€šå¸¸ç”¨æ•°å­—è¡¨ç¤ºï¼Œæ•°å­—è¶Šå¤§é€šå¸¸ä»£è¡¨æ€§èƒ½è¶Šå¼ºã€‚

ä»£è¡¨ GPU çš„å…·ä½“å‹å·ï¼Œé€šå¸¸ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªæ•°å­—è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼š

â€œ4â€ ç³»åˆ—ï¼šå…¥é—¨çº§æˆ–ä½åŠŸè€—çº§
â€œ10â€ ç³»åˆ—ï¼šä¸­ç«¯æ¨ç†ä¼˜åŒ–çº§
â€œ40â€ ç³»åˆ—ï¼šé«˜ç«¯å›¾å½¢å’Œè™šæ‹Ÿå·¥ä½œç«™çº§
â€œ100â€ ç³»åˆ—ï¼šæ——èˆ°çº§é«˜æ€§èƒ½è®¡ç®—å’Œäººå·¥æ™ºèƒ½çº§

### å¸¸è§çš„GPU å‹å·å¯¹æ¯”è§£æï¼šåŸºäº GPU å‘½åæ¨æ–­æ˜¾å¡ç‰¹æ€§

ç¤ºä¾‹ä¸€ï¼šT4 ä¸ L4 çš„æ¯”è¾ƒ

L4 æ˜¯ T4 çš„ç›´æ¥åç»§è€…ï¼Œå±äºåŒä¸€æ€§èƒ½å±‚çº§ï¼Œé’ˆå¯¹ç›¸ä¼¼çš„åº”ç”¨åœºæ™¯è®¾è®¡ã€‚ç„¶è€Œï¼Œä¸¤è€…åœ¨å¾®æ¶æ„å’ŒæŠ€æœ¯è§„æ ¼ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼š

å¾®æ¶æ„ï¼š L4 é‡‡ç”¨æ›´æ–°çš„ Ada Lovelace æ¶æ„ï¼ˆ2023 å¹´å‘å¸ƒï¼‰ï¼Œè€Œ T4 åˆ™é‡‡ç”¨è¾ƒæ—©çš„ Turing æ¶æ„ï¼ˆ2018 å¹´å‘å¸ƒï¼‰ã€‚
æ˜¾å­˜å®¹é‡ï¼š L4 é…å¤‡äº†æ›´å¤§çš„æ˜¾å­˜å®¹é‡ï¼Œè¾¾åˆ° 24 GBï¼Œè€Œ T4 ä»…æœ‰ 16 GBã€‚
æ ¸å¿ƒæ•°é‡å’Œæ€§èƒ½ï¼š L4 æ‹¥æœ‰æ›´å¤šä¸”æ›´å¼ºå¤§çš„è®¡ç®—æ ¸å¿ƒï¼Œå› æ­¤åœ¨æ€§èƒ½ä¸Šä¼˜äº T4ã€‚
è™½ç„¶ä¸¤è€…çš„ç›®æ ‡åŠŸè€—ç›¸ä¼¼ï¼Œä½† L4 å‡­å€Ÿæ›´å…ˆè¿›çš„æ¶æ„å’Œæ›´é«˜çš„æ˜¾å­˜å®¹é‡ï¼Œåœ¨ç›¸åŒçš„åŠŸè€—ä¸‹èƒ½å¤Ÿæä¾›æ›´å¼ºçš„è®¡ç®—æ€§èƒ½ï¼Œæ›´é€‚åˆå¤„ç†å¯¹æ˜¾å­˜å®¹é‡æœ‰è¾ƒé«˜è¦æ±‚çš„ä»»åŠ¡ã€‚

ç¤ºä¾‹äºŒï¼šA10 ä¸ A100 çš„æ¯”è¾ƒ

A100 æ˜¯åŸºäº Ampere æ¶æ„çš„æ——èˆ°çº§äº§å“ï¼Œè€Œ A10 åˆ™æ˜¯è¯¥æ¶æ„ä¸‹çš„ä¸€ä¸ªè¾ƒä½å±‚çº§çš„å‹å·ã€‚ä¸¤è€…éƒ½åŸºäºç›¸åŒçš„ Ampere å¾®æ¶æ„ï¼Œä½†åœ¨è§„æ¨¡å’Œæ€§èƒ½ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼š

æ ¸å¿ƒæ•°é‡å’Œæ€§èƒ½ï¼š A100 æ‹¥æœ‰è¿œå¤šäº A10 çš„è®¡ç®—æ ¸å¿ƒï¼Œå› æ­¤åœ¨è®¡ç®—æ€§èƒ½ä¸Šè¿œè¶… A10ã€‚
æ˜¾å­˜å®¹é‡ï¼š A100 é…å¤‡äº†æ›´å¤§çš„æ˜¾å­˜å®¹é‡ï¼Œä»¥æ”¯æŒæ›´å¤§è§„æ¨¡çš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ã€‚
åŠŸè€—ï¼š ç”±äºè§„æ¨¡æ›´å¤§ã€æ€§èƒ½æ›´å¼ºï¼ŒA100 çš„åŠŸè€—ä¹Ÿé«˜äº A10ã€‚
å› æ­¤ï¼ŒA100 æ›´é€‚åˆéœ€è¦å¤„ç†å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒã€å¾®è°ƒå’Œé«˜ååé‡æ¨ç†ç­‰ demanding è®¡ç®—ä»»åŠ¡çš„åœºæ™¯ï¼Œè€Œ A10 åˆ™æ›´é€‚åˆå¯¹æˆæœ¬å’ŒåŠŸè€—æ•æ„Ÿã€å¯¹æ€§èƒ½è¦æ±‚ç›¸å¯¹è¾ƒä½çš„åº”ç”¨åœºæ™¯ã€‚

### é¢å‘ AI å’Œæœºå™¨å­¦ä¹ çš„æ˜¾å¡

| æ’å | GPU å‹å· | æ¶æ„ | CUDA æ ¸å¿ƒæ•° | æ˜¾å­˜ | ä¸»è¦ç”¨é€” |
| --- | --- | --- | --- | --- | --- |
| 1 | NVIDIA H100 | Hopper | 16896 | 80GB HBM3 | å¤§è§„æ¨¡ AI è®­ç»ƒ |
| 2 | NVIDIA A100 | Ampere | 6912 | 40GB HBM2 | AI æ¨ç†ã€æœºå™¨å­¦ä¹  |
| 3 | NVIDIA V100 | Volta | 5120 | 32GB HBM2 | ç¥ç»ç½‘ç»œè®­ç»ƒã€ç§‘å­¦è®¡ç®— |
| 4 | Tesla T4 | Turing | 2560 | 16GB GDDR6 | è½»é‡ AI æ¨ç†ã€äº‘æ¨ç† |
| 5 | Tesla P100 | Pascal | 3584 | 16GB HBM2 | æ•°æ®ä¸­å¿ƒåŠ é€Ÿã€æœºå™¨å­¦ä¹ æ¨ç† |

### ç¡¬ä»¶èµ„æºé…ç½®

æ˜¾å­˜éœ€æ±‚ â‰ˆ æ¨¡å‹å‚æ•° Ã— å‚æ•°å­—èŠ‚æ•° Ã— å®‰å…¨ç³»æ•°ï¼ˆ1.3-1.5ï¼‰

CPU GPU é…ç½®æ¯”ä¾‹ï¼šå»ºè®®å†…å­˜æ˜¯æ˜¾å­˜çš„1.5å€ä»¥ä¸Š
