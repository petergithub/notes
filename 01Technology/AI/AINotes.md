# AI News & Information

AI干不了你干不了的事，因为你干不了你就看不出来它是胡干还是真干。AI的有效能力范围等于跟它配对的人的能力范围。

- [Humata - ChatGPT for all your files](https://www.humata.ai/)
- [Code Llama: Inference code for CodeLlama models](https://github.com/facebookresearch/codellama)

## GPT 分类

[高盛发布中国AI报告：全面剖析BAT三巨头以及26个核心玩家](https://36kr.com/p/5091725.html)

- 用 [Perplexity](http://Perplexity.ai) 代替 Google；
- 用 [Poe](https://poe.com/sage) 代替 ChatGPT.
- 用 Promptable 代替 OpenAI Playground.
- 用 [Elicit](https://elicit.org) 代替 Google Scholar

- [极客时间 AI 指南](https://zhinan.geekbang.org/)
- 写文案用 ChatGPT、Notion AI或 Jarvis，Claude，
  - [Poe](https://poe.com/sage)
  - Slack Claude
  - [文心一言 百度](https://yiyan.baidu.com/)
  - [通义千问 阿里](https://qianwen.aliyun.com/)
  - [Google AI Studio](https://aistudio.google.com/app/prompts/new_chat)
  - [Poe](https://poe.com/sage)
  - [Kimi.ai - 帮你看更大的世界](https://kimi.moonshot.cn/)
  - [DeepSeek](https://platform.deepseek.com)
  - 腾讯混元助手 微信小程序
  - Copy ai，
  - [Writesonic](https://app.writesonic.com，
  - Character AI，
  - [Jasper](https://www.jasper.ai/)
  - [Forefront Chat](https://chat.forefront.ai/)
- 图像处理
  - [文心一格 百度](https://yige.baidu.com/)
  - [通义万相 阿里](https://wanxiang.aliyun.com/creation)
  - Midjourney Discord 或 Getimg.ai，stable diffusion，dall-e
  - [OmniParser 图片解析](https://microsoft.github.io/OmniParser/)：OmniParser is a comprehensive method for parsing user interface screenshots into structured and easy-to-understand elements
  - [生成插画、漫画 Infinite AI Artboard - Recraft](https://www.recraft.ai/)
- 音视频转文本
  - [通义听悟 - 阿里](https://tingwu.aliyun.com)
  - [Aiko — Sindre Sorhus 语音转文字](https://sindresorhus.com/aiko)
  - [MacWhisper](https://goodsnooze.gumroad.com/l/macwhisper) [MacWhisper](https://app.gumroad.com/d/29e33b796f6ce9bb186f87cdf2fadb16)
  - Buzz transcribes and translates audio offline on your personal computer. [Buzz](https://github.com/chidiwilliams/buzz)
  - [AudioPen](https://audiopen.ai/demo)
- 文本转音视频
  - 音频生成用 voice.ai 或 Eleven Labs，连字幕都可以用自己通过AI“训练”的个性专属字体。
  - [AI Video Editor | Create viral videos with AI - Topview.ai](https://www.topview.ai/)
- 视频处理
  - 把画出来的人物，喂文案让他变成频视的是D-ID；
  - 视频制作用 Descript 或 Runway
  - 一个频视中的人物替换成虚拟人或CG的是Wonder；
  - 给频视配音创建音乐就用Soundraw；
  - [Open-Sora 视频生成](https://github.com/hpcaitech/Open-Sora)
  - [Open Sora demo - a Hugging Face Space by hpcai-tech](https://huggingface.co/spaces/hpcai-tech/open-sora)
  - [可灵 AI - 新一代 AI 创意生产力平台](https://klingai.kuaishou.com/)
- 智能化研发
  - [Codeium · Free AI Code Completion & Chat](https://codeium.com/faq)
  - [通义灵码_智能编码助手 阿里云](https://tongyi.aliyun.com/lingma/download)
  - 百度 Comate 2.0代码助手 [Baidu Comate · Coding mate](https://comate.baidu.com/)
  - [OpenDevin - OpenHands](https://github.com/All-Hands-AI/OpenHands)
  - [yetone/avante.nvim: Use your Neovim like using Cursor AI IDE!](https://github.com/yetone/avante.nvim)
  - [阿里智能化研发一年复盘](https://mp.weixin.qq.com/s/JTpLy8Z0klokHVcaDZm2RQ)
  - [CodeRabbit is an AI-powered code reviewer](https://docs.coderabbit.ai/)
  - [rikvermeulen/co-op-gitlab: Automate code reviews and feedback for GitLab Merge Requests using OpenAI GPT-3/4](https://github.com/rikvermeulen/co-op-gitlab)
  - [Diffblue Cover write Java unit tests](https://docs.diffblue.com/get-started/get-started/get-started-cover-plugin)
- 把真人变成虚拟人的是 Meta human，想更简单变虚拟人的是Ready player me；
- PPT:
  - chatgpt生成的markdown语法内容导入[MindShow](https://www.mindshow.fun/#/home)
  - [AI PPT Maker - Best Online Free (No Sign up)](https://aipptmaker.ai/en)
  - [快速做PPT Gamma - Presentations and Slide Decks with AI | Gamma](https://gamma.app/)
- 第三方社群，civitai
- 数学 [MathGPT 学而思](https://www.mathgpt.com/)
- 医疗问题回答[MediSearch](https://medisearch.io/zh)
- 查找法律案例、条文 [MetaLaw 类案检索，一键直达，让你的法律研究效率快人10倍](https://meta.law/)
- 办公小浣熊:制作excel表格。
- Scribe AI chrome 插件：自动生成可视化操作指南, 创建逐步指南的场景，包括操作说明、标准操作规程、培训手册等。
- 编排
  - [Dify github](https://github.com/langgenius/dify)
  - [TEN Agent is a conversational AI](https://github.com/TEN-framework/TEN-Agent)
  - [RAG七十二式：2024年度RAG清单](https://mp.weixin.qq.com/s/_pnezCv-sKmzhho7Xw3D2g)
- UI 设计工具
  - [Stitch - Design with AI](https://stitch.withgoogle.com/?pli=1)

[AI 工具列表 飞书文档](https://zl49so8lbq.feishu.cn/wiki/wikcn6YTN3CrZTS8RhrEca8c8Eg)
[极客时间 AIGC 知识库](https://gp477l8icq.feishu.cn/wiki/JUXnwzSuviL5E9kh6jUc8FRinHe)
[ModelScope魔搭社区 旨在打造下一代开源的模型即服务共享平台，为泛AI开发者提供灵活、易用、低成本的一站式模型服务产品，让模型应用更简单。](https://community.modelscope.cn/)
[ima.copilot-腾讯智能工作台](https://ima.qq.com/) ima.copilot（简称ima）是一款由腾讯混元大模型提供技术支持的，面向学习、办公场景，以知识库为核心的AI智能工作台，是读、搜、写一体的效率工具

[xtekky/gpt4free: decentralising the Ai Industry, just some language model api's...](https://github.com/xtekky/gpt4free)

| Website s | Model(s) |
| --- | --- |
| [forefront.ai](https://chat.forefront.ai) | GPT-4/3.5 |
| [poe.com](https://poe.com) | GPT-4/3.5 |
| [writesonic.com](https://writesonic.com) | GPT-3.5 / Internet |
| [t3nsor.com](https://t3nsor.com) | GPT-3.5 |
| [you.com](https://you.com) | GPT-3.5 / Internet / good search |
| [sqlchat.ai](https://sqlchat.ai) | GPT-3.5 |
| [bard.google.com](https://bard.google.com) | custom / search |
| [bing.com/chat](https://bing.com/chat) | GPT-4/3.5 |
| [italygpt.it](https://italygpt.it) | GPT-3.5 |

[分享一下我现在每天最常用的AI产品](https://mp.weixin.qq.com/s/ES-9RTkiwIk99hphoUbfyg)

1. AI知识问答： OpenAI-o3，豆包（国内）
2. 写作和写内容： GPT-4.5（其他都是垃圾）
3. 长文本处理： Gemini 2.5 Pro（其他都是垃圾）
4. 深度研究： ChatGPT DeepResearch，秘塔AI（国内）
5. 图片生成： Midjourney，即梦（国内），Flux（开源）
6. 中文海报生成： 即梦3.0（国内，海外都是垃圾）
7. 图片修改： Flux Context、豆包（国内）
8. AI通用Agent： MiniMax Agent（国内）
9. AI设计Agent： Lovart，星流（国内）
10. AI编程Agent： Claude code
11. AI编程IDE： Cursor，Trae（国内）
12. AI音乐生成： Suno 4.5+、Mureka v7（国内）
13. AI视频生成（带台词）： Veo3，即梦对口型大师版（国内）
14. AI视频生成（无台词）： Veo3，其他的都是国内的。即梦（最常用，做各种风格化），可灵（做高质量运动），hailuo（动作表演、高速打斗）
15. AI语音生成： 11Labs，Minimax（国内）
16. AI 3D生成： Tripo
17. AI工作流： 飞书多维表格
18. AI翻译插件： 沉浸式翻译
19. AI笔记和知识库： Get笔记

### prompt 提示词

提示词分为系统提示词和用户提示词，用户提示词就是我们的问题。系统提示词，是agent的背景/角色，设置了agent需要完成什么类型的任务。系统提示词主要包括：身份（Role）+ 上下文（Context）+  例子（Examples） + 输出规范（Output Format）。

现在已经有了很多帮助我们生产提示词的工具，如：
https://prompt.always200.com/
https://prompts.chat/
我们可以使用工具简单生成初版，再进行后续优化。

## AI 在研发场景落地的现状

- **智能研发插件**：以 Github Copilot/ 通义灵码 /Comate 为代表，主要以 JetBrains、VSCode 为插件形式为用户提供代码补全为主的智能编码服务
- **AI Native 的 IDE**：以 Cursor、Windsurf、MarsCode 为代表，以独立 IDE 的方式为开发者提供服务，而有一些公司如 PearAI 已经开始走开源路径，他们的共同特点是以 VSCode 为技术底座进行二次开发，好处是能极大程度上利用 VSCode 的插件和开源生态
- **CodeReview 智能化**：这个领域起步比较早，但效果始终一般，还需要很长时间的摸索，阿里内部很早就启动了这个项目，但效果并不显著，这里既存在模型的能力的问题，也存在工程化不足的问题
- **RAG 搜索场景**：RAG 其实解决的是搜索和 Summary 的问题，例如知识搜索，智能答疑，但也存在非常大的挑战，例如用户问题的上下文不足，知识不保鲜，信息不完整，很难评测，等等，但由于其门槛比较低，反而是大多数团队会首先涉足的领域
- **其他的场景**：例如智能解决代码冲突，自动解决编译问题等也都在阿里内部平台早已上线，智能诊断，智能监控等均有人在调研中
- **局部智能化的 Agent**：以 Gru.ai 等产品为代表帮助用户生成单元测试，以 readme-ai 为代表帮助开发者生成 Readme，以 RepoAgent 为代表帮用户补充注释等等，而阿里在内部也还实现了帮助用户按整个仓库生成注释，生成单元测试的 Agent ，这类 Agent 的特点是场景比较比较垂直简单，问题不发散，成功率比较高
- **广泛自动化的 Agent**：以 Devin、OpenDevin 为代表，以 SWE-bench 为主要评测集的方式，利用大模型生成实现一个任务的 plan，并调用工具，在一个独立的容器内执行，并且能和用户交互的方式来实现一些简单的 issue 或者需求

## AI 编码助手

[What else should determine my model use in Cline? : r/ChatGPTCoding](https://www.reddit.com/r/ChatGPTCoding/comments/1hsx76e/what_else_should_determine_my_model_use_in_cline/)

[lst97/claude-code-sub-agents: Collection of specialized AI subagents for Claude Code for personal use.](https://github.com/lst97/claude-code-sub-agents)

### Continue

[Context providers | Continue](https://docs.continue.dev/customize/context-providers#gitlab-merge-request)

add context: Ctrl+I

### Cline

[cline: Autonomous coding agent](https://github.com/cline/cline)

[VSCode + Cline + VLLM + Qwen2.5 = Fast : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1gbb2de/vscode_cline_vllm_qwen25_fast/)

[set up cline and LLM](https://www.reddit.com/r/LocalLLaMA/comments/1gbb2de/comment/ltkf1z3)

```sh
docker run --runtime nvidia --gpus all \
-v ~/.cache/huggingface:/root/.cache/huggingface \
--ipc=host -p 8000:8000 \
vllm/vllm-openai \
--model Qwen/Qwen2.5-32B-Instruct-AWQ  --tensor-parallel-size 2 \
--quantization awq_marlin --enable-auto-tool-choice --tool-call-parser hermes \
--kv-cache-dtype fp8_e5m2 \
--rope-scaling '{ "factor": 4.0, "original_max_position_embeddings": 32768, "type": "yarn" }'
```

[Install the Cline extension onto VSCode](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)

Cline 使用 GitHub Copilot：API Provider > vscode lm api > Language Model > github copilot

### Roo Code

[Roo Code (prev. Roo Cline) gives you a whole dev team of AI agents in your code editor.](https://github.com/RooVetGit/Roo-Code)

[[Poweruser Guide] Level Up Your RooCode: Become a Roo Poweruser! [Memory Bank] : r/RooCode](https://www.reddit.com/r/RooCode/comments/1jfx9mk/poweruser_guide_level_up_your_roocode_become_a/)

[GreatScottyMac/RooFlow: RooFlow - Enhanced Memory Bank System with ☢️Footgun Power☢️ Next-gen Memory Bank system with five integrated modes and system-level customization. Uses Roo Code's experimental "Footgun" feature for deep AI assistant customization while maintaining efficient token usage!](https://github.com/GreatScottyMac/RooFlow/tree/main)

Available Keyboard Commands:

- roo-cline.focusInput: ctrl+alt+r
- Add to Context: ctrl+alt+c
- roo.acceptInput: ctrl+enter

### Google Gemini Code Assist

[Gemini Code Assist for business | Google Cloud](https://codeassist.google/products/business)
[google-gemini/gemini-cli: An open-source AI agent that brings the power of Gemini directly into your terminal.](https://github.com/google-gemini/gemini-cli)

Toggle integration: After the one-time setup, you can easily manage the integration:
[Gemini CLI + VS Code: Native diffing and context-aware workflows - Google Developers Blog](https://developers.googleblog.com/en/gemini-cli-vs-code-native-diffing-context-aware-workflows/)

To activate it: /ide enable
To deactivate it: /ide disable

## AI 使用思路

### 把 AI 当工具的三种思路

[大模型时代的学习，可不只是写写 prompt](https://mp.weixin.qq.com/s/C-RHduSapy23Y1QQUeDImg)

我简单给大家介绍下，核心思路其实就是：AI 轻实践。不是推荐大家啃大部头或者死磕源代码，看论文，而是用 AI 当拐棍，在动手的过程能力学东西。

第一个方法叫“极简复刻”。遇到复杂的开源项目或论文，不用死磕细节，直接扔给 AI 编程工具，让它用你熟悉的语言做一个极简版 Demo。比如原项目是 .NET 写的，你用 Python 复刻个简化版，跑起来看看核心逻辑；论文里的算法流程，也能让 AI 生成个演示 demo。好处是能快速在自己的环境里跑通，从宏观到细节一层层挖，比对着文档空想实在多了。当然了，这种得有点编程的底子。

第二个方法藏在微信里——用“元宝” AI 助理练习批判性思维。看到公众号文章、群聊观点，直接转给它，多问几句“这结论有啥局限”，或者“和另一个观点冲突在哪”，等等。关键是别当伸手党，带着质疑去聊，得到的答案还能存成知识库。微信本来就是高频工具，顺手就能搞搞深度学习，比刷信息流强太多了。

第三个方法叫“万物皆可 Vibe Coding”。有个想法就赶紧让 AI 帮你弄成原型，比如给娃做个有声绘本网站，或者复刻某个低配版的产品。不用追求完美，搞个“用完即弃”的简易版就行。从消费者变成创造者，对技术和产品的理解立马不一样，保持好奇心，永远不老。

这三个方法，作者给出了实操指南，大家可以去墨问里学习，其实就是“用 AI 降低实践门槛，在动手里找体感”。不用等学透了才开始，边做边迭代，反而学得更扎实。

## AI 应用构建

### 围绕AI能力构建有价值的AI产品

[大语言模型适用业务场景及落地案例梳理-知乎](https://www.zhihu.com/xen/market/training/training-video/1893664728515068172/1893665727908644714?education_channel_code=ZHZN-cd8085beea05e6d)

[围绕AI能力构建有价值的AI产品-知乎](https://www.zhihu.com/xen/market/training/training-video/1890413236689564620/1890414867116177303?education_channel_code=ZHZN-cd8085beea05e6d)

[‌《AI大模型解决方案专家培养计划》课程大纲 - 飞书云文档](https://ncnmfdan85y5.feishu.cn/wiki/ODZiw9uoCiHfnQkvOF4cQksRnbd)

如何提升性能

提升状态判断准确度、提升Function Call 准确度、提升RAG 准确度、提升Agent 可控性

| 步骤 | 影响准确率的关键 |
| --- | --- |
| 处理用户的请求 判断是否唤起工作流 | LLM的理解能力 |
| 确定分析框架 | LLM的理解能力、Prompt、Agent |
| 重写用户请求 | LLM的理解能力+生成能力 0 |
| 检索资料 | RAG、Embedding |
| 整合生成回复 | LLM的理解能力+生成能力 |

最初级：只会用Prompt
初级：Agent反思+纠错
中级：多Agent协作、RAG
高级：Fine tune、Embedding定制
顶级：训练垂直行业LLM

模型能力与应用场景 模型能力如何应用在实际产品

做A产品，应该从哪里出发

1、选择有价值的应用场景，然后研究如何通过模型能力实现功能
2、研究各类模型能力边界，然后研究模型能力有何实际应用场景

一些公司A化项目的方向

1、不要一上来就想做一个单独的A产品
2、从一个优化点开始
3、2C产品建议从留存、活跃、互动率上考虑
4、2B从大规模效率出发
5、内部系统丛用户操作复杂度

### 阿里云大模型 课程

[阿里云大模型工程师ACA认证免费课程](https://edu.aliyun.com/course/3126500/)
[阿里云大模型高级工程师ACP认证课程](https://edu.aliyun.com/course/3130200/)

[10分钟搭建一个拥有大模型能力以及专属知识库的钉钉机器人\_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/use-cases/add-an-ai-assistant-to-your-dingtalk-in-10-minutes)
[10分钟让微信公众号成为智能客服\_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/use-cases/add-an-ai-assistant-to-your-wechat-in-10-minutes)

## 模型

### deepseek

vllm + ray

deepseek janus pro 多模态大模型炸裂出场，transformer架构，没有走diffusion路线

[deepseek-ai/DeepSeek-R1 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-R1)

[DeepSeek Token 用量计算 | DeepSeek API Docs](https://api-docs.deepseek.com/zh-cn/quick_start/token_usage)

[KTransformers 4090单卡跑671B DeepSeek-R1 - 知乎](https://zhuanlan.zhihu.com/p/23212558318)
[单卡RTX4090部署R1满血版之KTransformers篇-阿朱](https://mp.weixin.qq.com/s/g3JsrLUuMXDX-8lSSzb06A)

[4090单卡部署QWen2.5-VL视觉模型 - 阿朱](https://mp.weixin.qq.com/s/Ha-J5uUKk7XUqMfW_VEqHg)

[不到 4 万元的 DeepSeek-R1-671B-Q8 部署方案 - 腾讯玄武实验室](https://mp.weixin.qq.com/s/vIrvbVJ6Nv00Ehre1zZwMw)

[Deepseek V3 0324 modelfile : r/ollama](https://www.reddit.com/r/ollama/comments/1jpk3ty/deepseek_v3_0324_modelfile)

#### quantized DeepSeek-R1

[Deployment-ready reasoning with quantized DeepSeek-R1 models | Red Hat Developer](https://developers.redhat.com/articles/2025/03/03/deployment-ready-reasoning-quantized-deepseek-r1-models#)

INT4 models recover 97%+ accuracy for 7B and larger models, with the 1.5B model maintaining ~94%.

#### 内存使用量计算

[DeepSeek 本地化部署指南：硬件适配全解析-DeepSeek技术社区](https://deepseek.csdn.net/67c14dbab8d50678a2421282.html)

内存使用计算

fp16: (16/8)*70B = 140GB
fp16: (16/8)*671B = 1342 GB
int8: (8/8)*671B = 671 GB
int4: (4/8)*671B = 335.5 GB
int1: (1/8)*671B = 83.875 GB

对于 DeepSeek-R1-32B 模型，若以常规的 fp16 精度计算，每个参数占用 2 字节（16/8），基础参数占用为 320 亿 × 2 字节 = 640 亿字节，约 64GB。乘以安全系数 1.3 后，基础参数占用提升至 83.2GB。在实际运行中，每处理一定数量的上下文 token，会产生额外的上下文开销。假设处理 4096 tokens 的上下文会增加 2GB 的上下文开销（具体数值会因模型和运行环境略有差异），当处理 8192 个上下文 token 时，上下文扩展量为 2GB × 2 = 4GB。若再考虑系统缓存可能占用 3GB（实际会因系统配置不同而变化），则 总显存需求 = 83.2GB + 4GB + 3GB = 90.2GB。这表明在部署 DeepSeek-R1-32B 模型时，单卡显存若低于 90.2GB，可能无法稳定运行

1. 每个参数占用 2 字节，基础参数占用为 320 亿 × 2 字节 = 640 亿字节，约 64GB
2. 乘以安全系数 1.3 * 64GB = 83.2GB
3. 处理 8192 个上下文 token 时，上下文扩展量为 2GB × 2 = 4GB
4. 总显存需求 = 83.2GB + 4GB + 3GB = 90.2GB

[Run DeepSeek-R1 Dynamic 1.58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)
DeepSeek R1 has 61 layers

n (offload) = VRAM(GB) / Filesize(GB) × n (layers) − 4

#### The Temperature Parameter

[The Temperature Parameter | DeepSeek API Docs](https://api-docs.deepseek.com/quick_start/parameter_settings)

The default value of temperature is 1.0. We recommend users to set the temperature according to their use case listed in below.

Coding / Math                     0.0
Data Cleaning / Data Analysis     1.0
General Conversation              1.3
Translation                       1.3
Creative Writing / Poetry         1.5

#### 模型能力对比

| 对比项 | GPT (OpenAI) | DeepSeek-R1 |
| --- | --- | --- |
| 模型架构 | Transformer解码器架构 (全参数激活) | 混合专家模型 (MoE，部分参数激活) |
| 参数规模 | GPT-4：1.76万亿参数（全参数计算）01未 公开 | 总参数6,710亿，每次仅激活370亿 |
| 训练方法 | D 监督微调（SFT）+强化学习(RLHF) | 纯强化学习(RL)，不依赖 SFT |
| 推理能力 | 强大的自然语言理解和推理能力 | 优秀的推理能力，具备自我验证和反思 |
| 计算资源 | 需要高计算成本和能耗 | MoE 机制降低计算消耗 |
| 并行训练 | 基于标准的分布式训练框架 | 采用 HAI-LLM 并行框架(16路流水线并行、64路专家并行) |
| 开源情况 | GPT为闭源 | DeepSeek-R1为开源 |
| 适用场景 | 通用对话、代码生成、复杂推理 | 适用于高效推理任务，低成本大规模部署 |

## GPT 项目

GPT (Generative Pre-trained Transformer)

[ChatGPT](https://chat.openai.com/chat)
[What Is ChatGPT Doing … and Why Does It Work?—Stephen Wolfram Writings](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
[ChatGPT为啥这么强：万字长文详解 by WolframAlpha之父-今日头条](https://www.toutiao.com/article/7200604582392087095)
[ChatGPT学习资料合集 Original 吕建伟 阿朱说](https://mp.weixin.qq.com/s/ZApy_d873Y1DEmGc6NjplQ)

[如何使用ChatGPT API训练自定义知识库AI聊天机器人 - 闪电博](https://www.wbolt.com/how-to-train-ai-chatbot.html)
[How to Train an AI Chatbot With Custom Knowledge Base Using ChatGPT API | Beebom](https://beebom.com/how-train-ai-chatbot-custom-knowledge-base-chatgpt-api/)
[Fine-tuning - OpenAI API](https://platform.openai.com/docs/guides/fine-tuning)

[人人都能懂的ChatGPT解读_AI_张杰_InfoQ精选文章](https://www.infoq.cn/article/VWrPIRvRg6E3O74q7PtL)

[zhayujie/chatgpt-on-wechat: 使用ChatGPT搭建微信聊天机器人，基于ChatGPT3.5 API和itchat实现。Wechat robot based on ChatGPT, which using OpenAI api and itchat library.](https://github.com/zhayujie/chatgpt-on-wechat)

[ConnectAI-E/Dingtalk-OpenAI: 🔔 钉钉 & 🤖 GPT-3.5 让你的工作效率直接起飞 🚀 私聊群聊方式、单聊串聊模式、角色扮演、图片创作 🚀](https://github.com/ConnectAI-E/Dingtalk-OpenAI)

### AI 应用开发

[‌‍⁤​个人开源 AI 知识库 - 飞书云文档](https://tffyvtlai4.feishu.cn/wiki/OhQ8wqntFihcI1kWVDlcNdpznFf)
[大模型 AI 应用全栈开发知识体系 v1.3.1 - 飞书云文档](https://agiclass.feishu.cn/docx/Z3Aed6qXboiF8gxGuaccNHxanOc)
[GitHub - deepseek-ai/awesome-deepseek-integration: Integrate the DeepSeek API into popular softwares](https://github.com/deepseek-ai/awesome-deepseek-integration)
[火山引擎 高代码 Python SDK Arkitect volcengine/ai-app-lab](https://github.com/volcengine/ai-app-lab/)

[AI 全栈学员部分作品集 - 飞书云文档](https://agiclass.feishu.cn/docx/M5xydPVjWovB9exHBjDc7IMYnub)

[知乎《AI 大模型全栈工程师》课程大纲（第 05 期） - 飞书云文档](https://agiclass.feishu.cn/docx/KjFSdqxTZoDDfcxzikHcjjx0nDg)

[AI大模型颠覆程序员的价值 - 程序员的AI大模型进阶之旅0122期](https://www.zhihu.com/xen/market/training/training-video/1730906752995045376/1730906966715797506?education_channel_code=ZHZN-d62bb90dfad9e02) 讲解了自训练 微调等方法
[大模型应用开发技术体系串讲 - 程序员的AI大模型进阶之旅0125期](https://www.zhihu.com/xen/market/training/training-video/1731335160308744192/1731335415037206528?education_channel_code=ZHZN-cd8085beea05e6d) 孙志刚老师回答问题
[使⽤ Assistants API快速搭建领域专属AI - 程序员的AI大模型进阶之旅0122期](https://www.zhihu.com/xen/market/training/training-video/1730906752995045376/1730907032264232960?education_channel_code=ZHZN-d62bb90dfad9e02)

大模型领域岗位推荐
 · 大模型训练师
 · 大模型算法工程师
 · 提示词工程师
 · 大模型全栈开发工程师
 · 大模型方向产品经理
 · 大模型方向项目经理

继续本岗位 · 大大提升效率，横向卷同行，纵向卷上下游
成为超级个体 独立开发者，做自己的小老板
成为大模型训练师 做公司的技术核心
独立创业 凭大模型垂直落地能力解决独有场

使用 Assistants API 快速搭建领域专属AI助手
Demo框架 及具体实现 Streamlit 简介— A faster way to build and share data apps

[ChatTTS-ui: 一个简单的本地网页界面，直接使用ChatTTS将文字合成为语音，同时支持对外提供API接口。](https://github.com/jianchang512/ChatTTS-ui)

### Text-to-SQL

[Canner/WrenAI: Open-source GenBI AI Agent that empowers data-driven teams to chat with their data to generate Text-to-SQL, charts, spreadsheets, reports, and BI](https://github.com/Canner/WrenAI)

[CodePhiliaX/Chat2DB: AI-driven database tool and SQL client, The hottest GUI client, supporting MySQL, Oracle, PostgreSQL, DB2, SQL Server, DB2, SQLite, H2, ClickHouse, and more.](https://github.com/codePhiliaX/Chat2DB)

[vanna-ai/vanna: Chat with your SQL database 📊. Accurate Text-to-SQL Generation via LLMs using RAG.](https://github.com/vanna-ai/vanna)

## Text to Image

[DALL·E 2](https://openai.com/product/dall-e-2)

[comfyanonymous/ComfyUI: The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.](https://github.com/comfyanonymous/ComfyUI)

[ComfyUI：搭积木一样构建专属于自己的AIGC工作流（保姆级教程）](https://mp.weixin.qq.com/s/sr4Cpd6UAyCf75Jm2D8YTQ)

[10款AI绘画生成器，人人都是插画师！](https://pixso.cn/designskills/10-ai-paint-builders/)

[Text To Image - AI Image Generator API | DeepAI](https://deepai.org/machine-learning-model/text2img)

[文心一格 - AI艺术和创意辅助平台](https://yige.baidu.com/creation)

[22个国内AI绘画网站汇总](https://zl49so8lbq.feishu.cn/wiki/MdzYw0mtki3OPGkPk03cnd9hnfg#Uaaidf4Mro82FFx8ucic8MUOnBf)

### image prompt

[Guide for prompt writing | BoostPixels](https://boostpixels.com/guide)
[midjourney史上最全教程-持续更新 - Feishu Docs](https://nw3t0riwqkt.feishu.cn/docx/NCVdd118toPLkRxKBexcQZiunJZ)

[墨本关键词助手](https://www.mbprompt.com/#/)
[MidJourney Prompt Tool](https://prompt.noonshot.com/)

### Stable Diffusion

[Stable Diffusion Models: a beginner's guide - Stable Diffusion Art](https://stable-diffusion-art.com/models/)
[ControlNet: A Complete Guide - Stable Diffusion Art](https://stable-diffusion-art.com/controlnet/)
[常用的ControlNet以及如何在Stable Diffusion WebUI中使用 - 知乎](https://zhuanlan.zhihu.com/p/620074109)

[Install Stable Diffusion on Mac](https://uxplanet.org/install-stable-diffusion-ui-on-mac-beginners-guide-351e40a9e8e2)
[webui Online Services · AUTOMATIC1111/stable-diffusion-webui Wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)
[Stability-AI/generative-models: Generative Models by Stability AI](https://github.com/Stability-AI/generative-models)
[Mikubill/sd-webui-controlnet: WebUI extension for ControlNet](https://github.com/Mikubill/sd-webui-controlnet)

### MidJourney 案例

[【Midjourney教程】设计麻瓜也能10分钟上架一套表情包](https://mp.weixin.qq.com/s/FagQ3HdAnx-HLfJK4NRMBQ)

### Mistral-7B

来自法国的开源大模型

[最好的7B模型易主，免费开源可商用，来自“欧洲的OpenAI”-今日头条](https://www.toutiao.com/article/7287811935905546763/)
[mistralai (Mistral AI_)](https://huggingface.co/mistralai)

## OpenAI doc

[API Reference - OpenAI API](https://platform.openai.com/docs/api-reference/introduction)

[Playground - OpenAI API](https://platform.openai.com/playground)

[Assistants overview - OpenAI API](https://platform.openai.com/docs/assistants/overview)## prompt

[Maximizing the Potential of LLMs: A Guide to Prompt Engineering](https://www.ruxu.dev/articles/ai/maximizing-the-potential-of-llms/)

[f/awesome-chatgpt-prompts: This repo includes ChatGPT prompt curation to use ChatGPT better.](https://github.com/f/awesome-chatgpt-prompts)

[提示词技巧](https://mp.weixin.qq.com/s/eqIqbbyqlgkCU78SKuZCMw)

```sh
curl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \
-H "Authorization: Bearer $DASHSCOPE_API_KEY" \
-H "Content-Type: application/json" \
-d '{
    "model": "qwen-plus",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "你是谁？"
        }
    ]
}'
```

### [Learn Prompting](https://learnprompting.org/zh-Hans/docs/intro)

#### Learn Prompting Sample

[🟡 Coding Assistance | Learn Prompting](https://learnprompting.org/docs/basic_applications/coding_assistance)

1. act like a senior developer
2. as a very junior developer
3. You can also dictate that it have a certain area of expertise (e.g., sorting algorithms) or number of years of experience
4. Act as Microsoft SQL Server.

##### English translator and Improver

Prompt: I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is "lovin istanbul and the city"

##### Interviewer

Prompt: I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the position position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is "Hi"

##### English Pronunciation Helper

Prompt: I want you to act as an English pronunciation assistant for Turkish speaking people. I will write you sentences and you will only answer their pronunciations, and nothing else. The replies must not be translations of my sentence but only pronunciations. Pronunciations should use Turkish Latin letters for phonetics. Do not write explanations on replies. My first sentence is "how the weather is in Istanbul?"

##### Travel Guide

Prompt: I want you to act as a travel guide. I will write you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion request is ""I am in Istanbul/Beyoğlu and I want to visit only museums."

## 理论知识

[Deepseek大模型推理算法其实很简单 | 陈经](https://mp.weixin.qq.com/s/SaK9mlj6NCKxEFig6KFGVQ)

1. Function Call:基于LLM的语言理解能力，通过理解语义，自主决策使用某项工具，并结构化调用
2. RAG:通过句子或段落的语义相似度比较，检索相关资料，在资料支持下生成回复(Retrieval-Augmented Generation)

### 量化

[极简教程，大模型量化实践，1张4090跑QwQ？](https://mp.weixin.qq.com/s/27EsyfQNXk_A73I8FMz61A)

量化是一种将模型的浮点权重（通常是 32 位或 16 位）转换为低位整数（如 2 位、4 位、8 位等）的技术，目的是减少模型的存储空间和计算资源需求，同时尽可能保持模型的性能。

最主流的有以下量化的方法：

| 方法类型  | 代表技术      | 核心特征              | 适用场景      |
|-------|-----------|-------------------|-----------|
| 训练后量化 | GPTQ      | 4bit 权重量化，动态反量化推理 | GPU 加速推理  |
| 感知量化  | AWQ       | 激活值引导的智能量化        | 精度敏感型任务   |
| 混合推理  | GGUF/GGML | CPU-GPU 异构计算框架    | 边缘设备部署    |

然后这里还设计不同的量化位宽，其实常见也就K-Quants 增强系列

| 量化类型 | 位宽策略               | 存储效率   | 技术特点      |
|------|--------------------|--------|-----------|
| Q2_K | 2.56bit/权重         | 超高压缩率  | 16 块超块结构  |
| Q3_K | 3.44bit/权重（type-0） | 性能优先   | 平衡压缩与精度   |
| Q4_K | 4.5bit/权重（type-1）  | 通用型    | 主流部署方案    |
| Q5_K | 5.5bit/权重          | 精度增强   | 关键层保护     |
| Q6_K | 6.56bit/权重         | 准无损压缩  | 复杂任务保留    |
| Q8_K | 8bit 中间结果量化        | 资源充足场景 | 梯度计算优化    |

还有常见到的 IQ 系列量化方法

- IQ4_NL：4 位量化，超块包含 256 个权重，权重 w 通过 super_block_scale 和 importance matrix 计算得到
- IQ4_XS：4 位量化，超块包含 256 个权重，每个权重占用 4.25 位，通过 super_block_scale 和 importance matrix 计算得到

目前最流行的是混合量化策略，主打一个动态精度分配

- K_M 混合策略：对 attention.wv 等关键张量采用 Q6_K，其余使用 Q4_K
- K_S 均质策略：全模型统一量化配置，所有张量均使用 Q4_K

其中

- K：表示 k-quants 量化方法
- S (Small)：简单量化，所有张量均使用相同位数量化
- M (Mixed)：混合量化，对关键张量使用更高精度的量化，其余使用标准精度

在相同位数下，K_M 系列模型（比如今天我们要演示的 QwQ-32B-Q4_K_M）通常在模型大小和性能之间取得最佳平衡，是推荐的选择。

### 常用数据类型

[大模型量化技术（Quantization）可视化指南](https://mp.weixin.qq.com/s/L162LDMXXzAlTQhEcuOiMw)

首先，我们对比分析常规数据类型与32位（即 全精度 或 FP32）表示方式的差异：

- FP32 浮点格式 全精度
- FP16 浮点格式 半精度
- BF16：虽然与FP16占用相同的存储空间（16位），但其数值表示范围更广，因此被广泛应用于深度学习领域。
- INT8：当我们将比特位数进一步降低时，便会进入基于整数的表示方法范畴，而不再使用浮点表示法。以FP32浮点格式转换为8比特的INT8为例，其比特位数将缩减至原始值的四分之一

具体硬件条件下，基于整数的运算速度可能优于浮点运算，但这一优势并非绝对成立。然而，通常当使用较少比特位数时，计算速度会显著提升。

实际上，我们无需将完整的FP32范围[-3.4e38, 3.4e38]映射到INT8，只需找到将模型参数的实际数据范围适配到INT8的方法即可。

常见的压缩/映射方法包括 **对称量化** 和 **非对称量化** ，这些都属于 线性映射 的范畴。

### 内存计算

计算模型在给定数值时所需的内存空间: memory = nr_bits/8 * nr_params

注意：实际应用中，推理过程所需的(V)RAM容量还需考虑其他因素，例如上下文长度和模型架构设计。

大多数模型原生采用32位浮点数（常称为 全精度）表示, DeepSeek模型是 16位浮点数模型，对于70B（700亿）参数的模型，加载模型需要占用140GB内存空间。计算公式为

fp16: (16/8)*70B ~= 140GB
fp16: (16/8)*671B = 1342 GB 内存
int8: (8/8)*671B = 671 GB 内存
int4: (4/8)*671B = 335.5 GB 内存

## 学习教程

[Getting Started With MachineLearning (all in one)](https://pan.baidu.com/s/1tNXYQNadAsDGfPvuuj7_Tw)

[microsoft/ML-For-Beginners: 12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all](https://github.com/microsoft/ML-For-Beginners)

[microsoft/AI-For-Beginners: 12 Weeks, 24 Lessons, AI for All!](https://github.com/microsoft/AI-For-Beginners)

1. [第一章：全世界最简单的机器学习入门指南](https://zhuanlan.zhihu.com/p/24339995)
2. [第二章：用机器学习制作超级马里奥的关卡](https://zhuanlan.zhihu.com/p/24344720)
3. [第三章:图像识别【鸟or飞机】？深度学习与卷积神经网络] https://zhuanlan.zhihu.com/p/24524583
4. [第四章：用深度学习识别人脸](https://zhuanlan.zhihu.com/p/24567586)
5. [第五章：Google 翻译背后的黑科技：神经网络和序列到序列学习](https://zhuanlan.zhihu.com/p/24590838)
6. [第六章：如何用深度学习进行语音识别？](https://zhuanlan.zhihu.com/p/24703268)

### 召回率和精确率

[算法工程师说的召回是什么意思？ - 拒海空间](https://refusea.com/?p=1546)

在算法工程中，"召回"是一个重要的概念，特别是在信息检索和机器学习领域。

召回率（Recall）是一种衡量模型预测能力的指标，特别是模型识别出相关实例的能力。

具体来说，**召回率是指模型正确识别出的正例（真正例）占所有实际正例（真正例+假反例）的比例**。换句话说，它是模型找到的相关实例占所有相关实例的比例。

例如，如果我们有一个用于检测垃圾邮件的模型，那么召回率就是模型正确标记为垃圾邮件的邮件数量占所有实际垃圾邮件数量的比例。

召回率和精确率（Precision）通常一起使用，以获得模型性能的全面视图。

**精确率是模型预测为正例的实例中实际为正例的比例**，而召回率则关注模型能找到多少实际的正例。

例子
如果有 1000 邮件需要检测，算法检测出有 800 垃圾邮件，实际这 800 里真正的垃圾邮件是 600，同时算法还遗漏了 50 垃圾邮件。那么召回率和精确率是多少？怎么计算的？

在这个例子中，我们可以先定义以下几个概念：

真正例（True Positive，TP）：算法正确地预测为垃圾邮件的邮件数量，即600封。
假正例（False Positive，FP）：算法错误地预测为垃圾邮件的邮件数量，即800（算法预测为垃圾邮件的数量）- 600（真正的垃圾邮件数量）= 200封。
假反例（False Negative，FN）：算法错误地预测为非垃圾邮件的邮件数量，即遗漏的垃圾邮件数量，即50封。
根据这些定义，我们可以计算召回率和精确率：

召回率（Recall）= 真正例 / (真正例 + 假反例) = 600 / (600 + 50) = 0.923，或者说92.3%。
精确率（Precision）= 真正例 / (真正例 + 假正例) = 600 / (600 + 200) = 0.75，或者说75%。
所以，这个垃圾邮件检测算法的召回率是92.3%，精确率是75%。

### 简介

线性回归主要用来解决连续值预测的问题，逻辑回归用来解决分类的问题，输出的属于某个类别的概率，工业界经常会用逻辑回归来做排序

## LLM 介绍

[Getting Started With Large Language Models - DZone Refcardz](https://dzone.com/refcardz/getting-started-with-large-language-models)

文心大模型ERNIE是百度发布的产业级知识增强大模型，涵盖了NLP大模型和跨模态大模型。

https://github.com/PaddlePaddle/ERNIE

## ollama

[ollama/ollama: Get up and running with large language models.](https://github.com/ollama/ollama)
[ollama/ollama - Docker Image | Docker Hub](https://hub.docker.com/r/ollama/ollama)
[ollama Importing a model](https://github.com/ollama/ollama/blob/main/docs/import.md)
[ollama api](https://github.com/ollama/ollama/blob/main/docs/api.md)

```sh
# 设置好环境变量后，运行 ollama run 命令即可让 Ollama 使用指定的 GPU
export CUDA_VISIBLE_DEVICES=0,1
# Docker 使用所有gpu --gpus=all
# Docker 只使用第 0 和第 1 张 GPU 卡 --gpus=0,1
docker run -d --env OLLAMA_HOST=0.0.0.0:11434 -v /data/docker/ollama/ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
alias ollama='docker exec -it ollama ollama'

# Show model information
ollama show llama3.2
# List models on your computer
ollama list
# List which models are currently loaded
ollama ps
# Start Ollama
ollama serve

# Run a model
ollama run modelName
# Show more info: duration and eval rate
ollama run modelName --verbose
# Stop a running model
ollama stop modelName
# Remove a model
ollama rm modelName

#
# Run with parameter: num_ctx Context Window
ollama run llama3 –set parameter num_ctx 4096 --temperature 0.7 --top-p 0.9 --memory-limit 8GB --batch-size 8
# 检查系统资源
ollama run llama2 --debug
# 使用性能分析工具
ollama run llama2 --profile

# 使用curl测试API
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello, how are you?"
}'

curl http://localhost:11434/api/tags

# 批处理
ollama run llama2 < batch_prompts.txt > responses.txt

# [ollama Model library](https://ollama.com/library)

# deepseek
# deepseek 1.5b ~ 32b 上下文长度 32,768  最大输入 32,768
# [DeepSeek R1和DeepSeek V3 API_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/developer-reference/deepseek#94082c580cot9)
# [deepseek-r1:1.5b](https://ollama.com/library/deepseek-r1:1.5b)
# DeepSeek-R1-Distill-Qwen-1.5B
ollama run deepseek-r1:1.5b
# DeepSeek-R1-Distill-Qwen-7B
ollama run deepseek-r1:7b
# DeepSeek-R1-Distill-Qwen-32B
ollama run deepseek-r1:32b

# DeepSeek-R1-Distill-Llama-70B
# Error: model requires more system memory (37.3 GiB) than is available (24.7 GiB)
ollama run deepseek-r1:70b

# 34b 执行太慢
ollama run codellama:34b

# 70b Error: model requires more system memory (31.2 GiB) than is available (27.3 GiB)
ollama run codellama:70b

# Cline uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.5 Sonnet for its advanced agentic coding capabilities.
ollama run qwen2.5:32b

# [DeepSeek Coder](https://deepseekcoder.github.io/)
# [DeepSeek-Coder/Evaluation/HumanEval - deepseek-ai/DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder/tree/main/Evaluation/HumanEval)
#
# DeepSeek-Coder-V2 comes in two primary types: Instruct and Base.
# Base model
# A base model is a general-purpose language model trained on a large corpus of text (e.g., code, documentation, and natural language). It has no specific fine-tuning for instruction-following or task-oriented behaviour.
#
# Instruct model
# An instruct model is a fine-tuned version of a base model, optimized to follow instructions and perform specific tasks. It is trained on datasets containing instruction-response pairs (e.g., “Write a SQL query to find duplicates” → “SELECT …”). Excels at task-oriented interactions (e.g., debugging, refactoring, answering questions).

# [second-state-DeepSeek-Coder-V2-Lite-Instruct-GGUF: Mirror of https://huggingface.co/second-state/DeepSeek-Coder-V2-Lite-Instruct-GGUF](https://gitee.com/hf-models/second-state-DeepSeek-Coder-V2-Lite-Instruct-GGUF)
# DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf  Quant method: Q4_K_M
# medium, balanced quality - recommended
# DeepSeek-Coder-V2-Instruct 236B
# DeepSeek-Coder-V2-Lite-Instruct 16B
ollama run deepseek-coder-v2:16b-lite-instruct-q4_K_M

# [MFDoom/deepseek-r1-tool-calling:8b](https://ollama.com/MFDoom/deepseek-r1-tool-calling:8b)
# DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen. With Tool Calling support.
ollama run MFDoom/deepseek-r1-tool-calling:8b
```

### ollama 命令

[ollama的命令注解_ollama命令行-CSDN博客](https://blog.csdn.net/sunyuhua_keyboard/article/details/141174683)

```sh
>>> /?
Available Commands:
  /set            Set session variables
  /show           Show model information
  /load <model>   Load a session or model
  /save <model>   Save your current session
  /clear          Clear session context
  /bye            Exit
  /?, /help       Help for a command
  /? shortcuts    Help for keyboard shortcuts

Use """ to begin a multi-line message.

>>> /set
Available Commands:
  /set parameter ...     Set a parameter
  /set system <string>   Set system message
  /set template <string> Set prompt template
  /set history           Enable history
  /set nohistory         Disable history
  /set wordwrap          Enable wordwrap
  /set nowordwrap        Disable wordwrap
  /set format json       Enable JSON mode
  /set noformat          Disable formatting
  /set verbose           Show LLM stats
  /set quiet             Disable LLM stats
```

主命令
/set: 用于设置会话参数和配置。例如，设置消息格式、启用或禁用历史记录等。
/show: 显示模型的相关信息，如当前加载的模型的名称、版本等。
/load : 加载一个特定的模型或会话。你可以指定一个模型的名称或路径来加载它。
/save : 保存当前的会话状态或模型。你可以将当前会话或模型的配置保存为一个文件，以便以后使用。
/clear: 清除会话上下文。这将删除当前会话中的所有历史记录或对话内容。
/bye: 退出会话。这个命令将结束当前与模型的对话，并退出程序。
/? 或 /help: 显示帮助信息。如果你需要关于某个命令的详细信息，可以使用这些命令。
/? shortcuts: 显示键盘快捷键的帮助信息。这可以帮助你更快速地进行操作。

/set 子命令
/set parameter …: 设置某个参数。这可能包括一些特定的配置项，用于控制模型的行为或输出方式。
/set system : 设置系统消息。你可以提供一个字符串作为系统消息，这通常用于在对话开始时向模型传达背景信息或特定指令。
/set template : 设置提示模板。这允许你定义一个模板，用于格式化你与模型的对话。
/set history: 启用历史记录。这意味着模型会保存你当前会话中的对话历史，以便稍后参考或使用。
/set nohistory: 禁用历史记录。使用这个命令后，模型将不会保存会话历史。
/set wordwrap: 启用自动换行。这在长文本消息的情况下非常有用，可以让文本自动换行以便于阅读。
/set nowordwrap: 禁用自动换行。如果不需要自动换行，可以使用这个命令。
/set format json: 启用JSON模式格式化输出。这会将模型的响应格式化为JSON格式，方便结构化数据的处理。
/set noformat: 禁用格式化输出。如果不需要任何特定格式的输出，可以使用这个命令。
/set verbose: 启用详细模式，这会显示与LLM相关的统计信息，如响应时间、消耗资源等。
/set quiet: 禁用详细模式。启用后，将不会显示与LLM相关的统计信息，输出会更简洁。

应用场景

- 管理会话: 你可以使用 /load 和 /save 命令来保存和加载特定的会话状态，从而在不同时间点继续先前的工作。
- 自定义消息格式: 使用 /set template 和 /set format json 可以自定义和控制模型输出的格式，适用于不同的应用场景。
- 调试和性能监控: 通过 /set verbose 和 /set quiet，你可以控制是否查看模型的统计信息，这在调试或性能监控时特别有用。

这些命令和设置可以帮助你更灵活地控制模型的行为和会话的管理，使其更好地适应你的使用需求。

### Where are models stored?

macOS: ~/.ollama/models
Linux: /usr/share/ollama/.ollama/models
Windows: %userprofile%\.ollama\models

默认的模型保存路径位于C盘，（%userprofile%\.ollama\models），可以通过设置 OLLAMA_MODELS 进行修改，然后重启终端，重启ollama服务（需要去状态栏里关掉程序）

```bat
setx OLLAMA_MODELS "D:\ollama_model"
OLLAMA_MODELS=D:\workspace\ollama\models
```

### ollama Setting environment variables on Windows

[ollama/docs/faq](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-windows)

On Windows, Ollama inherits your user and system environment variables.

1. First Quit Ollama by clicking on it in the task bar.
2. Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for environment variables.
3. Click on Edit environment variables for your account.
4. Edit or create a new variable for your user account for OLLAMA_HOST, OLLAMA_MODELS, etc.
   1. OLLAMA_HOST=0.0.0.0:11434
5. Click OK/Apply to save.
6. Start the Ollama application from the Windows Start menu.

### keep a model loaded in memory or make it unload immediately?

The `keep_alive` parameter can be set to:

- a duration string (such as "10m" or "24h")
- a number in seconds (such as 3600)
- any negative number which will keep the model loaded in memory (e.g. -1 or "-1m")
- '0' which will unload the model immediately after generating a response

```sh
# to preload a model and leave it in memory use:
curl http://localhost:11434/api/generate -d '{"model": "deepseek-r1:7b", "keep_alive": -1}'

# To unload the model and free up memory use:
curl http://localhost:11434/api/generate -d '{"model": "MFDoom/deepseek-r1-tool-calling:8b", "keep_alive": 0}'
```

### config Ollama

#### context window size

[How to Increase Ollama Context Size: A Complete Step-by-Step Guide - Deep AI — Leading Generative AI-powered Solutions for Business](https://deepai.tn/glossary/ollama/how-increase-ollama-context-size/)
[GGUF specification](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)
huggingface 上或模型的 config 文件中记录的大小：sequence_len: 4096

By default, Ollama uses a context window size of 2048 tokens.

show the context size *really is* in the current model being run `ollama show (model name)`. In the ollama CLI, `/show info`

```sh
ollama show deepseek-r1:7b
# Model
#   architecture        qwen2
#   parameters          7.6B
#   context length      131072
#   embedding length    3584
#   quantization        Q4_K_M

# Parameters
#   stop    "<｜begin▁of▁sentence｜>"
#   stop    "<｜end▁of▁sentence｜>"
#   stop    "<｜User｜>"
#   stop    "<｜Assistant｜>"

# When using the API, specify the num_ctx parameter:
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "options": {
    "num_ctx": 4096
  }
}'

/set parameter num_ctx 131072
/save deepseek-r1:7b-ctx-128k

/set parameter num_ctx 65536
/save deepseek-r1:7b-ctx-64k
/set parameter num_ctx 32768
/save deepseek-r1:7b-ctx-32k
/set parameter num_ctx 16384
/save deepseek-r1:7b-ctx-16k
/set parameter num_ctx 8192
/save deepseek-r1:7b-ctx-8k
/set parameter num_ctx 6144
/save deepseek-r1:7b-ctx-6k
/set parameter num_ctx 4096
/save deepseek-r1:7b-ctx-4k
/set parameter num_ctx 3072
/save deepseek-r1:7b-ctx-3k
/set parameter num_ctx 2048
/save deepseek-r1:7b-ctx-2k


ollama ps
# NAME              ID              SIZE      PROCESSOR    UNTIL
# deepseek-r1:7b-ctx-128k    946b57ccb619    12 GB    100% CPU     4 minutes from now
# deepseek-r1:7b-ctx-64k    af0516d55326    13 GB    58%/42% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-64k    af0516d55326    8.4 GB    100% CPU     4 minutes from now  2025年3月3日
# deepseek-r1:7b-ctx-32k    1b1debea5066    9.5 GB    39%/61% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-16k    3fc83f5dea49    7.2 GB    19%/81% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-8k    fdb679a1bd50    6.3 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-6k    edd7d65f4ea1    6.1 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-4k    96cae7f73d2b    6.0 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-3k    25f4b7c66be3    5.5 GB    100% GPU     4 minutes from now
# deepseek-r1:7b-ctx-2k    9be990020b49    5.4 GB    100% GPU     4 minutes from now
# deepseek-r1:7b    0a8c26691023    5.4 GB    100% GPU     4 minutes from now
```

### cline with local deepseek

本地允许 deepseek 32b 模型，cline 调用时报错
[Cline is having trouble... · Issue #1094 · cline/cline](https://github.com/cline/cline/issues/1094)

```log
Cline uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.5 Sonnet for its advanced agentic coding capabilities.

```

## vLLM

A tool designed to run LLMs very efficiently, especially when serving many users at once.
[Ollama vs VLLM: Which Tool Handles AI Models Better? | by Naman Tripathi | Medium](https://medium.com/@naman1011/ollama-vs-vllm-which-tool-handles-ai-models-better-a93345b911e6)

[Welcome to vLLM — vLLM](https://docs.vllm.ai/en/stable/)
[aneeshjoy/vllm-windows: Docker compose to run vLLM on Windows](https://github.com/aneeshjoy/vllm-windows)
[vllm/vllm-openai Tags | Docker Hub](https://hub.docker.com/r/vllm/vllm-openai/tags)

[AutoAWQ — vLLM](https://docs.vllm.ai/en/latest/features/quantization/auto_awq.html) To create a new 4-bit quantized model, you can leverage AutoAWQ. Quantization reduces the model’s precision from BF16/FP16 to INT4 which effectively reduces the total model memory footprint. The main benefits are lower latency and memory usage.

To determine whether a given model is supported, you can check the config.json file inside the HF repository. If the "architectures" field contains a model architecture listed below, then it should be supported in theory. [Supported Models — vLLM](https://docs.vllm.ai/en/v0.6.5/models/supported_models.html)

### vllm api

```sh
# Test by accessing the /models endpoints
curl http://127.0.0.1:8003/v1/models
```

llm chat completion
[API Reference - OpenAI API](https://platform.openai.com/docs/api-reference/chat/create)

```sh
curl -X POST http://127.0.0.1:9997/v1/chat/completions \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "qwen2.5-instruct",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "What is the largest animal?"
        }
    ]
  }'
```

embeddings [API Reference - OpenAI API](https://platform.openai.com/docs/api-reference/embeddings)

```sh
curl -X 'POST' 'http://localhost:9997/v1/embeddings' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "bge-m3",
    "input": "Hello, world!"
  }'
```

rank

```sh
curl http://localhost:9997/v1/rerank \
  -H "Content-Type: application/json" \
  -d '{
  "model": "bge-reranker-v2-m3",
  "query": "Organic skincare products for sensitive skin",
  "documents": [
    "Eco-friendly kitchenware for modern homes",
    "Biodegradable cleaning supplies for eco-conscious consumers",
    "Organic cotton baby clothes for sensitive skin",
    "Natural organic skincare range for sensitive skin",
    "Tech gadgets for smart homes: 2024 edition",
    "Sustainable gardening tools and compost solutions",
    "Sensitive skin-friendly facial cleansers and toners",
    "Organic food wraps and storage solutions",
    "All-natural pet food for dogs with allergies",
    "Yoga mats made from recycled materials"
  ],
  "top_n": 3
}'

INFO 04-10 11:11:21 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000
INFO 04-10 11:11:21 [launcher.py:26] Available routes are:
INFO 04-10 11:11:21 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD
INFO 04-10 11:11:21 [launcher.py:34] Route: /docs, Methods: GET, HEAD
INFO 04-10 11:11:21 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 04-10 11:11:21 [launcher.py:34] Route: /redoc, Methods: GET, HEAD
INFO 04-10 11:11:21 [launcher.py:34] Route: /health, Methods: GET
INFO 04-10 11:11:21 [launcher.py:34] Route: /load, Methods: GET
INFO 04-10 11:11:21 [launcher.py:34] Route: /ping, Methods: POST, GET
INFO 04-10 11:11:21 [launcher.py:34] Route: /tokenize, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /detokenize, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/models, Methods: GET
INFO 04-10 11:11:21 [launcher.py:34] Route: /version, Methods: GET
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/chat/completions, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/completions, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/embeddings, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /pooling, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /score, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/score, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /rerank, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v1/rerank, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /v2/rerank, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /invocations, Methods: POST
INFO 04-10 11:11:21 [launcher.py:34] Route: /metrics, Methods: GET
```

### vllm run model

```sh
docker pull vllm/vllm-openai:v0.8.3

# 指定程序只能使用编号为 0 和 1 的 GPU。这对于多 GPU 系统非常有用，可以控制程序使用哪些 GPU。
export CUDA_VISIBLE_DEVICES=0,1

# Name or path of the huggingface model to use. Default: “facebook/opt-125m”
--model

# deepseek_r1 think enable
--enable-reasoning --reasoning-parser deepseek_r1

--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]
# The model name(s) used in the API. If multiple names are provided, the server will respond to any of the provided names. The model name in the model field of a response will be the first name in this list. If not specified, the model name will be the same as the --model argument. Noted that this name(s) will also be used in model_name tag content of prometheus metrics, if multiple names provided, metrics tag will take the first one.

--gpu-memory-utilization <value>
# gpu-memory-utilization 是用于设置 GPU 内存利用率的参数，<value> 是一个介于 0 到 1 之间的浮点数，表示 GPU 内存的使用比例
```

```sh
# By default, vLLM downloads models from HuggingFace. If you would like to use models from ModelScope, set the environment variable VLLM_USE_MODELSCOPE before initializing the engine.
# [DeepSeek-R1 · modelscope](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1)
# [DeepSeek-R1-Distill-Qwen-1.5B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)
# default max-model-len: max_seq_len=32768
    # --env "HUGGING_FACE_HUB_TOKEN=hf_oo" \
    # --model mistralai/Mistral-7B-v0.1
docker run --runtime nvidia --gpus all \
    --detach \
    --env TZ=Asia/Shanghai \
    --name vllm \
    --restart always \
    --env VLLM_USE_MODELSCOPE=True \
    --volume //d/workspace/vllm/.cache/:/root/.cache/ \
    --volume //d/workspace/models/:/models/ \
    --publish 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:v0.7.2 \
    --model /models/DeepSeek-R1-Distill-Qwen-32B \
    --served-model-name deepseek-r1:1.5b \
    --max-model-len 15520 \
    --gpu-memory-utilization 0.9

# Deploy with docker on Linux:
docker run --runtime nvidia --gpus all \
    --name my_vllm_container \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
     --env "HUGGING_FACE_HUB_TOKEN=<secret>" \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model deepseek-ai/DeepSeek-R1

# Load and run the model:
vllm serve "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
# Load and run the model:
docker exec -it my_vllm_container bash -c "vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

# Test by accessing the /models endpoints
http://127.0.0.1:8000/v1/models

# Check throughput ( I am running on a RTX 3090 )
http://127.0.0.1:8000/metrics

# Call the server using curl:
curl -X POST "http://localhost:8000/v1/chat/completions" \
    -H "Content-Type: application/json" \
    --data '{
        "model": "deepseek-r1:1.5b",
        "messages": [
            {
                "role": "user",
                "content": "What is the capital of France?"
            }
        ]
    }'

# OpenAI Completions API with vLLM
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
    }'

# OpenAI Chat Completions API with vLLM
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"}
        ]
    }'
```

### vllm 参数

[引擎参数 — vLLM 文档](https://docs.vllm.com.cn/en/latest/serving/engine_args.html)

```sh
# 要使用的 huggingface 模型的名称或路径。 默认值：“facebook/opt-125m”
--model

--api-key API_KEY

# 可选值：auto, generate, embedding, embed, classify, score, reward, transcription
# 模型要使用的任务。即使同一个模型可以用于多个任务，每个 vLLM 实例也只支持一个任务。当模型只支持一个任务时，可以使用 "auto" 来选择它；否则，您必须明确指定要使用的任务。
# 默认值：“auto”
--task

# 限制 PyTorch 可见的 GPU 设备
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
# 将张量并行化到 8 个 GPU 上。这个设置和你的 CUDA_VISIBLE_DEVICES 参数相符，要确认模型可以支持 8-way 的张量并行
--tensor-parallel-size 8

# 用于模型执行器的 GPU 内存 fraction，范围可以从 0 到 1。例如，值为 0.5 意味着 50% 的 GPU 内存利用率。如果未指定，将使用默认值 0.9。这是一个每个实例的限制，并且仅适用于当前的 vLLM 实例。如果您在同一 GPU 上运行另一个 vLLM 实例，则无关紧要。例如，如果您在同一 GPU 上运行两个 vLLM 实例，您可以将每个实例的 GPU 内存利用率设置为 0.5。
# 默认值：0.9
# 设置每个 GPU 最大的显存使用比例为 90%。如果 GPU 上的显存容量较大（例如 24GB 或 40GB），通常设置为 0.9 是安全的，但如果显存较小，或者你有多个进程在同时使用 GPU，可能会导致 Out of Memory 错误
# 确保每个 GPU 上的显存足够，并且没有其他进程占用显存。如果遇到内存溢出，可以尝试调整该值，或者逐步减少每个 GPU 的显存使用。
--gpu-memory-utilization 0.9

# 在显存不足时会使用 CPU 扩展内存，设置每 GPU 的 CPU offloading 空间（GiB）。根据可用系统内存设置，例如 45GB。
# 要卸载到 CPU 的空间（GiB），每个 GPU。默认为 0，表示不卸载。直观地看，此参数可以被视为增加 GPU 内存大小的虚拟方式。例如，如果您有一个 24 GB 的 GPU 并将其设置为 10，实际上您可以将其视为 34 GB 的 GPU。然后您可以加载一个 13B 模型与 BF16 权重，这至少需要 26GB 的 GPU 内存。请注意，这需要快速的 CPU-GPU 互连，因为模型的一部分在每个模型前向传递中从 CPU 内存动态加载到 GPU 内存。
# 默认值：0
--cpu-offload-gb 0

# 模型上下文长度。如果未指定，将自动从模型配置中派生。  "model_max_length": 16384
# 指定模型支持的最大输入长度为 8192 tokens。这个值需要与你的模型大小、显存和并行度相匹配。
--max-model-len 8192

# 要使用的模型实现。可选值：auto, vllm, transformers 默认值：“auto”
# “auto” 将尝试使用 vLLM 实现（如果存在），如果 vLLM 实现不可用，则回退到 Transformers 实现。
# “vllm” 将使用 vLLM 模型实现。
# “transformers” 将使用 Transformers 模型实现。
--model-impl

# 用于量化权重的方法。如果为 None，我们首先检查模型配置文件中的 quantization_config 属性。如果为 None，我们假设模型权重未量化，并使用 dtype 来确定权重的数据类型。
# 可选值：aqlm, awq, deepspeedfp, tpu_int8, fp8, ptpc_fp8, fbgemm_fp8, modelopt, marlin, gguf, gptq_marlin_24, gptq_marlin, awq_marlin, gptq, compressed-tensors, bitsandbytes, qqq, hqq, experts_int8, neuron_quant, ipex, quark, moe_wna16, None
--quantization, -q


# 使用负载均衡，确保它能够在多个 GPU 之间分配工作。
--max-requests
--max-requests-per-gpu

# 检查内存管理 如果模型因为内存不足只在 GPU 0 上运行，可以尝试调整内存分配设置，如环境变量 PYTORCH_CUDA_ALLOC_CONF。有助于减少 CUDA 内存的碎片化，允许模型更有效地使用多个 GPU。
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256

# 启用前缀缓存。
--enable-prefix-caching
# 信任远程代码
--trust-remote-code

# 使用版本1
VLLM_USE_V1=1
```

### vllm GGUF

[GGUF — vLLM](https://docs.vllm.ai/en/stable/features/quantization/gguf.html)

[gguf-split: split and merge gguf per batch of tensors by phymbert · Pull Request #6135 · ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp/pull/6135)
[llama.cpp/docs/docker.md at master · ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md)

```sh
# [llama.cpp gguf-split](https://github.com/ggml-org/llama.cpp/tree/b3785/examples/gguf-split)
# --merge
gguf-split --merge /tmp/ggml-out-q4_0-2-00001-of-00003.gguf /tmp/ggml-out-q4_0-2-merge.gguf

gguf_merge: /tmp/ggml-out-q4_0-2-00001-of-00003.gguf -> /tmp/ggml-out-q4_0-2-merge.gguf
gguf_merge: reading metadata /tmp/ggml-out-q4_0-2-00001-of-00003.gguf ...done
gguf_merge: reading metadata /tmp/ggml-out-q4_0-2-00002-of-00003.gguf ...done
gguf_merge: reading metadata /tmp/ggml-out-q4_0-2-00003-of-00003.gguf ...done
gguf_merge: writing tensors /tmp/ggml-out-q4_0-2-00001-of-00003.gguf ...done
gguf_merge: writing tensors /tmp/ggml-out-q4_0-2-00002-of-00003.gguf ...done
gguf_merge: writing tensors /tmp/ggml-out-q4_0-2-00003-of-00003.gguf ...done
gguf_merge: /tmp/ggml-out-q4_0-2-merge.gguf merged from 3 split with 325 tensors.
```

## llama.cpp

[ggml-org/llama.cpp: LLM inference in C/C++](https://github.com/ggml-org/llama.cpp)
[LLaMA.cpp HTTP Server README · ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md)

[Local AI performance variables table](https://martech.org/how-to-run-deepseek-locally-on-your-computer/)

llama-cli

```sh
# llama-cli
#
# A CLI tool for accessing and experimenting with most of llama.cpp's functionality.
# Run in conversation mode
# Models with a built-in chat template will automatically activate conversation mode. If this doesn't occur, you can manually enable it by adding -cnv and specifying a suitable chat template with --chat-template NAME
llama-cli -m model.gguf
#
# > hi, who are you?
# Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?
#
# > what is 1+1?
# Easy peasy! The answer to 1+1 is... 2!
```

llama-server

```sh
# llama-server
#
# A lightweight, OpenAI API compatible, HTTP server for serving LLMs.
# Start a local HTTP server with default configuration on port 8080
llama-server -m model.gguf --port 8080
#
# Basic web UI can be accessed via browser: http://localhost:8080
# Chat completion endpoint: http://localhost:8080/v1/chat/completions
Support multiple-users and parallel decoding
# up to 4 concurrent requests, each with 4096 max context
llama-server -m model.gguf -c 16384 -np 4

# Serve an embedding model
# use the /embedding endpoint
llama-server -m model.gguf --embedding --pooling cls -ub 8192
# Serve a reranking model
# use the /reranking endpoint
llama-server -m model.gguf --reranking
```

### llama docker

```sh
# ghcr.io/ggml-org/llama.cpp:full-cuda: Same as full but compiled with CUDA support. (platforms: linux/amd64)
$ docker run --gpus all -v /dsdata/modelscope/:/models ghcr.io/ggml-org/llama.cpp:full-cuda bash
Unknown command: bash
Available commands:
  --run (-r): Run a model previously converted into ggml
              ex: -m /models/7B/ggml-model-q4_0.bin -p "Building a website can be done in 10 simple steps:" -n 512
  --bench (-b): Benchmark the performance of the inference for various parameters.
              ex: -m model.gguf
  --perplexity (-p): Measure the perplexity of a model over a given text.
              ex: -m model.gguf -f file.txt
  --convert (-c): Convert a llama model into ggml
              ex: --outtype f16 "/models/7B/"
  --quantize (-q): Optimize with quantization process ggml
              ex: "/models/7B/ggml-model-f16.bin" "/models/7B/ggml-model-q4_0.bin" 2
  --all-in-one (-a): Execute --convert & --quantize
              ex: "/models/" 7B
  --server (-s): Run a model on the server
              ex: -m /models/7B/ggml-model-q4_0.bin -c 2048 -ngl 43 -mg 1 --port 8080
```

### Unsloth

[Run DeepSeek-R1 Dynamic 1.58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)
[Tutorial: How to Run DeepSeek-R1 Locally | Unsloth Documentation](https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/tutorial-how-to-run-deepseek-r1-locally)

```sh
# Example with Q4_0 K quantized cache Notice -no-cnv disables auto conversation mode
./llama.cpp/llama-cli \
    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --cache-type-k q4_0 \
    --threads 12 -no-cnv --prio 2 \
    --temp 0.6 \
    --ctx-size 8192 \
    --seed 3407 \
    --prompt "<｜User｜>What is 1+1?<｜Assistant｜>"

docker run -v /path/to/models:/models -p 8000:8000 ghcr.io/ggml-org/llama.cpp:server -m /models/7B/ggml-model-q4_0.gguf --port 8000 --host 0.0.0.0 -n 512
docker run --gpus all -v /path/to/models:/models local/llama.cpp:server-cuda -m /models/7B/ggml-model-q4_0.gguf --port 8000 --host 0.0.0.0 -n 512 --n-gpu-layers 1

# Deploy with docker on Linux:
docker run --runtime nvidia --gpus all \
    --detach \
    --restart always \
    --name llama \
    --env TZ=Asia/Shanghai \
    --env CUDA_VISIBLE_DEVICES=3,4 \
    --volume /dsdata/modelscope/:/models/ \
    --publish 8000:8000 \
    ghcr.io/ggml-org/llama.cpp:server \
    --port 8000 --host 0.0.0.0 -n 512 \
    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf
```

#### Chat Template Issues?

[Tutorial: How to Run DeepSeek-R1 Locally DeepSeek Chat Template | Unsloth Documentation](https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/tutorial-how-to-run-deepseek-r1-locally#deepseek-chat-template)

```sh
print_info: BOS token        = 0 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 1 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 1 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 128815 '<｜PAD▁TOKEN｜>'
print_info: LF token         = 201 'Ċ'
print_info: FIM PRE token    = 128801 '<｜fim▁begin｜>'
print_info: FIM SUF token    = 128800 '<｜fim▁hole｜>'
print_info: FIM MID token    = 128802 '<｜fim▁end｜>'
print_info: EOG token        = 1 '<｜end▁of▁sentence｜>'
```

All distilled versions and the main 671B R1 model use the same chat template:

```sh
<｜begin▁of▁sentence｜><｜User｜>What is 1+1?<｜Assistant｜>It's 2.<｜end▁of▁sentence｜><｜User｜>Explain more!<｜Assistant｜>
```

A BOS is forcibly added, and an EOS separates each interaction. To counteract double BOS tokens during inference, you should only call tokenizer.encode(..., add_special_tokens = False) since the chat template auto adds a BOS token as well.
For llama.cpp / GGUF inference, you should skip the BOS since it’ll auto add it.

`<｜User｜>What is 1+1?<｜Assistant｜>`

The `<think>` and `</think>` tokens get their own designated tokens. For the distilled versions for Qwen and Llama, some tokens are re-mapped, whilst Qwen for example did not have a BOS token, so <|object_ref_start|> had to be used instead.

### llama server 参数

[LLaMA.cpp HTTP Server README](https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md)

```sh
-t, --threads N     number of threads to use during generation (default: -1) (env: LLAMA_ARG_THREADS)
--prio N            set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)
-c, --ctx-size N    size of the prompt context (default: 4096, 0 = loaded from model) (env: LLAMA_ARG_CTX_SIZE)
-ctk, --cache-type-k TYPE   KV cache data type for K allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_K)
-ctv, --cache-type-v TYPE   KV cache data type for V allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_V)
-dev, --device <dev1,dev2,..>   comma-separated list of devices to use for offloading (none = don't offload) use --list-devices to see a list of available devices (env: LLAMA_ARG_DEVICE)
--list-devices    print list of available devices and exit
-ngl, --gpu-layers, --n-gpu-layers N  number of layers to store in VRAM (env: LLAMA_ARG_N_GPU_LAYERS)
-sm, --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:
                                      - none: use one GPU only
                                      - layer (default): split layers and KV across GPUs
                                      - row: split rows across GPUs (env: LLAMA_ARG_SPLIT_MODE)
-m, --model FNAME   model path (default: models/$filename with filename from --hf-file or --model-url if set, otherwise models/7B/ggml-model-f16.gguf)
(env: LLAMA_ARG_MODEL)
```

Sampling params

```sh
--jinja    Enable experimental Jinja templating engine (required for tool use)
--reasoning-format FORMAT    Controls extraction of model thinking traces and the format / field in which they are returned (default: deepseek; allowed values: deepseek, none; requires --jinja). none will leave thinking traces inline in message.content in a model-specific format, while deepseek will return them separately under message.reasoning_content
```

Example-specific params

```sh
--host HOST   ip address to listen (default: 127.0.0.1) (env: LLAMA_ARG_HOST)
--port PORT   port to listen (default: 8080) (env: LLAMA_ARG_PORT)

--embedding, --embeddings    restrict to only support embedding use case; use only with dedicated embedding models (default: disabled) (env: LLAMA_ARG_EMBEDDINGS)
--reranking, --rerank    enable reranking endpoint on server (default: disabled) (env: LLAMA_ARG_RERANKING)
--api-key KEY    API key to use for authentication (default: none) (env: LLAMA_API_KEY)  usage: --header "Authorization: Bearer KEY"
```

## 模型部署工具

Ollama：适合个人 + 本地部署 + 轻量体验
vLLM：适合企业级 + 服务器部署 + 高性能扩展

[对接本地大模型时，选择 Ollma 还是 vLLM？ - OSCHINA - 中文开源技术交流社区](https://www.oschina.net/news/321572)
[大模型工具对比：SGLang, Ollama, VLLM, LLaMA.cpp如何选择？](https://stable-learn.com/zh/ai-model-tools-comparison/)

[Integrate Local Models Deployed by Xinference | Dify](https://docs.dify.ai/development/models-integration/xinference)
[Integrate Local Models Deployed by OpenLLM | Dify](https://docs.dify.ai/development/models-integration/openllm)
[Integrate Local Models Deployed by LocalAI | Dify](https://docs.dify.ai/development/models-integration/localai)

| 工具名称        | 性能表现                                                     | 易用性                                 | 适用场景                                 | 硬件需求                       | 模型支持                      | 部署方式                       |
|-------------|----------------------------------------------------------|-------------------------------------|--------------------------------------|----------------------------|---------------------------|----------------------------|
| SGLang v0.4 | 零开销批处理提升1.1倍吞吐量，缓存感知负载均衡提升1.9倍，结构化输出提速10倍                | 需一定技术基础，但提供完整API和示例                 | 企业级推理服务、高并发场景、需要结构化输出的应用             | 推荐A100/H100，支持多GPU部署       | 全面支持主流大模型，特别优化DeepSeek等模型 | Docker、Python包             |
| Ollama      | 继承 llama.cpp 的高效推理能力，提供便捷的模型管理和运行机制                      | 小白友好，提供图形界面安装程序一键运行和命令行，支持 REST API | 个人开发者创意验证、学生辅助学习、日常问答、创意写作等个人轻量级应用场景 | 与 llama.cpp 相同，但提供更简便的资源管理 | 模型库丰富，涵盖 1700 多款，支持一键下载安装 | 独立应用程序、Docker、REST API     |
| VLLM        | 借助 PagedAttention 和 Continuous Batching 技术，多 GPU 环境下性能优异 | 需要一定技术基础，配置相对复杂                     | 大规模在线推理服务、高并发场景                      | 要求 NVIDIA GPU，推荐 A100/H100 | 支持主流 Hugging Face 模型      | Python包、OpenAI兼容API、Docker |
| LLaMA.cpp   | 多级量化支持，跨平台优化，高效推理                                        | 命令行界面直观，提供多语言绑定                     | 边缘设备部署、移动端应用、本地服务                    | CPU/GPU 均可，针对各类硬件优化        | GGUF格式模型，广泛兼容性            | 命令行工具、API服务器、多语言绑定         |

### 模型显存使用量计算

[Can Your Computer Run This LLM?](https://www.canirunthisllm.net/)
[Ollama GPU Compatibility Calculator - React App](https://aleibovici.github.io/ollama-gpu-calculator/)

[模型显存使用量计算 — Xinference](https://inference.readthedocs.io/zh-cn/stable/models/model_memory.html)
[LLM Model VRAM Calculator - a Hugging Face Space by NyxKrage](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)

[STOP asking for "the best model for my pc" : r/ollama](https://www.reddit.com/r/ollama/comments/1j8qp3g/stop_asking_for_the_best_model_for_my_pc/?share_id=xGRLbTeGKPHcJcmxcm5Je&utm_content=1&utm_medium=ios_app&utm_name=iossmf&utm_source=share&utm_term=22)

```sh
xinference cal-model-mem -s 7 -f gptq -c 8192 -n GOT-OCR2_0
xinference cal-model-mem -s 7 -q Int4 -f gptq -c 16384 -n qwen1.5-chat
# model_name: qwen1.5-chat
# kv_cache_dtype: 16
# model size: 7.0 B
# quant: Int4
# context: 16384
# gpu mem usage:
#   model mem: 4139 MB
#   kv_cache: 8192 MB
#   overhead: 650 MB
#   active: 17024 MB
#   total: 30005 MB (30 GB)

## GPU 使用情况监控
watch -n 1 nvidia-smi
```

### ollama 模型部署工具

Ollama, a popular local LLM deployment tool, supports a broad range of open-source LLMs and offers an intuitive experience, making it ideal for single-user, local environments.

### OpenLLM

[bentoml/OpenLLM: Run any open-source LLMs, such as Llama, Mistral, as OpenAI compatible API endpoint in the cloud.](https://github.com/bentoml/OpenLLM)

From Ollama to OpenLLM: Running LLMs in the Cloud
[From Ollama to OpenLLM: Running LLMs in the Cloud](https://www.bentoml.com/blog/from-ollama-to-openllm-running-llms-in-the-cloud)

```powershell
# [【解决】无法将“XXX”项识别为 cmdlet、函数、脚本文件或可运行程序的名称。请检查名称的拼写，如果包括路径，请确保路径正确，然后再试一次_无法将“labelme”项识别为 cmdlet、函数、脚本文件或可运行程序的名称。请检查名-CSDN博客](https://blog.csdn.net/weixin_41362657/article/details/110649744)
PS D:\>
Get-ExecutionPolicy -List

        Scope ExecutionPolicy
        ----- ---------------
MachinePolicy       Undefined
   UserPolicy       Undefined
      Process       Undefined
  CurrentUser       Undefined
 LocalMachine       Undefined

# Scope: Process, CurrentUser, LocalMachine, UserPolicy, MachinePolicy
# ExecutionPolicy: Unrestricted, RemoteSigned, AllSigned, Restricted, Default, Bypass, Undefined”
Set-ExecutionPolicy RemoteSigned -Scope CurrentUser
Set-ExecutionPolicy Unrestricted -Scope CurrentUser
Set-ExecutionPolicy RemoteSigned -Scope LocalMachine
```

### LocalAI

[LocalAI-examples/langchain-chroma at main · mudler/LocalAI-examples](https://github.com/mudler/LocalAI-examples/tree/main/langchain-chroma)

```sh
# 配置到dify 时 404
# 2025-02-11 11:51:10 3:51AM WRN Client error ip=172.20.0.1 latency="24.32µs" method=POST status=404 url=/rerank

curl http://localhost:8080/console/api/workspaces/current/models/model-types/rerank

curl http://localhost:8080/v1/rerank \
  -H "Content-Type: application/json" \
  -d '{
  "model": "cross-encoder",
  "query": "Organic skincare products for sensitive skin",
  "documents": [
    "Eco-friendly kitchenware for modern homes",
    "Biodegradable cleaning supplies for eco-conscious consumers",
    "Organic cotton baby clothes for sensitive skin",
    "Natural organic skincare range for sensitive skin",
    "Tech gadgets for smart homes: 2024 edition",
    "Sustainable gardening tools and compost solutions",
    "Sensitive skin-friendly facial cleansers and toners",
    "Organic food wraps and storage solutions",
    "All-natural pet food for dogs with allergies",
    "Yoga mats made from recycled materials"
  ],
  "top_n": 3
}'

```

### xinference

[在Xinference上部署自定义大模型——FreedomIntelligence/HuatuoGPT2-13B为例 - 知乎](https://zhuanlan.zhihu.com/p/685747169)

```sh
# 将模型下载源设置为 ModelScope。设置环境变量 XINFERENCE_MODEL_SRC=modelscope
docker pull registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference
docker image tag registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xprobe_xinference/xinference:latest
docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9997:9997 --gpus all xprobe/xinference:v<your_version> xinference-local -H 0.0.0.0 --log-level debug

  --restart always \
docker run --detach \
  --name xinference \
  --env TZ=Asia/Shanghai \
  --publish 9997:9997 \
  --env XINFERENCE_MODEL_SRC=modelscope \
  --volume //c/workspace/xinference/.xinference:/root/.xinference \
  --volume //c/workspace/xinference/.cache/huggingface:/root/.cache/huggingface \
  --volume //c/workspace/xinference/.cache/modelscope:/root/.cache/modelscope \
  --gpus all \
  xprobe_xinference/xinference:v0.15.4 \
  sh /root/.xinference/startup.sh

# Windows下改成一行执行
docker run --detach --env TZ=Asia/Shanghai --publish 9997:9997 --name xinference --restart always -e XINFERENCE_MODEL_SRC=modelscope -v //c/workspace/xinference/.xinference:/root/.xinference -v //c/workspace/xinference/.cache/huggingface:/root/.cache/huggingface -v //c/workspace/xinference/.cache/modelscope:/root/.cache/modelscope --gpus all xprobe_xinference/xinference sh /root/.xinference/startup.sh

alias xinference='docker exec -it xinference xinference'
```

```sh
#!/bin/bash
# startup.sh 放在外部磁盘挂载进去，启动时执行
# 启动且后台运行
xinference-local -H 0.0.0.0 &
# xinference-local -H 0.0.0.0 --log-level debug &
# 检测是否启动
while true; do
  if curl -s "http://localhost:9997" > /dev/null; then
    break
  else
    sleep 1
  fi
done

#自动加载 embedding
xinference launch --model-name bge-m3 --model-type embedding &
# xinference launch --model-name jina-embeddings-v3 --model-type embedding &
#自动加载 rerank
xinference launch --model-name jina-reranker-v2 --model-type rerank &

#等待后台运行结束，实际上xinference-local是不会结束的，所以能保证此脚本进程不结束，从而不会自动重启
wait
```

调用模型接口

```sh
# llm chat
curl -X 'POST' \
  'http://127.0.0.1:9997/v1/chat/completions' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "qwen2.5-instruct",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "What is the largest animal?"
        }
    ]
  }'

# embeddings
curl -X 'POST' 'http://localhost:9997/v1/embeddings' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "bge-m3",
    "input": "Hello, world!"
  }'

# rank
curl http://localhost:9997/v1/rerank \
  -H "Content-Type: application/json" \
  -d '{
  "model": "bge-reranker-v2-m3",
  "query": "Organic skincare products for sensitive skin",
  "documents": [
    "Eco-friendly kitchenware for modern homes",
    "Biodegradable cleaning supplies for eco-conscious consumers",
    "Organic cotton baby clothes for sensitive skin",
    "Natural organic skincare range for sensitive skin",
    "Tech gadgets for smart homes: 2024 edition",
    "Sustainable gardening tools and compost solutions",
    "Sensitive skin-friendly facial cleansers and toners",
    "Organic food wraps and storage solutions",
    "All-natural pet food for dogs with allergies",
    "Yoga mats made from recycled materials"
  ],
  "top_n": 3
}'
```

本地安装

```sh
# 本地安装
conda create --name py312 python=3.12
conda activate py312

# 安装xinference的依赖
pip install "xinference[all]"

# 启动xinference
xinference-local # 我使用这个命令启动不了
# 设置使用modelscope下载模型
# 如果你就一块gpu还是0的话就要指定启动
CUDA_VISIBLE_DEVICES=0
# 使用这个命令可以启动
XINFERENCE_HOME=d:/xinference/ XINFERENCE_MODEL_SRC=modelscope xinference-local --host 0.0.0.0 --port 9997

# 列举本地模型
xinference list --endpoint "http://127.0.0.1:9997"

# [jina-embeddings-v2-base-zh — Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/embedding/jina-embeddings-v2-base-zh.html)
# Model ID: jinaai/jina-embeddings-v2-base-zh
# xinference launch --model-name jina-embeddings-v2-base-zh --model-type embedding
xinference launch --model-name jina-embeddings-v3 --model-type embedding &
xinference launch --model-name bge-m3 --model-type embedding &

# [jina-reranker-v2 — Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/rerank/jina-reranker-v2.html)
# Model ID: jinaai/jina-reranker-v2-base-multilingual
xinference launch --model-name jina-reranker-v2 --model-type rerank &
xinference launch --model-name bge-reranker-large --model-type rerank &
xinference launch --model-name bge-reranker-v2-minicpm-layerwise --model-type rerank &

# OCR 模型 [GOT-OCR2_0 — Xinference](https://inference.readthedocs.io/zh-cn/v0.16.3/models/builtin/image/got-ocr2_0.html)
xinference launch --model-name GOT-OCR2_0 --model-type image
# Launch model name: GOT-OCR2_0 with kwargs: {}
# Model uid: GOT-OCR2_0

# 列出所有内置支持的模型
xinference registrations -t embedding
xinference registrations -t rerank
```

### SGLang

[sgl-project/sglang: SGLang is a fast serving framework for large language models and vision language models.](https://github.com/sgl-project/sglang)

### Embedding model

[Embedding models · Ollama Blog](https://ollama.com/blog/embedding-models)

[MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)
MTEB（Massive Text Embedding Benchmark）是一个用于评估文本嵌入（Embedding）模型的综合性基准测试平台。通过多任务和多数据集的组合，MTEB可以全面衡量不同Embedding模型在各种自然语言处理（NLP）任务中的表现，如文本分类、语义检索、文本聚类等。

```sh
ollama pull mxbai-embed-large
```

### reranking models

Ollama rerank model [linux6200/bge-reranker-v2-m3](https://ollama.com/linux6200/bge-reranker-v2-m3)

deploy local embedding/reranking models using xinference/LocalAI/OpenLLM.

[Using Xinference — Xinference](https://inference.readthedocs.io/en/latest/getting_started/using_xinference.html#using-xinference-with-docker)

```sh
docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9998:9997 --gpus all xprobe/xinference:<your_version> xinference-local -H 0.0.0.0 --log-level debug

# 1024 维度  最大 token 数 8192
jina-embeddings-v3
```

## 本地知识库搭建

[chatchat-space/Langchain-Chatchat: Langchain-Chatchat（原Langchain-ChatGLM）基于 Langchain 与 ChatGLM 等语言模型的本地知识库问答 | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM) QA app with langchain](https://github.com/chatchat-space/Langchain-Chatchat)
[Langchain-ChatGLM：基于本地知识库问答_langchain chatglm-CSDN博客](https://blog.csdn.net/dzysunshine/article/details/131003488)

[AnythingLLM、Dify 与 Open-WebUI：如何接入 Ollama，它们有何不同？一、前言 随着大语言模 - 掘金](https://juejin.cn/post/7455148200627781684)
[Dify、Anything LLM、Ollama、硅基流动 分别与 DeepSeek 联合搭建本地知识库的对比表格，涵盖功能、性能、成本和适用场景等核心维度。 #普通人如何玩转DeepSeek# #deepseek#](https://www.toutiao.com/w/1823311450553344/?app=news_article&category_new=search_thread_aggr&chn_id=94349607399&is_new_connect=0&is_new_user=0&req_id_new=202502090734553364518F875E49146204&share_did=MS4wLjACAAAAZtHq-FtDCLT2HiypHOsw85chAV9wX6K1Jv5UfAUyX0mVQ7HN00yXQhdxXLFA_4OY&share_token=D19DE10F-BE2F-4A6D-A704-AA68397479DC&share_uid=MS4wLjABAAAADwEoMfih4HUgYxYOjTmk9NvGlX1gGSG-ctDaxpGLfOkP-tRdhwDh6SLBAt5iihFb&source=m_redirect&timestamp=1739057696&tt_from=weixin_moments&use_new_style=1&utm_campaign=client_share&utm_medium=toutiao_ios&utm_source=weixin_moments&wxshare_count=1)

LangChain 是一个用于开发由语言模型驱动的应用程序的框架，主要拥有 3个能力：

1. 可以调用LLM模型
2. 可以将 LLM 模型与外部数据源进行连接
3. 允许与 LLM 模型进行交互

知识库问答实现步骤
基于Langchain思想实现基于本地知识库的问答应用。实现过程如下：
1、加载文件
2、读取文本
3、文本分割
4、文本向量化
5、问句向量化
6、在文本向量中匹配出与问句向量最相似的top k个
7、匹配出的文本作为上下文和问题一起添加到prompt中
8、提交给LLM生成回答。

### open-webui

[open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)](https://github.com/open-webui/open-webui)

### MaxKB 方案

[MaxKB/README_CN.md at main · 1Panel-dev/MaxKB](https://github.com/1Panel-dev/MaxKB/blob/main/README_CN.md)

[MaxKB 离线安装包下载 - FIT2CLOUD 飞致云](https://community.fit2cloud.com/#/download/maxkb/v1-9-1)

```sh
# Linux 机器
docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data -v ~/.python-packages:/opt/maxkb/app/sandbox/python-packages cr2.fit2cloud.com/1panel/maxkb

# Windows 机器
docker run -d --name=maxkb --restart=always -p 8080:8080 -v C:/maxkb:/var/lib/postgresql/data -v C:/python-packages:/opt/maxkb/app/sandbox/python-packages cr2.fit2cloud.com/1panel/maxkb
docker run -d --name=maxkb --restart=always -p 8080:8080 -v d:/docker/maxkb:/var/lib/postgresql/data -v d:/docker/maxkb/python-packages:/opt/maxkb/app/sandbox/python-packages 1panel/maxkb:v1.10.0-lts

# 用户名: admin
# 密码: MaxKB@123..
# Max1+1
```

### anything-llm

[Mintplex-Labs/anything-llm: The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, and more.](https://github.com/Mintplex-Labs/anything-llm)

[anything-llm HOW_TO_USE_DOCKER](https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/HOW_TO_USE_DOCKER.md)

```powershell
# Run this in powershell terminal
$env:STORAGE_LOCATION="$HOME\Documents\anythingllm"; `
If(!(Test-Path $env:STORAGE_LOCATION)) {New-Item $env:STORAGE_LOCATION -ItemType Directory}; `
If(!(Test-Path "$env:STORAGE_LOCATION\.env")) {New-Item "$env:STORAGE_LOCATION\.env" -ItemType File}; `
docker run -d -p 3001:3001 `
--cap-add SYS_ADMIN `
-v "$env:STORAGE_LOCATION`:/app/server/storage" `
-v "$env:STORAGE_LOCATION\.env:/app/server/.env" `
-e STORAGE_DIR="/app/server/storage" `
mintplexlabs/anythingllm;

# anyadmin / Anyadmin1+1
```

### FastGPT

[labring/FastGPT: FastGPT is a knowledge-based platform built on the LLMs, offers a comprehensive suite of out-of-the-box capabilities such as data processing, RAG retrieval, and visual AI workflow orchestration, letting you easily develop and deploy complex question-answering systems without the need for extensive setup or configuration.](https://github.com/labring/FastGPT)

[快速了解 FastGPT | FastGPT](https://doc.tryfastgpt.ai/docs/intro/)

## Dify

[Dify github](https://github.com/langgenius/dify)
[在线版 Studio - Dify](https://cloud.dify.ai/apps)

[Dify 文档](https://docs.dify.ai/zh-hans)

[特性与技术规格 | Dify](https://docs.dify.ai/zh-hans/getting-started/readme/features-and-specifications)

[DifyShare - Share your flows. View the magic.](https://difyshare.com/)
[BannyLon/DifyAIA: 基于Dify自主创建的AI应用DSL工作流](https://github.com/BannyLon/DifyAIA)
[DeepSeek+dify 本地知识库：高级应用Agent+工作流](https://mp.weixin.qq.com/s/1p5KRDflsIISdOvm4QkWYw)
[Dify 架构篇| 多租户下的SSO功能 - 53AI-AI知识库|大模型知识库|大模型训练|智能体开发](https://www.53ai.com/news/dify/2025032992518.html)

Dify is an open-source LLM app development platform. Dify's intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.

案例：

- 训练出专属于“你”的问答机器人
- 官网 AI 智能客服
- 接入微信
- 接入钉钉

[jaguarliuu/rookie\_text2data: Dify插件 - 自然语言获取数据库数据](https://github.com/jaguarliuu/rookie_text2data)
[Markdown转md文件 Markdown Exporter - Dify Marketplace](https://marketplace.dify.ai/plugins/bowenliang123/md_exporter)

### dify deploy

[Deploy Dify on Kubernetes](https://github.com/Winson-030/dify-kubernetes)

.env 文件修改

```sh
LOG_LEVEL=DEBUG

LOG_TZ=Asia/Shanghai
LOG_FILE=/app/logs/server.log

DEBUG=true
FLASK_DEBUG=true

## customize
# Add environment variables below at the end of .env file, then docker compose down && docker compose up -d
PLUGIN_PYTHON_ENV_INIT_TIMEOUT=720
PIP_MIRROR_URL=https://mirrors.aliyun.com/pypi/simple
```

### Dify 1.0.1 deploy

[Release v1.0.1 · langgenius/dify](https://github.com/langgenius/dify/releases/tag/1.0.1)

PowerShell 下启动

### Dify 1.0.0 deploy

[dify-docs/zh_CN/development/migration/migrate-to-v1.md at main · langgenius/dify-docs](https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/migration/migrate-to-v1.md)

```sh
root@e81a1e9bfdc9:/app/api# poetry run flask install-plugins --workers=2
2025-03-09 06:08:52.421 INFO [MainThread] [utils.py:149] - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-03-09 06:08:52.422 INFO [MainThread] [utils.py:162] - NumExpr defaulting to 16 threads.
```

### Dify 0.15.3 deploy

[Docker Compose 部署 | Dify](https://docs.dify.ai/zh-hans/getting-started/install-self-hosted/docker-compose)

.env 修改部分

```sh
# .env

# Used to change the OpenAI base address, default is https://api.openai.com/v1.
# When OpenAI cannot be accessed in China, replace it with a domestic mirror address,
# or when a local model provides OpenAI compatible API, it can be replaced.
OPENAI_API_BASE=https://api.openai.com/v1

# Defaults to gevent. If using windows, it can be switched to sync or solo.
SERVER_WORKER_CLASS=gevent

# Upload file size limit, default 15M.
UPLOAD_FILE_SIZE_LIMIT=15
# Upload image file size limit, default 10M.
UPLOAD_IMAGE_FILE_SIZE_LIMIT=10
# Upload video file size limit, default 100M.
UPLOAD_VIDEO_FILE_SIZE_LIMIT=100
# Upload audio file size limit, default 50M.
UPLOAD_AUDIO_FILE_SIZE_LIMIT=50

MAIL_DEFAULT_SEND_FROM=自己的邮箱
# SMTP server configuration, used when MAIL_TYPE is `smtp`
SMTP_SERVER= 对应邮箱的smtp，一般都在设置里
SMTP_PORT=465
SMTP_USERNAME= 自己的邮箱
SMTP_PASSWORD=  自己的密码
SMTP_USE_TLS=true
SMTP_OPPORTUNISTIC_TLS=false
```

Docker Compose 部署

```sh
# 克隆 Dify 源代码至本地环境。
# 假设当前最新版本为 0.15.3
git clone https://github.com/langgenius/dify.git --branch 0.15.3

# 启动 Dify
# 1.  进入 Dify 源代码的 Docker 目录
cd dify/docker
# 2.  复制环境配置文件
cp .env.example .env
# 3.  启动 Docker 容器
docker-compose up -d
```

更新 Dify 进入 dify 源代码的 docker 目录，按顺序执行以下命令：

```sh
cd dify/docker
docker compose down
git pull origin main
docker compose pull
docker compose up -d
```

访问 Dify
你可以先前往管理员初始化页面设置设置管理员账户：

```sh
# 本地环境
http://localhost/install

# 服务器环境
http://your_server_ip/install

# Dify 主页面：
# 本地环境
http://localhost

# 服务器环境
http://your_server_ip
```

### 如何重置dify管理员密码

```sh
docker exec -it docker-api-1 flask reset-password
# 然后按照提示输入管理员email以及两次新密码即可。
```

### dify 钉钉

[将 Dify 应用与钉钉机器人集成 | Dify](https://docs.dify.ai/zh-hans/learn-more/use-cases/dify-on-dingtalk)

### dify database

select name,email,interface_language,last_login_at,last_login_ip,status,created_at,last_active_at from accounts;

### dify issue

```sh
# agent 调用雅虎财经
# prompt 今天有哪些新闻
[ollama] Error: API request failed with status code 400: {"error":"registry.ollama.ai/library/deepseek-r1:7b does not support tools"}
```

## firecrawl

[firecrawl/CONTRIBUTING](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md)

[localhost:3002](http://localhost:3002/)
[admin/queues](http://localhost:3002/admin/queues)

```sh
docker pull node:18-slim
docker pull node:22-slim
docker pull rust:1-slim
docker pull golang:1.24


curl -X POST http://localhost:3002/v1/crawl \
    -H 'Content-Type: application/json' \
    -d '{
      "url": "https://www.baidu.com"
    }'

curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "limit": 10,
      "scrapeOptions": {
        "formats": ["markdown", "html"]
      }
    }'


```

## GPU 介绍

GPU 命名规则解读

[一文读懂 NVIDIA GPU 产品线-51CTO.COM](https://www.51cto.com/article/805028.html)
[英伟达GPU各型号对比 - 知乎](https://zhuanlan.zhihu.com/p/718450083)
[比较 GeForce 系列最新一代显卡和前代显卡 | NVIDIA](https://www.nvidia.cn/geforce/graphics-cards/compare/)

### 字母： 架构代号（Architecture）

新的架构通常代表着性能、能效比和新技术的显著提升。

代表 GPU 的核心架构，通常用一个或多个字母表示，代表 GPU 的微架构。例如：

K：Kepler 架构 2012
V：Volta 架构 2017
T：Turing 架构 2018, RTX 20 系列, GTX 16 系列
A：Ampere 架构 2019, RTX 30 系列
H：Hopper 架构 2022, RTX 4000 系列
L: Ada Lovelace 架构 2022, RTX 40 系列
B: Blackwell 架构, RTX 50 系列

### 数字：性能层级（Tier）

通常用数字表示，数字越大通常代表性能越强。

代表 GPU 的具体型号，通常用一个或多个数字表示。例如：

“4” 系列：入门级或低功耗级
“10” 系列：中端推理优化级
“40” 系列：高端图形和虚拟工作站级
“100” 系列：旗舰级高性能计算和人工智能级

### 常见的GPU 型号对比解析：基于 GPU 命名推断显卡特性

示例一：T4 与 L4 的比较

L4 是 T4 的直接后继者，属于同一性能层级，针对相似的应用场景设计。然而，两者在微架构和技术规格上存在显著差异：

微架构： L4 采用更新的 Ada Lovelace 架构（2023 年发布），而 T4 则采用较早的 Turing 架构（2018 年发布）。
显存容量： L4 配备了更大的显存容量，达到 24 GB，而 T4 仅有 16 GB。
核心数量和性能： L4 拥有更多且更强大的计算核心，因此在性能上优于 T4。
虽然两者的目标功耗相似，但 L4 凭借更先进的架构和更高的显存容量，在相同的功耗下能够提供更强的计算性能，更适合处理对显存容量有较高要求的任务。

示例二：A10 与 A100 的比较

A100 是基于 Ampere 架构的旗舰级产品，而 A10 则是该架构下的一个较低层级的型号。两者都基于相同的 Ampere 微架构，但在规模和性能上存在显著差异：

核心数量和性能： A100 拥有远多于 A10 的计算核心，因此在计算性能上远超 A10。
显存容量： A100 配备了更大的显存容量，以支持更大规模的模型训练和推理。
功耗： 由于规模更大、性能更强，A100 的功耗也高于 A10。
因此，A100 更适合需要处理大规模模型训练、微调和高吞吐量推理等 demanding 计算任务的场景，而 A10 则更适合对成本和功耗敏感、对性能要求相对较低的应用场景。

### 面向 AI 和机器学习的显卡

| 排名 | GPU 型号 | 架构 | CUDA 核心数 | 显存 | 主要用途 |
| --- | --- | --- | --- | --- | --- |
| 1 | NVIDIA H100 | Hopper | 16896 | 80GB HBM3 | 大规模 AI 训练 |
| 2 | NVIDIA A100 | Ampere | 6912 | 40GB HBM2 | AI 推理、机器学习 |
| 3 | NVIDIA V100 | Volta | 5120 | 32GB HBM2 | 神经网络训练、科学计算 |
| 4 | Tesla T4 | Turing | 2560 | 16GB GDDR6 | 轻量 AI 推理、云推理 |
| 5 | Tesla P100 | Pascal | 3584 | 16GB HBM2 | 数据中心加速、机器学习推理 |
| 6 | NVIDIA A40 | Ampere | ? | 48GB | AI 推理、机器学习 |

H20 96GB

### 硬件资源配置

显存需求 ≈ 模型参数 × 参数字节数 × 安全系数（1.3-1.5）

CPU GPU 配置比例：建议内存是显存的1.5倍以上

### gpu 相关命令

```sh
nvcc --version
# nvcc: NVIDIA (R) Cuda compiler driver
# Copyright (c) 2005-2021 NVIDIA Corporation
# Built on Thu_Nov_18_09:45:30_PST_2021
# Cuda compilation tools, release 11.5, V11.5.119
# Build cuda_11.5.r11.5/compiler.30672275_0

nvidia-smi
# Mon Mar 31 14:24:25 2025
# +-----------------------------------------------------------------------------------------+
# | NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
# |-----------------------------------------+------------------------+----------------------+
# | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
# |                                         |                        |               MIG M. |
# |=========================================+========================+======================|
# |   0  NVIDIA A40                     Off |   00000000:67:00.0 Off |                    0 |
# |  0%   75C    P0            303W /  300W |   43985MiB /  46068MiB |     96%      Default |
# |                                         |                        |                  N/A |
# +-----------------------------------------+------------------------+----------------------+

# +-----------------------------------------------------------------------------------------+
# | Processes:                                                                              |
# |  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
# |        ID   ID                                                               Usage      |
# |=========================================================================================|
# |    0   N/A  N/A           25809      C   /usr/local/bin/ollama                 43976MiB |
# +-----------------------------------------------------------------------------------------+

pip install nvitop
# 查看
nvitop
```
