# AI News & Information

AI干不了你干不了的事，因为你干不了你就看不出来它是胡干还是真干。AI的有效能力范围等于跟它配对的人的能力范围。

- [Humata - ChatGPT for all your files](https://www.humata.ai/)
- [Code Llama: Inference code for CodeLlama models](https://github.com/facebookresearch/codellama)

## GPT 分类

[高盛发布中国AI报告：全面剖析BAT三巨头以及26个核心玩家](https://36kr.com/p/5091725.html)

- 用 [Perplexity](http://Perplexity.ai) 代替 Google；
- 用 [Poe](https://poe.com/sage) 代替 ChatGPT.
- 用 Promptable 代替 OpenAI Playground.
- 用 [Elicit](https://elicit.org) 代替 Google Scholar

- [极客时间 AI 指南](https://zhinan.geekbang.org/)
- 写文案用 ChatGPT、Notion AI或 Jarvis，Claude，
  - [Poe](https://poe.com/sage)
  - Slack Claude
  - [文心一言 百度](https://yiyan.baidu.com/)
  - [通义千问 阿里](https://qianwen.aliyun.com/)
  - [Google AI Studio](https://aistudio.google.com/app/prompts/new_chat)
  - [Poe](https://poe.com/sage)
  - [Kimi.ai - 帮你看更大的世界](https://kimi.moonshot.cn/)
  - [DeepSeek](https://platform.deepseek.com)
  - [讯飞星火大模型 文案 图片 PPT 代码](https://xinghuo.xfyun.cn/desk)
  - [商汤商量语言大模型 注册需要企业邀请码](https://chat.sensetime.com/wb/register)
  - 腾讯混元助手 微信小程序
  - bard，
  - Copy ai，
  - [Writesonic](https://app.writesonic.com，
  - Character AI，
  - [Jasper](https://www.jasper.ai/)
  - [Forefront Chat](https://chat.forefront.ai/)
- 图像处理
  - [文心一格 百度](https://yige.baidu.com/)
  - [通义万相 阿里](https://wanxiang.aliyun.com/creation)
  - Midjourney Discord 或 Getimg.ai，stable diffusion，dall-e
  - [OmniParser 图片解析](https://microsoft.github.io/OmniParser/)：OmniParser is a comprehensive method for parsing user interface screenshots into structured and easy-to-understand elements
  - [生成插画、漫画 Infinite AI Artboard - Recraft](https://www.recraft.ai/)
- 音视频转文本
  - [通义听悟 - 阿里](https://tingwu.aliyun.com)
  - [Aiko — Sindre Sorhus 语音转文字](https://sindresorhus.com/aiko)
  - [MacWhisper](https://goodsnooze.gumroad.com/l/macwhisper) [MacWhisper](https://app.gumroad.com/d/29e33b796f6ce9bb186f87cdf2fadb16)
  - Buzz transcribes and translates audio offline on your personal computer. [Buzz](https://github.com/chidiwilliams/buzz)
  - [AudioPen](https://audiopen.ai/demo)
- 文本转音视频
  - 音频生成用 voice.ai 或 Eleven Labs，连字幕都可以用自己通过AI“训练”的个性专属字体。
- 视频处理
  - 把画出来的人物，喂文案让他变成频视的是D-ID；
  - 视频制作用 Descript 或 Runway
  - 一个频视中的人物替换成虚拟人或CG的是Wonder；
  - 给频视配音创建音乐就用Soundraw；
  - [Open-Sora 视频生成](https://github.com/hpcaitech/Open-Sora)
  - [Open Sora demo - a Hugging Face Space by hpcai-tech](https://huggingface.co/spaces/hpcai-tech/open-sora)
  - [可灵 AI - 新一代 AI 创意生产力平台](https://klingai.kuaishou.com/)
- 智能化研发
  - [Codeium · Free AI Code Completion & Chat](https://codeium.com/faq)
  - [通义灵码_智能编码助手 阿里云](https://tongyi.aliyun.com/lingma/download)
  - 百度 Comate 2.0代码助手 [Baidu Comate · Coding mate](https://comate.baidu.com/)
  - [OpenDevin - OpenHands](https://github.com/All-Hands-AI/OpenHands)
  - [yetone/avante.nvim: Use your Neovim like using Cursor AI IDE!](https://github.com/yetone/avante.nvim)
  - [阿里智能化研发一年复盘](https://mp.weixin.qq.com/s/JTpLy8Z0klokHVcaDZm2RQ)
  - [CodeRabbit is an AI-powered code reviewer](https://docs.coderabbit.ai/)
  - [rikvermeulen/co-op-gitlab: Automate code reviews and feedback for GitLab Merge Requests using OpenAI GPT-3/4](https://github.com/rikvermeulen/co-op-gitlab)
  - [Diffblue Cover write Java unit tests](https://docs.diffblue.com/get-started/get-started/get-started-cover-plugin)
- 把真人变成虚拟人的是 Meta human，想更简单变虚拟人的是Ready player me；
- PPT:
  - chatgpt生成的markdown语法内容导入[MindShow](https://www.mindshow.fun/#/home)
  - [AI PPT Maker - Best Online Free (No Sign up)](https://aipptmaker.ai/en)
  - [快速做PPT Gamma - Presentations and Slide Decks with AI | Gamma](https://gamma.app/)
- 第三方社群，civitai
- 数学 [MathGPT 学而思](https://www.mathgpt.com/)
- 医疗问题回答[MediSearch](https://medisearch.io/zh)
- 查找法律案例、条文 [MetaLaw 类案检索，一键直达，让你的法律研究效率快人10倍](https://meta.law/)
- 办公小浣熊:制作excel表格。
- Scribe AI chrome 插件：自动生成可视化操作指南, 创建逐步指南的场景，包括操作说明、标准操作规程、培训手册等。
- 编排
  - [Dify github](https://github.com/langgenius/dify)
  - [TEN Agent is a conversational AI](https://github.com/TEN-framework/TEN-Agent)
  - [RAG七十二式：2024年度RAG清单](https://mp.weixin.qq.com/s/_pnezCv-sKmzhho7Xw3D2g)

[AI 工具列表 飞书文档](https://zl49so8lbq.feishu.cn/wiki/wikcn6YTN3CrZTS8RhrEca8c8Eg)
[极客时间 AIGC 知识库](https://gp477l8icq.feishu.cn/wiki/JUXnwzSuviL5E9kh6jUc8FRinHe)
[ModelScope魔搭社区 旨在打造下一代开源的模型即服务共享平台，为泛AI开发者提供灵活、易用、低成本的一站式模型服务产品，让模型应用更简单。](https://community.modelscope.cn/)
[ima.copilot-腾讯智能工作台](https://ima.qq.com/) ima.copilot（简称ima）是一款由腾讯混元大模型提供技术支持的，面向学习、办公场景，以知识库为核心的AI智能工作台，是读、搜、写一体的效率工具

[xtekky/gpt4free: decentralising the Ai Industry, just some language model api's...](https://github.com/xtekky/gpt4free)

| Website s | Model(s) |
| --- | --- |
| [forefront.ai](https://chat.forefront.ai) | GPT-4/3.5 |
| [poe.com](https://poe.com) | GPT-4/3.5 |
| [writesonic.com](https://writesonic.com) | GPT-3.5 / Internet |
| [t3nsor.com](https://t3nsor.com) | GPT-3.5 |
| [you.com](https://you.com) | GPT-3.5 / Internet / good search |
| [sqlchat.ai](https://sqlchat.ai) | GPT-3.5 |
| [bard.google.com](https://bard.google.com) | custom / search |
| [bing.com/chat](https://bing.com/chat) | GPT-4/3.5 |
| [italygpt.it](https://italygpt.it) | GPT-3.5 |

## 模型

### deepseek

deepseek janus pro 多模态大模型炸裂出场，transformer架构，没有走diffusion路线

[DeepSeek Token 用量计算 | DeepSeek API Docs](https://api-docs.deepseek.com/zh-cn/quick_start/token_usage)

[KTransformers 4090单卡跑671B DeepSeek-R1 - 知乎](https://zhuanlan.zhihu.com/p/23212558318)
[单卡RTX4090部署R1满血版之KTransformers篇-阿朱](https://mp.weixin.qq.com/s/g3JsrLUuMXDX-8lSSzb06A)

[4090单卡部署QWen2.5-VL视觉模型](https://mp.weixin.qq.com/s/Ha-J5uUKk7XUqMfW_VEqHg)

## GPT 项目

GPT (Generative Pre-trained Transformer)

[ChatGPT](https://chat.openai.com/chat)
[What Is ChatGPT Doing … and Why Does It Work?—Stephen Wolfram Writings](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
[ChatGPT为啥这么强：万字长文详解 by WolframAlpha之父-今日头条](https://www.toutiao.com/article/7200604582392087095)
[ChatGPT学习资料合集 Original 吕建伟 阿朱说](https://mp.weixin.qq.com/s/ZApy_d873Y1DEmGc6NjplQ)

[如何使用ChatGPT API训练自定义知识库AI聊天机器人 - 闪电博](https://www.wbolt.com/how-to-train-ai-chatbot.html)
[How to Train an AI Chatbot With Custom Knowledge Base Using ChatGPT API | Beebom](https://beebom.com/how-train-ai-chatbot-custom-knowledge-base-chatgpt-api/)
[Fine-tuning - OpenAI API](https://platform.openai.com/docs/guides/fine-tuning)

[笔记本就能运行的ChatGPT平替来了，附完整版技术报告-今日头条](https://www.toutiao.com/article/7216255969465303613)
[nomic-ai/gpt4all: gpt4all: a chatbot trained on a massive collection of clean assistant data including code, stories and dialogue](https://github.com/nomic-ai/gpt4all)

[用笔记本运行650亿参数大模型 InfoQ精选文章](https://www.infoq.cn/article/qucNy1wcUq87HCSTjddQ)
[Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp | Simon Willison’s TILs](https://til.simonwillison.net/llms/llama-7b-m2)
[ggerganov/llama.cpp: Port of Facebook's LLaMA model in C/C++](https://github.com/ggerganov/llama.cpp)

[人人都能懂的ChatGPT解读_AI_张杰_InfoQ精选文章](https://www.infoq.cn/article/VWrPIRvRg6E3O74q7PtL)

[go-zoox/chatgpt-for-chatbot-feishu: 快速将 ChatGPT 接入飞书，基于 OpenAI 官方接口，作为私人工作助理或者企业员工助理](https://github.com/go-zoox/chatgpt-for-chatbot-feishu)

[zhayujie/chatgpt-on-wechat: 使用ChatGPT搭建微信聊天机器人，基于ChatGPT3.5 API和itchat实现。Wechat robot based on ChatGPT, which using OpenAI api and itchat library.](https://github.com/zhayujie/chatgpt-on-wechat)

[ConnectAI-E/Dingtalk-OpenAI: 🔔 钉钉 & 🤖 GPT-3.5 让你的工作效率直接起飞 🚀 私聊群聊方式、单聊串聊模式、角色扮演、图片创作 🚀](https://github.com/ConnectAI-E/Dingtalk-OpenAI)

[chatgpt-web: 基于ChatGPT3.5 API实现的私有化web程序](https://github.com/869413421/chatgpt-web)

[pengzhile/pandora: 潘多拉，一个让你呼吸顺畅的ChatGPT。Pandora, a ChatGPT that helps you breathe smoothly.](https://github.com/pengzhile/pandora)

[ChatALL](https://github.com/sunner/ChatALL/releases)

### AI 应用开发

[大模型 AI 应用全栈开发知识体系 v1.3.1 - 飞书云文档](https://agiclass.feishu.cn/docx/Z3Aed6qXboiF8gxGuaccNHxanOc)
[GitHub - deepseek-ai/awesome-deepseek-integration: Integrate the DeepSeek API into popular softwares](https://github.com/deepseek-ai/awesome-deepseek-integration)
[火山引擎 高代码 Python SDK Arkitect volcengine/ai-app-lab](https://github.com/volcengine/ai-app-lab/)

[AI 全栈学员部分作品集 - 飞书云文档](https://agiclass.feishu.cn/docx/M5xydPVjWovB9exHBjDc7IMYnub)

[知乎《AI 大模型全栈工程师》课程大纲（第 05 期） - 飞书云文档](https://agiclass.feishu.cn/docx/KjFSdqxTZoDDfcxzikHcjjx0nDg)

[AI大模型颠覆程序员的价值 - 程序员的AI大模型进阶之旅0122期](https://www.zhihu.com/xen/market/training/training-video/1730906752995045376/1730906966715797506?education_channel_code=ZHZN-d62bb90dfad9e02) 讲解了自训练 微调等方法
[大模型应用开发技术体系串讲 - 程序员的AI大模型进阶之旅0125期](https://www.zhihu.com/xen/market/training/training-video/1731335160308744192/1731335415037206528?education_channel_code=ZHZN-cd8085beea05e6d) 孙志刚老师回答问题
[使⽤ Assistants API快速搭建领域专属AI - 程序员的AI大模型进阶之旅0122期](https://www.zhihu.com/xen/market/training/training-video/1730906752995045376/1730907032264232960?education_channel_code=ZHZN-d62bb90dfad9e02)

大模型领域岗位推荐
 · 大模型训练师
 · 大模型算法工程师
 · 提示词工程师
 · 大模型全栈开发工程师
 · 大模型方向产品经理
 · 大模型方向项目经理

继续本岗位 · 大大提升效率，横向卷同行，纵向卷上下游
成为超级个体 独立开发者，做自己的小老板
成为大模型训练师 做公司的技术核心
独立创业 凭大模型垂直落地能力解决独有场

使用 Assistants API 快速搭建领域专属AI助手
官方Web界面可体验/调试 https://platform.openapi/playground
Demo框架 及具体实现 Streamlit 简介— A faster way to build and share data apps

[ChatTTS-ui: 一个简单的本地网页界面，直接使用ChatTTS将文字合成为语音，同时支持对外提供API接口。](https://github.com/jianchang512/ChatTTS-ui)

### pengzhile/pandora

[pengzhile/pandora: 潘多拉，一个让你呼吸顺畅的ChatGPT。Pandora, a ChatGPT that helps you breathe smoothly.](https://github.com/pengzhile/pandora)

[ChatGPT Auth 帮助 ChatGPT 被拒用户获取 Access Token。](https://ai.fakeopen.com/auth)

#### docker script

```sh
#!/bin/bash
# docker run -rm -e PANDORA_CLOUD=cloud -e PANDORA_SERVER=0.0.0.0:8899 -p 8899:8899 -d pengzhile/pandora --name=pandora

docker run --detach \
    --hostname 127.0.0.1 \
    --publish 8899:8899 \
    --name pandora \
    --restart always \
    -e TZ=Asia/Shanghai \
    -e PANDORA_CLOUD=cloud \
    -e PANDORA_SERVER=0.0.0.0:8899 \
    --volume /data/pandora:/data \
    pengzhile/pandora
```

### Text-to-SQL

[Canner/WrenAI: Open-source GenBI AI Agent that empowers data-driven teams to chat with their data to generate Text-to-SQL, charts, spreadsheets, reports, and BI](https://github.com/Canner/WrenAI)

[CodePhiliaX/Chat2DB: AI-driven database tool and SQL client, The hottest GUI client, supporting MySQL, Oracle, PostgreSQL, DB2, SQL Server, DB2, SQLite, H2, ClickHouse, and more.](https://github.com/codePhiliaX/Chat2DB)

[vanna-ai/vanna: Chat with your SQL database 📊. Accurate Text-to-SQL Generation via LLMs using RAG.](https://github.com/vanna-ai/vanna)

## prompt

[Maximizing the Potential of LLMs: A Guide to Prompt Engineering](https://www.ruxu.dev/articles/ai/maximizing-the-potential-of-llms/)

[f/awesome-chatgpt-prompts: This repo includes ChatGPT prompt curation to use ChatGPT better.](https://github.com/f/awesome-chatgpt-prompts)

[提示词技巧](https://mp.weixin.qq.com/s/eqIqbbyqlgkCU78SKuZCMw)

### [Learn Prompting](https://learnprompting.org/zh-Hans/docs/intro)

#### Learn Prompting Sample

[🟡 Coding Assistance | Learn Prompting](https://learnprompting.org/docs/basic_applications/coding_assistance)

1. act like a senior developer
2. as a very junior developer
3. You can also dictate that it have a certain area of expertise (e.g., sorting algorithms) or number of years of experience
4. Act as Microsoft SQL Server.

##### English translator and Improver

Prompt: I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is "lovin istanbul and the city"

##### Interviewer

Prompt: I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the position position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is "Hi"

##### English Pronunciation Helper

Prompt: I want you to act as an English pronunciation assistant for Turkish speaking people. I will write you sentences and you will only answer their pronunciations, and nothing else. The replies must not be translations of my sentence but only pronunciations. Pronunciations should use Turkish Latin letters for phonetics. Do not write explanations on replies. My first sentence is "how the weather is in Istanbul?"

##### Travel Guide

Prompt: I want you to act as a travel guide. I will write you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion request is ""I am in Istanbul/Beyoğlu and I want to visit only museums."

## Text to Image

[10款AI绘画生成器，人人都是插画师！](https://pixso.cn/designskills/10-ai-paint-builders/)

[DALL·E 2](https://openai.com/product/dall-e-2)

[Text To Image - AI Image Generator API | DeepAI](https://deepai.org/machine-learning-model/text2img)

[文心一格 - AI艺术和创意辅助平台](https://yige.baidu.com/creation)

[22个国内AI绘画网站汇总](https://zl49so8lbq.feishu.cn/wiki/MdzYw0mtki3OPGkPk03cnd9hnfg#Uaaidf4Mro82FFx8ucic8MUOnBf)

### image prompt

[Guide for prompt writing | BoostPixels](https://boostpixels.com/guide)
[midjourney史上最全教程-持续更新 - Feishu Docs](https://nw3t0riwqkt.feishu.cn/docx/NCVdd118toPLkRxKBexcQZiunJZ)

[墨本关键词助手](https://www.mbprompt.com/#/)
[MidJourney Prompt Tool](https://prompt.noonshot.com/)

### Stable Diffusion

[Stable Diffusion Models: a beginner's guide - Stable Diffusion Art](https://stable-diffusion-art.com/models/)
[ControlNet: A Complete Guide - Stable Diffusion Art](https://stable-diffusion-art.com/controlnet/)
[常用的ControlNet以及如何在Stable Diffusion WebUI中使用 - 知乎](https://zhuanlan.zhihu.com/p/620074109)

[Install Stable Diffusion on Mac](https://uxplanet.org/install-stable-diffusion-ui-on-mac-beginners-guide-351e40a9e8e2)
[webui Online Services · AUTOMATIC1111/stable-diffusion-webui Wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)
[Stability-AI/generative-models: Generative Models by Stability AI](https://github.com/Stability-AI/generative-models)
[Mikubill/sd-webui-controlnet: WebUI extension for ControlNet](https://github.com/Mikubill/sd-webui-controlnet)

### MidJourney 案例

[【Midjourney教程】设计麻瓜也能10分钟上架一套表情包](https://mp.weixin.qq.com/s/FagQ3HdAnx-HLfJK4NRMBQ)

### gpt4all

```sh
#  get started with the CPU quantized gpt4all model
./gpt4all-lora-quantized-OSX-m1

```

### llama

```sh
./main -m ./models/7B/ggml-model-q4_0.bin \
  -t 8 \
  -n 128 \
  -p 'The first man on the moon was '

./talk.sh "The first man on the moon was "
```

### Mistral-7B

来自法国的开源大模型

[最好的7B模型易主，免费开源可商用，来自“欧洲的OpenAI”-今日头条](https://www.toutiao.com/article/7287811935905546763/)
[mistralai (Mistral AI_)](https://huggingface.co/mistralai)

## ChatGPT doc

[Playground - OpenAI API](https://platform.openai.com/playground)

[Assistants overview - OpenAI API](https://platform.openai.com/docs/assistants/overview)

## 理论知识

[Deepseek大模型推理算法其实很简单 | 陈经](https://mp.weixin.qq.com/s/SaK9mlj6NCKxEFig6KFGVQ)

## 机器学习教程

[Getting Started With MachineLearning (all in one)](https://pan.baidu.com/s/1tNXYQNadAsDGfPvuuj7_Tw)

[microsoft/ML-For-Beginners: 12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all](https://github.com/microsoft/ML-For-Beginners)

[microsoft/AI-For-Beginners: 12 Weeks, 24 Lessons, AI for All!](https://github.com/microsoft/AI-For-Beginners)

1. [第一章：全世界最简单的机器学习入门指南](https://zhuanlan.zhihu.com/p/24339995)
2. [第二章：用机器学习制作超级马里奥的关卡](https://zhuanlan.zhihu.com/p/24344720)
3. [第三章:图像识别【鸟or飞机】？深度学习与卷积神经网络] https://zhuanlan.zhihu.com/p/24524583
4. [第四章：用深度学习识别人脸](https://zhuanlan.zhihu.com/p/24567586)
5. [第五章：Google 翻译背后的黑科技：神经网络和序列到序列学习](https://zhuanlan.zhihu.com/p/24590838)
6. [第六章：如何用深度学习进行语音识别？](https://zhuanlan.zhihu.com/p/24703268)

### 召回率和精确率

[算法工程师说的召回是什么意思？ - 拒海空间](https://refusea.com/?p=1546)

在算法工程中，"召回"是一个重要的概念，特别是在信息检索和机器学习领域。

召回率（Recall）是一种衡量模型预测能力的指标，特别是模型识别出相关实例的能力。

具体来说，**召回率是指模型正确识别出的正例（真正例）占所有实际正例（真正例+假反例）的比例**。换句话说，它是模型找到的相关实例占所有相关实例的比例。

例如，如果我们有一个用于检测垃圾邮件的模型，那么召回率就是模型正确标记为垃圾邮件的邮件数量占所有实际垃圾邮件数量的比例。

召回率和精确率（Precision）通常一起使用，以获得模型性能的全面视图。

**精确率是模型预测为正例的实例中实际为正例的比例**，而召回率则关注模型能找到多少实际的正例。

例子
如果有 1000 邮件需要检测，算法检测出有 800 垃圾邮件，实际这 800 里真正的垃圾邮件是 600，同时算法还遗漏了 50 垃圾邮件。那么召回率和精确率是多少？怎么计算的？

在这个例子中，我们可以先定义以下几个概念：

真正例（True Positive，TP）：算法正确地预测为垃圾邮件的邮件数量，即600封。
假正例（False Positive，FP）：算法错误地预测为垃圾邮件的邮件数量，即800（算法预测为垃圾邮件的数量）- 600（真正的垃圾邮件数量）= 200封。
假反例（False Negative，FN）：算法错误地预测为非垃圾邮件的邮件数量，即遗漏的垃圾邮件数量，即50封。
根据这些定义，我们可以计算召回率和精确率：

召回率（Recall）= 真正例 / (真正例 + 假反例) = 600 / (600 + 50) = 0.923，或者说92.3%。
精确率（Precision）= 真正例 / (真正例 + 假正例) = 600 / (600 + 200) = 0.75，或者说75%。
所以，这个垃圾邮件检测算法的召回率是92.3%，精确率是75%。

### 简介

线性回归主要用来解决连续值预测的问题，逻辑回归用来解决分类的问题，输出的属于某个类别的概率，工业界经常会用逻辑回归来做排序

## LLM 介绍

[Getting Started With Large Language Models - DZone Refcardz](https://dzone.com/refcardz/getting-started-with-large-language-models)

文心大模型ERNIE是百度发布的产业级知识增强大模型，涵盖了NLP大模型和跨模态大模型。

https://github.com/PaddlePaddle/ERNIE

## AI 在研发场景落地的现状

- **智能研发插件**：以 Github Copilot/ 通义灵码 /Comate 为代表，主要以 JetBrains、VSCode 为插件形式为用户提供代码补全为主的智能编码服务
- **AI Native 的 IDE**：以 Cursor、Windsurf、MarsCode 为代表，以独立 IDE 的方式为开发者提供服务，而有一些公司如 PearAI 已经开始走开源路径，他们的共同特点是以 VSCode 为技术底座进行二次开发，好处是能极大程度上利用 VSCode 的插件和开源生态
- **CodeReview 智能化**：这个领域起步比较早，但效果始终一般，还需要很长时间的摸索，阿里内部很早就启动了这个项目，但效果并不显著，这里既存在模型的能力的问题，也存在工程化不足的问题
- **RAG 搜索场景**：RAG 其实解决的是搜索和 Summary 的问题，例如知识搜索，智能答疑，但也存在非常大的挑战，例如用户问题的上下文不足，知识不保鲜，信息不完整，很难评测，等等，但由于其门槛比较低，反而是大多数团队会首先涉足的领域
- **其他的场景**：例如智能解决代码冲突，自动解决编译问题等也都在阿里内部平台早已上线，智能诊断，智能监控等均有人在调研中
- **局部智能化的 Agent**：以 Gru.ai 等产品为代表帮助用户生成单元测试，以 readme-ai 为代表帮助开发者生成 Readme，以 RepoAgent 为代表帮用户补充注释等等，而阿里在内部也还实现了帮助用户按整个仓库生成注释，生成单元测试的 Agent ，这类 Agent 的特点是场景比较比较垂直简单，问题不发散，成功率比较高
- **广泛自动化的 Agent**：以 Devin、OpenDevin 为代表，以 SWE-bench 为主要评测集的方式，利用大模型生成实现一个任务的 plan，并调用工具，在一个独立的容器内执行，并且能和用户交互的方式来实现一些简单的 issue 或者需求

## AI 编码助手

[What else should determine my model use in Cline? : r/ChatGPTCoding](https://www.reddit.com/r/ChatGPTCoding/comments/1hsx76e/what_else_should_determine_my_model_use_in_cline/)

### Continue

[Context providers | Continue](https://docs.continue.dev/customize/context-providers#gitlab-merge-request)

add context: Ctrl+I

### Cline

[cline: Autonomous coding agent](https://github.com/cline/cline)

[VSCode + Cline + VLLM + Qwen2.5 = Fast : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1gbb2de/vscode_cline_vllm_qwen25_fast/)

[set up cline and LLM](https://www.reddit.com/r/LocalLLaMA/comments/1gbb2de/comment/ltkf1z3)

```sh
docker run --runtime nvidia --gpus all \
-v ~/.cache/huggingface:/root/.cache/huggingface \
--ipc=host -p 8000:8000 \
vllm/vllm-openai \
--model Qwen/Qwen2.5-32B-Instruct-AWQ  --tensor-parallel-size 2 \
--quantization awq_marlin --enable-auto-tool-choice --tool-call-parser hermes \
--kv-cache-dtype fp8_e5m2 \
--rope-scaling '{ "factor": 4.0, "original_max_position_embeddings": 32768, "type": "yarn" }'
```

Install the Cline extension onto VSCode: https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev

Cline 使用 GitHub Copilot：API Provider > vscode lm api > Language Model > github copilot

### Gemini Code Assist

[Gemini Code Assist for business | Google Cloud](https://codeassist.google/products/business)

## ollama

[ollama/ollama: Get up and running with large language models.](https://github.com/ollama/ollama)
[ollama/ollama - Docker Image | Docker Hub](https://hub.docker.com/r/ollama/ollama)
[ollama Importing a model](https://github.com/ollama/ollama/blob/main/docs/import.md)
[ollama api](https://github.com/ollama/ollama/blob/main/docs/api.md)

```sh
docker run -d --env OLLAMA_HOST=0.0.0.0:11434 -v /data/docker/ollama/ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
alias ollama='docker exec -it ollama ollama'

# Show model information
ollama show llama3.2
# List models on your computer
ollama list
# List which models are currently loaded
ollama ps
# Start Ollama
ollama serve

# Run a model
ollama run modelName
# Show more info: duration and eval rate
ollama run modelName --verbose
# Stop a running model
ollama stop modelName
# Remove a model
ollama rm modelName

#
# Run with parameter: num_ctx Context Window
ollama run llama3 –set parameter num_ctx 4096 --temperature 0.7 --top-p 0.9 --memory-limit 8GB --batch-size 8
# 检查系统资源
ollama run llama2 --debug
# 使用性能分析工具
ollama run llama2 --profile

# 使用curl测试API
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello, how are you?"
}'

# 批处理
ollama run llama2 < batch_prompts.txt > responses.txt

# [ollama Model library](https://ollama.com/library)

# deepseek
# deepseek 1.5b ~ 32b 上下文长度 32,768  最大输入 32,768
# [DeepSeek R1和DeepSeek V3 API_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/developer-reference/deepseek#94082c580cot9)
# [deepseek-r1:1.5b](https://ollama.com/library/deepseek-r1:1.5b)
# DeepSeek-R1-Distill-Qwen-1.5B
ollama run deepseek-r1:1.5b
# DeepSeek-R1-Distill-Qwen-7B
ollama run deepseek-r1:7b
# DeepSeek-R1-Distill-Qwen-32B
ollama run deepseek-r1:32b

# DeepSeek-R1-Distill-Llama-70B
# Error: model requires more system memory (37.3 GiB) than is available (24.7 GiB)
ollama run deepseek-r1:70b

# 34b 执行太慢
ollama run codellama:34b

# 70b Error: model requires more system memory (31.2 GiB) than is available (27.3 GiB)
ollama run codellama:70b

# Cline uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.5 Sonnet for its advanced agentic coding capabilities.
ollama run qwen2.5:32b

# [DeepSeek Coder](https://deepseekcoder.github.io/)
# [DeepSeek-Coder/Evaluation/HumanEval - deepseek-ai/DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder/tree/main/Evaluation/HumanEval)
#
# DeepSeek-Coder-V2 comes in two primary types: Instruct and Base.
# Base model
# A base model is a general-purpose language model trained on a large corpus of text (e.g., code, documentation, and natural language). It has no specific fine-tuning for instruction-following or task-oriented behaviour.
#
# Instruct model
# An instruct model is a fine-tuned version of a base model, optimized to follow instructions and perform specific tasks. It is trained on datasets containing instruction-response pairs (e.g., “Write a SQL query to find duplicates” → “SELECT …”). Excels at task-oriented interactions (e.g., debugging, refactoring, answering questions).

# [second-state-DeepSeek-Coder-V2-Lite-Instruct-GGUF: Mirror of https://huggingface.co/second-state/DeepSeek-Coder-V2-Lite-Instruct-GGUF](https://gitee.com/hf-models/second-state-DeepSeek-Coder-V2-Lite-Instruct-GGUF)
# DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf  Quant method: Q4_K_M
# medium, balanced quality - recommended
# DeepSeek-Coder-V2-Instruct 236B
# DeepSeek-Coder-V2-Lite-Instruct 16B
ollama run deepseek-coder-v2:16b-lite-instruct-q4_K_M

# [MFDoom/deepseek-r1-tool-calling:8b](https://ollama.com/MFDoom/deepseek-r1-tool-calling:8b)
# DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen. With Tool Calling support.
ollama run MFDoom/deepseek-r1-tool-calling:8b
```

### ollama 命令

[ollama的命令注解_ollama命令行-CSDN博客](https://blog.csdn.net/sunyuhua_keyboard/article/details/141174683)

```sh
>>> /?
Available Commands:
  /set            Set session variables
  /show           Show model information
  /load <model>   Load a session or model
  /save <model>   Save your current session
  /clear          Clear session context
  /bye            Exit
  /?, /help       Help for a command
  /? shortcuts    Help for keyboard shortcuts

Use """ to begin a multi-line message.

>>> /set
Available Commands:
  /set parameter ...     Set a parameter
  /set system <string>   Set system message
  /set template <string> Set prompt template
  /set history           Enable history
  /set nohistory         Disable history
  /set wordwrap          Enable wordwrap
  /set nowordwrap        Disable wordwrap
  /set format json       Enable JSON mode
  /set noformat          Disable formatting
  /set verbose           Show LLM stats
  /set quiet             Disable LLM stats
```

主命令
/set: 用于设置会话参数和配置。例如，设置消息格式、启用或禁用历史记录等。
/show: 显示模型的相关信息，如当前加载的模型的名称、版本等。
/load : 加载一个特定的模型或会话。你可以指定一个模型的名称或路径来加载它。
/save : 保存当前的会话状态或模型。你可以将当前会话或模型的配置保存为一个文件，以便以后使用。
/clear: 清除会话上下文。这将删除当前会话中的所有历史记录或对话内容。
/bye: 退出会话。这个命令将结束当前与模型的对话，并退出程序。
/? 或 /help: 显示帮助信息。如果你需要关于某个命令的详细信息，可以使用这些命令。
/? shortcuts: 显示键盘快捷键的帮助信息。这可以帮助你更快速地进行操作。

/set 子命令
/set parameter …: 设置某个参数。这可能包括一些特定的配置项，用于控制模型的行为或输出方式。
/set system : 设置系统消息。你可以提供一个字符串作为系统消息，这通常用于在对话开始时向模型传达背景信息或特定指令。
/set template : 设置提示模板。这允许你定义一个模板，用于格式化你与模型的对话。
/set history: 启用历史记录。这意味着模型会保存你当前会话中的对话历史，以便稍后参考或使用。
/set nohistory: 禁用历史记录。使用这个命令后，模型将不会保存会话历史。
/set wordwrap: 启用自动换行。这在长文本消息的情况下非常有用，可以让文本自动换行以便于阅读。
/set nowordwrap: 禁用自动换行。如果不需要自动换行，可以使用这个命令。
/set format json: 启用JSON模式格式化输出。这会将模型的响应格式化为JSON格式，方便结构化数据的处理。
/set noformat: 禁用格式化输出。如果不需要任何特定格式的输出，可以使用这个命令。
/set verbose: 启用详细模式，这会显示与LLM相关的统计信息，如响应时间、消耗资源等。
/set quiet: 禁用详细模式。启用后，将不会显示与LLM相关的统计信息，输出会更简洁。

应用场景

- 管理会话: 你可以使用 /load 和 /save 命令来保存和加载特定的会话状态，从而在不同时间点继续先前的工作。
- 自定义消息格式: 使用 /set template 和 /set format json 可以自定义和控制模型输出的格式，适用于不同的应用场景。
- 调试和性能监控: 通过 /set verbose 和 /set quiet，你可以控制是否查看模型的统计信息，这在调试或性能监控时特别有用。

这些命令和设置可以帮助你更灵活地控制模型的行为和会话的管理，使其更好地适应你的使用需求。

### Where are models stored?

macOS: ~/.ollama/models
Linux: /usr/share/ollama/.ollama/models
Windows: %userprofile%\.ollama\models

默认的模型保存路径位于C盘，（%userprofile%\.ollama\models），可以通过设置 OLLAMA_MODELS 进行修改，然后重启终端，重启ollama服务（需要去状态栏里关掉程序）

```bat
setx OLLAMA_MODELS "D:\ollama_model"
```

### ollama Setting environment variables on Windows

[ollama/docs/faq](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-windows)

On Windows, Ollama inherits your user and system environment variables.

1. First Quit Ollama by clicking on it in the task bar.
2. Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for environment variables.
3. Click on Edit environment variables for your account.
4. Edit or create a new variable for your user account for OLLAMA_HOST, OLLAMA_MODELS, etc.
   1. OLLAMA_HOST=0.0.0.0:11434
5. Click OK/Apply to save.
6. Start the Ollama application from the Windows Start menu.

### keep a model loaded in memory or make it unload immediately?

The `keep_alive` parameter can be set to:

- a duration string (such as "10m" or "24h")
- a number in seconds (such as 3600)
- any negative number which will keep the model loaded in memory (e.g. -1 or "-1m")
- '0' which will unload the model immediately after generating a response

```sh
# to preload a model and leave it in memory use:
curl http://10.10.65.77:11434/api/generate -d '{"model": "deepseek-r1:7b", "keep_alive": -1}'

# To unload the model and free up memory use:
curl http://10.10.65.77:11434/api/generate -d '{"model": "MFDoom/deepseek-r1-tool-calling:8b", "keep_alive": 0}'
```

### config Ollama

#### context window size

[How to Increase Ollama Context Size: A Complete Step-by-Step Guide - Deep AI — Leading Generative AI-powered Solutions for Business](https://deepai.tn/glossary/ollama/how-increase-ollama-context-size/)
[GGUF specification](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)
huggingface 上或模型的 config 文件中记录的大小：sequence_len: 4096

By default, Ollama uses a context window size of 2048 tokens.

show the context size *really is* in the current model being run `ollama show (model name)`. In the ollama CLI, `/show info`

```sh
ollama show deepseek-r1:7b
# Model
#   architecture        qwen2
#   parameters          7.6B
#   context length      131072
#   embedding length    3584
#   quantization        Q4_K_M

# Parameters
#   stop    "<｜begin▁of▁sentence｜>"
#   stop    "<｜end▁of▁sentence｜>"
#   stop    "<｜User｜>"
#   stop    "<｜Assistant｜>"

# When using the API, specify the num_ctx parameter:
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "options": {
    "num_ctx": 4096
  }
}'

/set parameter num_ctx 131072
/save deepseek-r1:7b-ctx-128k

/set parameter num_ctx 65536
/save deepseek-r1:7b-ctx-64k
/set parameter num_ctx 32768
/save deepseek-r1:7b-ctx-32k
/set parameter num_ctx 16384
/save deepseek-r1:7b-ctx-16k
/set parameter num_ctx 8192
/save deepseek-r1:7b-ctx-8k
/set parameter num_ctx 6144
/save deepseek-r1:7b-ctx-6k
/set parameter num_ctx 4096
/save deepseek-r1:7b-ctx-4k
/set parameter num_ctx 3072
/save deepseek-r1:7b-ctx-3k
/set parameter num_ctx 2048
/save deepseek-r1:7b-ctx-2k


ollama ps
# NAME              ID              SIZE      PROCESSOR    UNTIL
# deepseek-r1:7b-ctx-128k    946b57ccb619    12 GB    100% CPU     4 minutes from now
# deepseek-r1:7b-ctx-64k    af0516d55326    13 GB    58%/42% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-64k    af0516d55326    8.4 GB    100% CPU     4 minutes from now  2025年3月3日
# deepseek-r1:7b-ctx-32k    1b1debea5066    9.5 GB    39%/61% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-16k    3fc83f5dea49    7.2 GB    19%/81% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-8k    fdb679a1bd50    6.3 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-6k    edd7d65f4ea1    6.1 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-4k    96cae7f73d2b    6.0 GB    7%/93% CPU/GPU    4 minutes from now
# deepseek-r1:7b-ctx-3k    25f4b7c66be3    5.5 GB    100% GPU     4 minutes from now
# deepseek-r1:7b-ctx-2k    9be990020b49    5.4 GB    100% GPU     4 minutes from now
# deepseek-r1:7b    0a8c26691023    5.4 GB    100% GPU     4 minutes from now
```

### cline with local deepseek

本地允许 deepseek 32b 模型，cline 调用时报错
[Cline is having trouble... · Issue #1094 · cline/cline](https://github.com/cline/cline/issues/1094)

```log
Cline uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.5 Sonnet for its advanced agentic coding capabilities.

```

## 模型部署工具

Ollama：适合个人 + 本地部署 + 轻量体验
vLLM：适合企业级 + 服务器部署 + 高性能扩展

[对接本地大模型时，选择 Ollma 还是 vLLM？ - OSCHINA - 中文开源技术交流社区](https://www.oschina.net/news/321572)

### 模型显存使用量计算

[模型显存使用量计算 — Xinference](https://inference.readthedocs.io/zh-cn/stable/models/model_memory.html)
[LLM Model VRAM Calculator - a Hugging Face Space by NyxKrage](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)

```sh
xinference cal-model-mem -s 7 -f gptq -c 8192 -n GOT-OCR2_0
xinference cal-model-mem -s 7 -q Int4 -f gptq -c 16384 -n qwen1.5-chat
# model_name: qwen1.5-chat
# kv_cache_dtype: 16
# model size: 7.0 B
# quant: Int4
# context: 16384
# gpu mem usage:
#   model mem: 4139 MB
#   kv_cache: 8192 MB
#   overhead: 650 MB
#   active: 17024 MB
#   total: 30005 MB (30 GB)
```

### ollama 模型部署工具

Ollama, a popular local LLM deployment tool, supports a broad range of open-source LLMs and offers an intuitive experience, making it ideal for single-user, local environments.

### OpenLLM

[bentoml/OpenLLM: Run any open-source LLMs, such as Llama, Mistral, as OpenAI compatible API endpoint in the cloud.](https://github.com/bentoml/OpenLLM)

From Ollama to OpenLLM: Running LLMs in the Cloud
[From Ollama to OpenLLM: Running LLMs in the Cloud](https://www.bentoml.com/blog/from-ollama-to-openllm-running-llms-in-the-cloud)

```powershell
# [【解决】无法将“XXX”项识别为 cmdlet、函数、脚本文件或可运行程序的名称。请检查名称的拼写，如果包括路径，请确保路径正确，然后再试一次_无法将“labelme”项识别为 cmdlet、函数、脚本文件或可运行程序的名称。请检查名-CSDN博客](https://blog.csdn.net/weixin_41362657/article/details/110649744)
PS D:\>
Get-ExecutionPolicy -List

        Scope ExecutionPolicy
        ----- ---------------
MachinePolicy       Undefined
   UserPolicy       Undefined
      Process       Undefined
  CurrentUser       Undefined
 LocalMachine       Undefined

# Scope: Process, CurrentUser, LocalMachine, UserPolicy, MachinePolicy
# ExecutionPolicy: Unrestricted, RemoteSigned, AllSigned, Restricted, Default, Bypass, Undefined”
Set-ExecutionPolicy RemoteSigned -Scope CurrentUser
Set-ExecutionPolicy Unrestricted -Scope CurrentUser
Set-ExecutionPolicy RemoteSigned -Scope LocalMachine
```

### LocalAI

[LocalAI-examples/langchain-chroma at main · mudler/LocalAI-examples](https://github.com/mudler/LocalAI-examples/tree/main/langchain-chroma)

```sh
# 配置到dify 时 404
# 2025-02-11 11:51:10 3:51AM WRN Client error ip=172.20.0.1 latency="24.32µs" method=POST status=404 url=/rerank

curl http://10.10.65.77:8080/console/api/workspaces/current/models/model-types/rerank

curl http://10.10.65.77:8080/v1/rerank \
  -H "Content-Type: application/json" \
  -d '{
  "model": "cross-encoder",
  "query": "Organic skincare products for sensitive skin",
  "documents": [
    "Eco-friendly kitchenware for modern homes",
    "Biodegradable cleaning supplies for eco-conscious consumers",
    "Organic cotton baby clothes for sensitive skin",
    "Natural organic skincare range for sensitive skin",
    "Tech gadgets for smart homes: 2024 edition",
    "Sustainable gardening tools and compost solutions",
    "Sensitive skin-friendly facial cleansers and toners",
    "Organic food wraps and storage solutions",
    "All-natural pet food for dogs with allergies",
    "Yoga mats made from recycled materials"
  ],
  "top_n": 3
}'

```

### xinference

[在Xinference上部署自定义大模型——FreedomIntelligence/HuatuoGPT2-13B为例 - 知乎](https://zhuanlan.zhihu.com/p/685747169)

```sh
# 将模型下载源设置为 ModelScope。设置环境变量 XINFERENCE_MODEL_SRC=modelscope
docker pull registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference
docker image tag registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xprobe_xinference/xinference:latest
docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9997:9997 --gpus all xprobe/xinference:v<your_version> xinference-local -H 0.0.0.0 --log-level debug

docker run --detach \
  --env TZ=Asia/Shanghai \
  --publish 9997:9997 \
  --name xinference \
  --restart always \
  -e XINFERENCE_MODEL_SRC=modelscope \
  -v //d/xinference/.xinference:/root/.xinference \
  -v //d/xinference/.cache/huggingface:/root/.cache/huggingface \
  -v //d/xinference/.cache/modelscope:/root/.cache/modelscope \
  --gpus all \
  xprobe_xinference/xinference \
  sh /root/.xinference/startup.sh

# Windows下改成一行执行
docker run --detach --env TZ=Asia/Shanghai --publish 9997:9997 --name xinference --restart always -e XINFERENCE_MODEL_SRC=modelscope -v //d/xinference/.xinference:/root/.xinference -v //d/xinference/.cache/huggingface:/root/.cache/huggingface -v //d/xinference/.cache/modelscope:/root/.cache/modelscope --gpus all xprobe_xinference/xinference sh /root/.xinference/startup.sh

alias xinference='docker exec -it xinference xinference'
```

```sh
#!/bin/bash
# startup.sh 放在外部磁盘挂载进去，启动时执行
# 启动且后台运行
xinference-local -H 0.0.0.0 &
# xinference-local -H 0.0.0.0 --log-level debug &
# 检测是否启动
while true; do
  if curl -s "http://localhost:9997" > /dev/null; then
    break
  else
    sleep 1
  fi
done

xinference launch --model-name jina-embeddings-v3 --model-type embedding &
xinference launch --model-name jina-reranker-v2 --model-type rerank &

#自动加载 embedding
xinference launch --model-name jina-embeddings-v3 --model-type embedding &
xinference launch --model-name bge-m3 --model-type embedding &

#自动加载 rerank
xinference launch --model-name jina-reranker-v2 --model-type rerank &
xinference launch --model-name bge-reranker-large --model-type rerank &
xinference launch --model-name bge-reranker-v2-minicpm-layerwise --model-type rerank &

#等待后台运行结束，实际上xinference-local是不会结束的，所以能保证此脚本进程不结束，从而不会自动重启
wait
```

```sh
# 本地安装
conda create --name py312 python=3.12
conda activate py312

# 安装xinference的依赖
pip install "xinference[all]"

# 启动xinference
xinference-local # 我使用这个命令启动不了
# 设置使用modelscope下载模型
# 如果你就一块gpu还是0的话就要指定启动
CUDA_VISIBLE_DEVICES=0
# 使用这个命令可以启动
XINFERENCE_HOME=d:/xinference/ XINFERENCE_MODEL_SRC=modelscope xinference-local --host 0.0.0.0 --port 9997

# 列举本地模型
xinference list --endpoint "http://127.0.0.1:9997"

# [jina-embeddings-v2-base-zh — Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/embedding/jina-embeddings-v2-base-zh.html)
# Model ID: jinaai/jina-embeddings-v2-base-zh
# xinference launch --model-name jina-embeddings-v2-base-zh --model-type embedding
xinference launch --model-name jina-embeddings-v3 --model-type embedding &
xinference launch --model-name bge-m3 --model-type embedding &

# [jina-reranker-v2 — Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/rerank/jina-reranker-v2.html)
# Model ID: jinaai/jina-reranker-v2-base-multilingual
xinference launch --model-name jina-reranker-v2 --model-type rerank &
xinference launch --model-name bge-reranker-large --model-type rerank &
xinference launch --model-name bge-reranker-v2-minicpm-layerwise --model-type rerank &

# OCR 模型 [GOT-OCR2_0 — Xinference](https://inference.readthedocs.io/zh-cn/v0.16.3/models/builtin/image/got-ocr2_0.html)
xinference launch --model-name GOT-OCR2_0 --model-type image
# Launch model name: GOT-OCR2_0 with kwargs: {}
# Model uid: GOT-OCR2_0
```

### VLLM

A tool designed to run LLMs very efficiently, especially when serving many users at once.
[Ollama vs VLLM: Which Tool Handles AI Models Better? | by Naman Tripathi | Medium](https://medium.com/@naman1011/ollama-vs-vllm-which-tool-handles-ai-models-better-a93345b911e6)

[Using Docker — vLLM](https://docs.vllm.ai/en/latest/deployment/docker.html)
[aneeshjoy/vllm-windows: Docker compose to run vLLM on Windows](https://github.com/aneeshjoy/vllm-windows)
[vllm/vllm-openai Tags | Docker Hub](https://hub.docker.com/r/vllm/vllm-openai/tags)

```sh

--model
# Name or path of the huggingface model to use. Default: “facebook/opt-125m”

--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]
# The model name(s) used in the API. If multiple names are provided, the server will respond to any of the provided names. The model name in the model field of a response will be the first name in this list. If not specified, the model name will be the same as the --model argument. Noted that this name(s) will also be used in model_name tag content of prometheus metrics, if multiple names provided, metrics tag will take the first one.

--gpu-memory-utilization <value>
# gpu-memory-utilization 是用于设置 GPU 内存利用率的参数，<value> 是一个介于 0 到 1 之间的浮点数，表示 GPU 内存的使用比例
```

```sh
# By default, vLLM downloads models from HuggingFace. If you would like to use models from ModelScope, set the environment variable VLLM_USE_MODELSCOPE before initializing the engine.
# default max-model-len: max_seq_len=32768
    # --env "HUGGING_FACE_HUB_TOKEN=hf_oo" \
    # --model mistralai/Mistral-7B-v0.1
docker run --runtime nvidia --gpus all \
    --detach \
    --env TZ=Asia/Shanghai \
    --name vllm \
    --restart always \
    --env VLLM_USE_MODELSCOPE=True \
    -v //d/workspace/vllm/.cache/:/root/.cache/ \
    -v //d/workspace/models/:/models/ \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:v0.7.2 \
    --model /models/DeepSeek-R1-Distill-Qwen-32B \
    --served-model-name deepseek-reasoner \
    --max-model-len 15520 \
    --gpu-memory-utilization 0.9

# Load and run the model:
vllm serve "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
# Load and run the model:
docker exec -it my_vllm_container bash -c "vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

# Test by accessing the /models endpoints
http://127.0.0.1:8000/v1/models

# Check throughput ( I am running on a RTX 3090 )
http://127.0.0.1:8000/metrics

# Call the server using curl:
curl -X POST "http://localhost:8000/v1/chat/completions" \
    -H "Content-Type: application/json" \
    --data '{
        "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "messages": [
            {
                "role": "user",
                "content": "What is the capital of France?"
            }
        ]
    }'

# OpenAI Completions API with vLLM
curl http://localhost:8000/v1/completions \
curl http://192.168.2.4:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
    }'

# OpenAI Chat Completions API with vLLM
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"}
        ]
    }'
```

## 本地知识库搭建

[chatchat-space/Langchain-Chatchat: Langchain-Chatchat（原Langchain-ChatGLM）基于 Langchain 与 ChatGLM 等语言模型的本地知识库问答 | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM) QA app with langchain](https://github.com/chatchat-space/Langchain-Chatchat)
[Langchain-ChatGLM：基于本地知识库问答_langchain chatglm-CSDN博客](https://blog.csdn.net/dzysunshine/article/details/131003488)

[AnythingLLM、Dify 与 Open-WebUI：如何接入 Ollama，它们有何不同？一、前言 随着大语言模 - 掘金](https://juejin.cn/post/7455148200627781684)
[Dify、Anything LLM、Ollama、硅基流动 分别与 DeepSeek 联合搭建本地知识库的对比表格，涵盖功能、性能、成本和适用场景等核心维度。 #普通人如何玩转DeepSeek# #deepseek#](https://www.toutiao.com/w/1823311450553344/?app=news_article&category_new=search_thread_aggr&chn_id=94349607399&is_new_connect=0&is_new_user=0&req_id_new=202502090734553364518F875E49146204&share_did=MS4wLjACAAAAZtHq-FtDCLT2HiypHOsw85chAV9wX6K1Jv5UfAUyX0mVQ7HN00yXQhdxXLFA_4OY&share_token=D19DE10F-BE2F-4A6D-A704-AA68397479DC&share_uid=MS4wLjABAAAADwEoMfih4HUgYxYOjTmk9NvGlX1gGSG-ctDaxpGLfOkP-tRdhwDh6SLBAt5iihFb&source=m_redirect&timestamp=1739057696&tt_from=weixin_moments&use_new_style=1&utm_campaign=client_share&utm_medium=toutiao_ios&utm_source=weixin_moments&wxshare_count=1)

LangChain 是一个用于开发由语言模型驱动的应用程序的框架，主要拥有 3个能力：

1. 可以调用LLM模型
2. 可以将 LLM 模型与外部数据源进行连接
3. 允许与 LLM 模型进行交互

知识库问答实现步骤
基于Langchain思想实现基于本地知识库的问答应用。实现过程如下：
1、加载文件
2、读取文本
3、文本分割
4、文本向量化
5、问句向量化
6、在文本向量中匹配出与问句向量最相似的top k个
7、匹配出的文本作为上下文和问题一起添加到prompt中
8、提交给LLM生成回答。

### open-webui

[open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)](https://github.com/open-webui/open-webui)

### MaxKB 方案

[MaxKB/README_CN.md at main · 1Panel-dev/MaxKB](https://github.com/1Panel-dev/MaxKB/blob/main/README_CN.md)

[MaxKB 离线安装包下载 - FIT2CLOUD 飞致云](https://community.fit2cloud.com/#/download/maxkb/v1-9-1)

```sh
# Linux 机器
docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data -v ~/.python-packages:/opt/maxkb/app/sandbox/python-packages cr2.fit2cloud.com/1panel/maxkb

# Windows 机器
docker run -d --name=maxkb --restart=always -p 8080:8080 -v C:/maxkb:/var/lib/postgresql/data -v C:/python-packages:/opt/maxkb/app/sandbox/python-packages cr2.fit2cloud.com/1panel/maxkb
docker run -d --name=maxkb --restart=always -p 8080:8080 -v d:/docker/maxkb:/var/lib/postgresql/data -v d:/docker/maxkb/python-packages:/opt/maxkb/app/sandbox/python-packages 1panel/maxkb:v1.10.0-lts

# 用户名: admin
# 密码: MaxKB@123..
# Max1+1
```

### anything-llm

[Mintplex-Labs/anything-llm: The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, and more.](https://github.com/Mintplex-Labs/anything-llm)

[anything-llm HOW_TO_USE_DOCKER](https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/HOW_TO_USE_DOCKER.md)

```powershell
# Run this in powershell terminal
$env:STORAGE_LOCATION="$HOME\Documents\anythingllm"; `
If(!(Test-Path $env:STORAGE_LOCATION)) {New-Item $env:STORAGE_LOCATION -ItemType Directory}; `
If(!(Test-Path "$env:STORAGE_LOCATION\.env")) {New-Item "$env:STORAGE_LOCATION\.env" -ItemType File}; `
docker run -d -p 3001:3001 `
--cap-add SYS_ADMIN `
-v "$env:STORAGE_LOCATION`:/app/server/storage" `
-v "$env:STORAGE_LOCATION\.env:/app/server/.env" `
-e STORAGE_DIR="/app/server/storage" `
mintplexlabs/anythingllm;

# anyadmin / Anyadmin1+1
```

### FastGPT

[labring/FastGPT: FastGPT is a knowledge-based platform built on the LLMs, offers a comprehensive suite of out-of-the-box capabilities such as data processing, RAG retrieval, and visual AI workflow orchestration, letting you easily develop and deploy complex question-answering systems without the need for extensive setup or configuration.](https://github.com/labring/FastGPT)

[快速了解 FastGPT | FastGPT](https://doc.tryfastgpt.ai/docs/intro/)

## Dify

[Dify github](https://github.com/langgenius/dify)
[在线版 Studio - Dify](https://cloud.dify.ai/apps)

[Dify 文档](https://docs.dify.ai/zh-hans)

[特性与技术规格 | Dify](https://docs.dify.ai/zh-hans/getting-started/readme/features-and-specifications)

[DifyShare - Share your flows. View the magic.](https://difyshare.com/)
[BannyLon/DifyAIA: 基于Dify自主创建的AI应用DSL工作流](https://github.com/BannyLon/DifyAIA)

Dify is an open-source LLM app development platform. Dify's intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.

案例：

- 训练出专属于“你”的问答机器人
- 官网 AI 智能客服
- 接入微信
- 接入钉钉

### Dify 0.15.3 deploy

[Docker Compose 部署 | Dify](https://docs.dify.ai/zh-hans/getting-started/install-self-hosted/docker-compose)

.env 修改部分

```sh
# .env

# Used to change the OpenAI base address, default is https://api.openai.com/v1.
# When OpenAI cannot be accessed in China, replace it with a domestic mirror address,
# or when a local model provides OpenAI compatible API, it can be replaced.
OPENAI_API_BASE=https://api.openai.com/v1

# Defaults to gevent. If using windows, it can be switched to sync or solo.
SERVER_WORKER_CLASS=gevent

# Upload file size limit, default 15M.
UPLOAD_FILE_SIZE_LIMIT=15
# Upload image file size limit, default 10M.
UPLOAD_IMAGE_FILE_SIZE_LIMIT=10
# Upload video file size limit, default 100M.
UPLOAD_VIDEO_FILE_SIZE_LIMIT=100
# Upload audio file size limit, default 50M.
UPLOAD_AUDIO_FILE_SIZE_LIMIT=50

MAIL_DEFAULT_SEND_FROM=自己的邮箱
# SMTP server configuration, used when MAIL_TYPE is `smtp`
SMTP_SERVER= 对应邮箱的smtp，一般都在设置里
SMTP_PORT=465
SMTP_USERNAME= 自己的邮箱
SMTP_PASSWORD=  自己的密码
SMTP_USE_TLS=true
SMTP_OPPORTUNISTIC_TLS=false
```

Docker Compose 部署

```sh
# 克隆 Dify 源代码至本地环境。
# 假设当前最新版本为 0.15.3
git clone https://github.com/langgenius/dify.git --branch 0.15.3

# 启动 Dify
# 1.  进入 Dify 源代码的 Docker 目录
cd dify/docker
# 2.  复制环境配置文件
cp .env.example .env
# 3.  启动 Docker 容器
docker-compose up -d
```

更新 Dify 进入 dify 源代码的 docker 目录，按顺序执行以下命令：

```sh
cd dify/docker
docker compose down
git pull origin main
docker compose pull
docker compose up -d
```

访问 Dify
你可以先前往管理员初始化页面设置设置管理员账户：

```sh
# 本地环境
http://localhost/install

# 服务器环境
http://your_server_ip/install

# Dify 主页面：
# 本地环境
http://localhost

# 服务器环境
http://your_server_ip
```

### 如何重置dify管理员密码

```sh
docker exec -it docker-api-1 flask reset-password
# 然后按照提示输入管理员email以及两次新密码即可。
```

### dify 钉钉

[将 Dify 应用与钉钉机器人集成 | Dify](https://docs.dify.ai/zh-hans/learn-more/use-cases/dify-on-dingtalk)

### 部署模型

[Integrate Local Models Deployed by Xinference | Dify](https://docs.dify.ai/development/models-integration/xinference)
[Integrate Local Models Deployed by OpenLLM | Dify](https://docs.dify.ai/development/models-integration/openllm)
[Integrate Local Models Deployed by LocalAI | Dify](https://docs.dify.ai/development/models-integration/localai)

wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O bert
wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j

### Embedding model

[Embedding models · Ollama Blog](https://ollama.com/blog/embedding-models)

[MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)
MTEB（Massive Text Embedding Benchmark）是一个用于评估文本嵌入（Embedding）模型的综合性基准测试平台。通过多任务和多数据集的组合，MTEB可以全面衡量不同Embedding模型在各种自然语言处理（NLP）任务中的表现，如文本分类、语义检索、文本聚类等。

```sh
ollama pull mxbai-embed-large
```

### reranking models

Ollama rerank model [linux6200/bge-reranker-v2-m3](https://ollama.com/linux6200/bge-reranker-v2-m3)

deploy local embedding/reranking models using xinference/LocalAI/OpenLLM.

[Using Xinference — Xinference](https://inference.readthedocs.io/en/latest/getting_started/using_xinference.html#using-xinference-with-docker)

```sh
docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9998:9997 --gpus all xprobe/xinference:<your_version> xinference-local -H 0.0.0.0 --log-level debug

# 1024 维度  最大 token 数 8192
jina-embeddings-v3
```

### dify issue

```sh
# agent 调用雅虎财经
# prompt 今天有哪些新闻
[ollama] Error: API request failed with status code 400: {"error":"registry.ollama.ai/library/deepseek-r1:7b does not support tools"}
```

## firecrawl

[firecrawl/CONTRIBUTING](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md)

[localhost:3002](http://localhost:3002/)
[admin/queues](http://localhost:3002/admin/queues)

```sh
docker pull node:18-slim
docker pull node:22-slim
docker pull rust:1-slim
docker pull golang:1.24


curl -X POST http://localhost:3002/v1/crawl \
    -H 'Content-Type: application/json' \
    -d '{
      "url": "https://www.baidu.com"
    }'

curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "limit": 10,
      "scrapeOptions": {
        "formats": ["markdown", "html"]
      }
    }'


```

## GPU 介绍

GPU 命名规则解读

[一文读懂 NVIDIA GPU 产品线-51CTO.COM](https://www.51cto.com/article/805028.html)
[英伟达GPU各型号对比 - 知乎](https://zhuanlan.zhihu.com/p/718450083)
[比较 GeForce 系列最新一代显卡和前代显卡 | NVIDIA](https://www.nvidia.cn/geforce/graphics-cards/compare/)

### 字母： 架构代号（Architecture）

新的架构通常代表着性能、能效比和新技术的显著提升。

代表 GPU 的核心架构，通常用一个或多个字母表示，代表 GPU 的微架构。例如：

K：Kepler 架构 2012
V：Volta 架构 2017
T：Turing 架构 2018, RTX 20 系列, GTX 16 系列
A：Ampere 架构 2019, RTX 30 系列
H：Hopper 架构 2022, RTX 4000 系列
L: Ada Lovelace 架构 2022, RTX 40 系列
B: Blackwell 架构, RTX 50 系列

### 数字：性能层级（Tier）

通常用数字表示，数字越大通常代表性能越强。

代表 GPU 的具体型号，通常用一个或多个数字表示。例如：

“4” 系列：入门级或低功耗级
“10” 系列：中端推理优化级
“40” 系列：高端图形和虚拟工作站级
“100” 系列：旗舰级高性能计算和人工智能级

### 常见的GPU 型号对比解析：基于 GPU 命名推断显卡特性

示例一：T4 与 L4 的比较

L4 是 T4 的直接后继者，属于同一性能层级，针对相似的应用场景设计。然而，两者在微架构和技术规格上存在显著差异：

微架构： L4 采用更新的 Ada Lovelace 架构（2023 年发布），而 T4 则采用较早的 Turing 架构（2018 年发布）。
显存容量： L4 配备了更大的显存容量，达到 24 GB，而 T4 仅有 16 GB。
核心数量和性能： L4 拥有更多且更强大的计算核心，因此在性能上优于 T4。
虽然两者的目标功耗相似，但 L4 凭借更先进的架构和更高的显存容量，在相同的功耗下能够提供更强的计算性能，更适合处理对显存容量有较高要求的任务。

示例二：A10 与 A100 的比较

A100 是基于 Ampere 架构的旗舰级产品，而 A10 则是该架构下的一个较低层级的型号。两者都基于相同的 Ampere 微架构，但在规模和性能上存在显著差异：

核心数量和性能： A100 拥有远多于 A10 的计算核心，因此在计算性能上远超 A10。
显存容量： A100 配备了更大的显存容量，以支持更大规模的模型训练和推理。
功耗： 由于规模更大、性能更强，A100 的功耗也高于 A10。
因此，A100 更适合需要处理大规模模型训练、微调和高吞吐量推理等 demanding 计算任务的场景，而 A10 则更适合对成本和功耗敏感、对性能要求相对较低的应用场景。

### 面向 AI 和机器学习的显卡

| 排名 | GPU 型号 | 架构 | CUDA 核心数 | 显存 | 主要用途 |
| --- | --- | --- | --- | --- | --- |
| 1 | NVIDIA H100 | Hopper | 16896 | 80GB HBM3 | 大规模 AI 训练 |
| 2 | NVIDIA A100 | Ampere | 6912 | 40GB HBM2 | AI 推理、机器学习 |
| 3 | NVIDIA V100 | Volta | 5120 | 32GB HBM2 | 神经网络训练、科学计算 |
| 4 | Tesla T4 | Turing | 2560 | 16GB GDDR6 | 轻量 AI 推理、云推理 |
| 5 | Tesla P100 | Pascal | 3584 | 16GB HBM2 | 数据中心加速、机器学习推理 |

### 硬件资源配置

显存需求 ≈ 模型参数 × 参数字节数 × 安全系数（1.3-1.5）

CPU GPU 配置比例：建议内存是显存的1.5倍以上
